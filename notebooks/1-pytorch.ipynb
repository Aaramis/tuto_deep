{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TE2ItlsI956"
   },
   "source": [
    "# Séance 1 :  Deep Learning - Introduction à Pytorch \n",
    "\n",
    "Les notebooks sont très largement inspirés des cours de **N. Baskiotis et B. Piwowarski**. Ils peuvent être complétés efficacement par les tutoriels *officiels* présents sur le site de pytorch:\n",
    "https://pytorch.org/tutorials/\n",
    "\n",
    "Au niveau de la configuration, toutes les installations doivent fonctionner sur Linux et Mac. Pour windows, ça peut marcher avec Anaconda à jour... Mais il est difficile de récupérer les problèmes.\n",
    "\n",
    "* Aide à la configuration des machines: [lien](https://dac.lip6.fr/master/environnement-deep/)\n",
    "* Alternative 1 à Windows: installer Ubuntu sous Windows:  [Ubuntu WSL](https://ubuntu.com/wsl)\n",
    "* Alternative 2: travailler sur Google Colab (il faut un compte gmail + prendre le temps de comprendre comment accéder à des fichers) [Colab](https://colab.research.google.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAER7frwJu9L"
   },
   "source": [
    "# A. Préambule\n",
    "\n",
    "Les lignes suivantes permettent d'importer pytorch et vérifier si un GPU est disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3Y9YOOHHhJKY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La version de torch est :  1.13.0+cu117\n",
      "Le calcul GPU est disponible ?  False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"La version de torch est : \",torch.__version__)\n",
    "print(\"Le calcul GPU est disponible ? \", torch.cuda.is_available())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour le chargement des données MNIST (à la fin)\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2LqFo3wzwYP"
   },
   "source": [
    "## Syntaxe\n",
    "\n",
    "Le principal objet manipulé sous Pytorch est **torch.Tensor** qui correspond à un tenseur mathématique (généralisation de la notion de matrice en $n$-dimensions), très proche dans l'utilisation de **numpy.array**.   Cet objet est optimisé pour les calculs sur GPU ce qui implique quelques contraintes plus importantes que sous **numpy**. En particulier :\n",
    "* le type du tenseur manipulé est très important et les conversions ne sont pas automatique (**FloatTensor** de type **torch.float**, **DoubleTensor** de type **torch.double**,  **ByteTensor** de type **torch.byte**, **IntTensor** de type **torch.int**, **LongTensor** de type **torch.long**). Pour un tenseur **t** La conversion se fait très simplement en utilisant les fonctions : **t.double()**, **t.float()**, **t.long()** ...\n",
    "* la plupart des opérations ont une version *inplace*, c'est-à-dire qui modifie le tenseur plutôt que de renvoyer un nouveau tenseur; elles sont suffixées par **_** (**add_** par exemple).\n",
    "\n",
    "Voici ci-dessous quelques exemples d'opérations usuelles, n'hésitez pas à vous référez à la [documentation officielle](https://pytorch.org/docs/stable/tensors.html) pour la liste exhaustive des opérations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "code",
    "id": "VZxNfy1b1u43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [2., 3., 4.]])\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Création de tenseurs et caractéristiques\n",
    "## Créer un tenseur à partir d'une liste\n",
    "print(torch.tensor([[1.,2.,3.],[2.,3,4.]])) \n",
    "## Créer un tenseur  tenseur rempli de 1 de taille 2x3x4\n",
    "print(torch.ones(2,3,4)) \n",
    "## tenseur de zéros de taille 2x3 de type float\n",
    "print(torch.zeros(2,3,dtype=torch.float))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10., 13., 13.],\n",
      "        [12., 13., 12.]])\n",
      "tensor([[1.0639, 1.1589, 1.1052],\n",
      "        [0.8915, 0.9541, 0.9379]])\n",
      "tensor([[-1.5211,  1.7089,  0.2703,  0.6607],\n",
      "        [-0.8275,  1.5839, -1.6362,  1.5110],\n",
      "        [-0.0473,  0.4667,  0.4528, -0.9543]])\n",
      "tensor([ 0.3882,  0.2874, -0.5953])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## tirage uniforme entier entre 10 et 15, \n",
    "## remarquez l'utilisation du _ dans random pour l'opération inplace\n",
    "print(torch.zeros(2,3).random_(10,15)) \n",
    "## tirage suivant la loi normale\n",
    "a=torch.zeros(2,3).normal_(1,0.1)\n",
    "print(a)\n",
    "## equivalent à zeros(3,4).normal_\n",
    "b = torch.randn(3,4) \n",
    "print(b)\n",
    "## Création d'un vecteur\n",
    "c = torch.randn(3)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0258, 1.0963, 0.9744, 1.0258, 1.0963, 0.9744],\n",
      "        [1.0587, 0.9920, 1.0075, 1.0587, 0.9920, 1.0075]])\n",
      "3 torch.Size([3, 4]) torch.Size([3, 4])\n",
      "tensor([[1, 1, 0],\n",
      "        [1, 0, 1]], dtype=torch.int32) torch.IntTensor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## concatenation de tenseurs\n",
    "print(torch.cat((a,a),1))\n",
    "## Taille des tenseurs/vecteurs shape => size\n",
    "print(a.size(1),b.shape,b.size())\n",
    "## Conversion de type\n",
    "print(a.int(),a.int().type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5878)\n",
      "tensor([[-2.6297,  4.1696, -1.1083,  1.3994],\n",
      "        [-2.1899,  3.4724, -0.8955,  1.1356]]) tensor([[-2.6297,  4.1696, -1.1083,  1.3994],\n",
      "        [-2.1899,  3.4724, -0.8955,  1.1356]])\n",
      "tensor([[1.0639, 0.8915],\n",
      "        [1.1589, 0.9541],\n",
      "        [1.1052, 0.9379]]) tensor([[1.0639, 0.8915],\n",
      "        [1.1589, 0.9541],\n",
      "        [1.1052, 0.9379]])\n",
      "argmax :  tensor([1, 1])\n",
      "tensor([ 1.1186,  0.6312, -0.0821]) tensor(1.6678)\n",
      "tensor([ 0.2797,  0.1578, -0.0205]) tensor(0.1390)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Opérations élémentaires sur les tenseurs \n",
    "## produit scalaire (et contrairement à numpy, que produit scalaire)\n",
    "print(c.dot(c))\n",
    "## produit matriciel : utilisation de @ ou de la fonction mm\n",
    "print(a.mm(b), a @ b)\n",
    "## transposé\n",
    "print(a.t(),a.T)\n",
    "## index du maximum selon une dimension\n",
    "print(\"argmax : \",a.argmax(dim=1))\n",
    "## somme selon une dimension/de tous les éléments\n",
    "print(b.sum(1), b.sum()) \n",
    "## moyenne selon  une dimension/sur tous les éléments\n",
    "print(b.mean(1), b.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1278, 2.3178, 2.2104],\n",
      "        [1.7829, 1.9082, 1.8759]])\n"
     ]
    }
   ],
   "source": [
    "## ATTENTION: la spécificité et les capacités des tenseurs empêchent les conversions à la volée\n",
    "# ce qui marchait en numpy ne marche plus en torch\n",
    "\n",
    "# print(a@[[1], [1], [1]])    # ERREUR de type (même résultat avec n'importe quelle opération)\n",
    "print(a*2)                  # OK pour un scalaire\n",
    "# print(a*[2.,2.,2.])         # ERREUR de type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1278, 2.3178, 2.2104],\n",
      "        [1.7829, 1.9082, 1.8759]]) \n",
      " tensor([[1.1319, 1.3431, 1.2215],\n",
      "        [0.7947, 0.9103, 0.8797]]) \n",
      " tensor([[1.1319, 1.3431, 1.2215],\n",
      "        [0.7947, 0.9103, 0.8797]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) tensor([0., 0., 0., 0., 0.])\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]]) tensor([[0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## somme/produit/puissance termes a termes\n",
    "print(a+a, \"\\n\", a*a, \"\\n\", a**2)\n",
    "## attention ! comme sous numpy, il peut y avoir des pièges ! \n",
    "## Vérifier toujours les dimensions !!\n",
    "d=torch.zeros(5,1)\n",
    "e = torch.zeros(5)\n",
    "print(d,e)\n",
    "## la première opération fait un broadcast et le résultat est tenseur à 2 dimensiosn,\n",
    "## le résultat de la deuxième opération est bien un vecteur\n",
    "print(d-e,d.t()-e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.5828,  1.7703, -0.0596, -0.7327],\n",
      "        [ 2.0084, -0.5536, -0.3492,  1.1030],\n",
      "        [-1.6760, -0.3032,  0.6674, -0.3052]])\n",
      "tensor([[ 2.5828,  1.7703, -0.0596, -0.7327,  2.0084, -0.5536],\n",
      "        [-0.3492,  1.1030, -1.6760, -0.3032,  0.6674, -0.3052]])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[ 3.5608],\n",
      "        [ 2.2087],\n",
      "        [-1.6171]])\n",
      "tensor([ 3.5608,  2.2087, -1.6171])\n",
      "tensor([[ True,  True,  True],\n",
      "        [ True,  True,  True],\n",
      "        [False, False, False]])\n",
      "tensor([ True,  True, False])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## changer les dimensions du tenseur (la taille totale doit être inchangée) = np.reshape\n",
    "b = torch.randn(3,4) \n",
    "print(b)\n",
    "print(b.view(2,6))\n",
    "\n",
    "# on utilise souvent view pour passer de matrice à vecteur\n",
    "e=torch.tensor([[1],[1],[1],[1]], dtype=torch.float)\n",
    "print(e)\n",
    "d = b@e\n",
    "print(d)\n",
    "# conversion en vecteur\n",
    "print(d.view(-1))\n",
    "\n",
    "# usage typique\n",
    "y = torch.tensor([1,-1,1], dtype=torch.float)\n",
    "print(d > y)            # résultat catastrophique (dispatch)\n",
    "print(d.view(-1) > y)   # résultat attendu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzhX7D8KKIvt"
   },
   "source": [
    "# B. Autograd et graphe de calcul\n",
    "Un élément central de pytorch est le graphe de calcul : lors du calcul d'une variable, l'ensemble des opérations qui ont servies au calcul sont stockées sous la forme d'un graphe acyclique, dit de *calcul*. Les noeuds internes du graphe représentent les opérations, le noeud terminal le résultat et les racines les variables d'entrées. Ce graphe sert en particulier à calculer les dérivées partielles de la sortie par rapport aux variables d'entrées - en utilisant les règles de dérivations chainées des fonctions composées. \n",
    "Pour cela, toutes les fonctions disponibles dans pytorch comportent un mécanisme, appelé *autograd* (automatique differentiation), qui permet de calculer les dérivées partielles des opérations. \n",
    "\n",
    "## B.1. Différenciation automatique\n",
    "(De manière simplifiée, pour les détails cf [la documentation](https://pytorch.org/docs/stable/notes/extending.html))\n",
    "\n",
    "Toute opération sous pytorch hérite de la classe **Function** et doit définir :\n",
    "* une méthode **forward(\\*args)** : passe avant, calcule le résultat de la fonction appliquée aux arguments \n",
    "* une méthode **backward(\\*args)** : passe arrière, calcule les dérivées partielles par rapport aux entrées. Les arguments de  cette méthode correspondent aux valeurs des dérivées suivantes dans le graphe de calcul. En particulier, il y a autant d'arguments à **backward**  que de sorties pour la méthode **forward** (rétro-propagation : on doit connaître les dérivés qui viennent  en aval du calcul) et autant de sorties que d'arguments dans la méthode **forward** (chaque sortie correspond à  une dérivée partielle par rapport à chaque entrée du module). Le calcul se fait sur les valeurs du dernier appel de **forward**. \n",
    "\n",
    "Par exemple, pour la fonction d'addition  **add(x,y)**, **add.forward(x,y)** renverra **x+y** (l'appel de la fonction est équivalent à l'appel de **forward**) et **add.backward(1)** renverra le couple **(1,1)** (la dérivée par rapport à x, et celle par rapport à y) .\n",
    "\n",
    "En pratique, ce ne sont pas les méthodes de ces fonctions qui sont utilisées, mais des méthodes équivalentes sur les tenseurs. La méthode **backward** d'un tenseur permet de rétro-propager le calcul du gradient sur toutes les variables qui ont servies à son calcul.\n",
    "\n",
    "La valeur du gradient pour chaque dérivée partielle se trouve dans l'attribut **grad** de la variable concernée. \n",
    "\n",
    "Comme c'est un mécanisme lourd, l'autograd n'est pas activé par défaut pour une variable. Afin de l'activer, il faut mettre le flag **requires_grad** de cette variable à vrai. Dès lors, tout calcul qui utilise cette variable sera enregistré dans le graphe de calcul et le gradient sera disponible.\n",
    "\n",
    "\n",
    "Exemple : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "J_mYVeXMfsTV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphe de calcul ?  False\n",
      "Dérivée de z/a :  2.0  z/b : 1.0\n",
      "Erreur :  element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(1.)\n",
    "# Par défaut, requires_grad est à False\n",
    "print(\"Graphe de calcul ? \",a.requires_grad)\n",
    "# On peut demander à ce que le graphe de calcul soit retenu\n",
    "a.requires_grad = True \n",
    "# Ou lors de la création du tenseur directement\n",
    "b = torch.tensor(2.,requires_grad=True)\n",
    "z = 2*a + b\n",
    "# Calcul des dérivées partielles par rapport à z\n",
    "z.backward()\n",
    "print(\"Dérivée de z/a : \", a.grad.item(),\" z/b :\", b.grad.item())\n",
    "\n",
    "# Si on a oublié de demander le graphe de calcul :\n",
    "a, b = torch.tensor(1.),torch.tensor(2.)\n",
    "z = 2*a+b\n",
    "try: # on sait que ça va provoquer une erreur\n",
    "  z.backward()\n",
    "except Exception as e: # erreur => simple message\n",
    "  print(\"Erreur : \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eApYmHIZa917"
   },
   "source": [
    "## B.2. <span class=\"alert-success\">     Exercice :  Utilisation de backward     </div>\n",
    "* Implémentez (en une ligne) la fonction de coût aux moindres carrés $MSE(\\hat{y},y)=\\frac{1}{2N} \\sum_{i=1}^N\\|\\hat{y_i}-y_i\\|^2$ où $\\hat{y},y$ sont deux matrices de taille $N\\times d$, et $y_i,\\hat{y_i}$ les $i$-èmes vecteurs lignes des matrices.\n",
    "* Engendrez **y,yhat** deux matrices aléatoires de taille $(1,5)$.\n",
    "* Calculez **MSE(y,yhat)**\n",
    "* Calculez à la main le gradient de **MSE** par rapport à **y**, **yhat**\n",
    "* Calculez grâce à pytorch le gradient de **MSE** par rapport à **y** et **yhat** et vérifier le résultat.\n",
    "* Appelez une deuxième fois **MSE** sur les mêmes vecteurs et la méthode **backward**. Qu'observez vous pour le gradient ? Comment l'expliquez vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4Rd37M_3gkqu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE : tensor(0.6707, grad_fn=<DivBackward0>)\n",
      "\t y :\t tensor([-0.9815,  0.4935, -0.1612,  0.4270,  0.3370], requires_grad=True) \n",
      " \t yhat :\t tensor([-0.6747,  0.3059,  1.6617,  1.3496, -1.2133], requires_grad=True)\n",
      "Gradient \n",
      " \t y :\t tensor([-0.0614,  0.0375, -0.3646, -0.1845,  0.3101]) \n",
      " \t yhat :\t tensor([ 0.0614, -0.0375,  0.3646,  0.1845, -0.3101])\n",
      "Gradient \n",
      " \t y: \t tensor([-0.1227,  0.0750, -0.7292, -0.3690,  0.6201]) \n",
      " \t yhat :\t tensor([ 0.1227, -0.0750,  0.7292,  0.3690, -0.6201])\n"
     ]
    }
   ],
   "source": [
    "def MSE(yhat,y):\n",
    "    # return ((yhat - y)**2).sum()/(2 * len(y))\n",
    "    # return ((yhat - y)**2).mean()/2\n",
    "    return torch.nn.MSELoss()(y, yhat)/2\n",
    "\n",
    "# y = torch.randn(1,5,requires_grad=True)\n",
    "# yhat = torch.randn(1,5,requires_grad=True)\n",
    "y = torch.tensor([-0.9815,  0.4935, -0.1612,  0.4270,  0.3370], requires_grad=True)\n",
    "yhat = torch.tensor([-0.6747,  0.3059,  1.6617,  1.3496, -1.2133], requires_grad=True)\n",
    "# # y1 = torch.tensor([[2.],[3.]], requires_grad=True)\n",
    "# yhat1 = torch.tensor([[1.],[1.]], requires_grad=True)\n",
    "# print(y)\n",
    "# print(y1)\n",
    "# print(y, \"\\n\", yhat)\n",
    "mse = MSE(yhat,y)\n",
    "print(\"MSE :\" ,mse)\n",
    "print(\"\\t y :\\t\", y, \"\\n \\t yhat :\\t\", yhat)\n",
    "\n",
    "# 1. retro-propager l'erreur\n",
    "mse.backward()\n",
    "# 2. afficher le gradient sur les deux vecteurs et comprendre ce qui se passe\n",
    "print(\"Gradient \\n \\t y :\\t\", y.grad, \"\\n \\t yhat :\\t\", yhat.grad)\n",
    "# 3. faire une itération supplémentaire et afficher de nouveau\n",
    "mse = MSE(yhat,y)\n",
    "mse.backward()\n",
    "print(\"Gradient \\n \\t y: \\t\", y.grad, \"\\n \\t yhat :\\t\", yhat.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxgCbApIgU5X"
   },
   "source": [
    "\n",
    "## B.3. <span class=\"alert-success\"> Exercice :   Régression linéaire en pytorch </span>\n",
    "\n",
    "* Définissez la fonction **flineaire(x,w,b)** fonction linéaire qui calcule $f(x,w,b)=x.w^t+b$  avec $x\\in \\mathbb{R}^{{n\\times d}},~w\\in\\mathbb{R}^{1,d}, b\\in \\mathbb{R}$\n",
    "* Complétez le code ci-dessous pour réaliser une descente de gradient et apprendre les paramètres optimaux de la regression linéaire : $$w^∗,b^∗=\\text{argmin}_{w,b}\\frac{1}{N} \\sum_{i=1}^N \\|f(x^i,w,b)-y^i\\|^2$$\n",
    "\n",
    "Pour tester votre code, utilisez le jeu de données très classique *Boston*, le prix des loyers à Boston en fonction de caractéristiques socio-économiques des quartiers. Le code ci-dessous permet de les charger.\n",
    "\n",
    "<span style=\"color:red\"> ATTENTION ! </span>\n",
    "* pour la mise-à-jour des paramètres, <span style=\"color:red\">vous ne pouvez pas faire directement</span> \n",
    "$$w = w-\\epsilon*gradient$$ \n",
    "(pourquoi ?). Vous devez passer par w.data qui permet de ne pas enregistrer les opérations dans le graphe de calcul (ou utiliser la méthode ```.detach()``` d'une variable qui permet de créer une copie détachée du graphe de calcul). \n",
    "* Note: il est aussi possible de faire:\n",
    "    ```\n",
    "    with torch.no_grad():\n",
    "        w -= eps*gradient\n",
    "    ```\n",
    "    * Désactivation temporaire du graph de calcul, on manipule les tensors comme des variables classiques\n",
    "    * ATTENTION à faire des ```-=``` ou ```+=``` => Si vous construisez un nouveau tenseur, il ne se reconnectera pas au graphe de calcul!\n",
    "* l'algorithme doit converger avec la valeur de epsilon fixée; si ce n'est pas le cas, il y a une erreur (la plupart du temps au niveau du calcul du coût).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "dArOgSWNTVvb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'exemples :  506 Dimension :  13\n",
      "Nom des attributs :  CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT\n"
     ]
    }
   ],
   "source": [
    "def flineaire(x,w,b):\n",
    "    return x@w.T + b\n",
    "\n",
    "## Chargement des données Boston (depuis sklearn) et transformation en tensor.\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston() ## chargement des données\n",
    "boston_x = torch.tensor(boston['data'],dtype=torch.float) # penser à typer les données pour éliminer les incertitudes\n",
    "boston_y = torch.tensor(boston['target'],dtype=torch.float)\n",
    "\n",
    "print(\"Nombre d'exemples : \",boston_x.size(0), \"Dimension : \",boston_x.size(1))\n",
    "print(\"Nom des attributs : \", \", \".join(boston['feature_names']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 200\n",
    "EPS = 1e-6\n",
    "#initialisation aléatoire de w et b\n",
    "w = torch.randn(1,boston_x.size(1),requires_grad=True)\n",
    "b =  torch.randn(1,1,requires_grad=True)\n",
    "loss_h = []\n",
    "# torch.nn.MSELoss()(w.data,b)\n",
    "# print(loss_h)\n",
    "for i in range(EPOCHS):\n",
    "    ## SOLUTION 1: Penser à aller chercher w.data (et sa contrepartie dans le gradient)\n",
    "    yhat = flineaire(boston_x, w, b) # [506,1]\n",
    "    yhat = yhat.view(-1) # passage en mode vecteur [506] On a redimensionné notre matrice\n",
    "    # 1. Construire la loss (+stocker la valeur dans loss_h)\n",
    "    loss = MSE(yhat, boston_y)\n",
    "    loss_h.append(loss.data)\n",
    "    # 2. Retro-propager\n",
    "    loss.backward()\n",
    "    # 3. MAJ des paramètres\n",
    "    w.data -= EPS*w.grad\n",
    "    b.data -= EPS*b.grad\n",
    "    # 4. Penser à remettre le gradient à 0 (cf exo précédent)\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# une seconde version du même code avec l'environnement torch.no_grad()\n",
    "# attention, dans ce cas, le += est obligatoire\n",
    "# code identique (juste changer les 2 lignes de MAJ)\n",
    "\n",
    "EPOCHS = 200\n",
    "EPS = 1e-6\n",
    "#initialisation aléatoire de w et b\n",
    "w = torch.randn(1,boston_x.size(1),requires_grad=True)\n",
    "b =  torch.randn(1,1,requires_grad=True)\n",
    "loss_h = [] # sauvegarde des valeurs de loss (pas si trivial!)\n",
    "for i in range(EPOCHS):\n",
    "    yhat = flineaire(boston_x, w, b) # [506,1]\n",
    "    yhat = yhat.view(-1) # passage en mode vecteur [506] On a redimensionné notre matrice\n",
    "    # 1. Construire la loss (+stocker la valeur dans loss_h)\n",
    "    loss = MSE(yhat, boston_y)\n",
    "    loss_h.append(loss.data)\n",
    "    # 2. Retro-propager\n",
    "    loss.backward()\n",
    "    # 3. MAJ des paramètres\n",
    "    with torch.no_grad():\n",
    "        w -= EPS*w.grad\n",
    "        b -= EPS*b.grad\n",
    "    # 4. Penser à remettre le gradient à 0 (cf exo précédent)\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse loss')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# affichage de l'optimisation\n",
    "plt.figure()\n",
    "plt.plot(loss_h)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"mse loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70il-LA_XGSO"
   },
   "source": [
    "\n",
    "## Optimiseur \n",
    "La descente de gradient représente en fait un code standard puisque les dérivées sont calculées automatiquement et que les variables sont idéntifiées.\n",
    "Pytorch inclut une classe très utile pour la descente de gradient, [torch.optim](https://pytorch.org/docs/stable/optim.html), qui permet :\n",
    "* d'économiser quelques lignes de codes\n",
    "* d'automatiser la mise-à-jour des paramètres \n",
    "* d'abstraire le type de descente de gradient utilisé (sgd,adam, rmsprop, ...)\n",
    "\n",
    "Une liste de paramètres à optimiser est passée à l'optimiseur lors de l'initialisation. La méthode **zero_grad()** permet de remettre le gradient à zéro et la méthode **step()** permet de faire une mise-à-jour des paramètres.\n",
    "\n",
    "Un exemple de code  utilisant l'optimiseur est donné ci-dessous. Testez et comparez les résultats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "58sP5ryLeP3Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0, loss : 751983.375\n",
      "iteration : 100, loss : 632.293701171875\n",
      "iteration : 200, loss : 478.4914855957031\n",
      "iteration : 300, loss : 429.10186767578125\n",
      "iteration : 400, loss : 386.78460693359375\n",
      "iteration : 500, loss : 349.8293762207031\n",
      "iteration : 600, loss : 317.48101806640625\n",
      "iteration : 700, loss : 289.1015625\n",
      "iteration : 800, loss : 264.14593505859375\n",
      "iteration : 900, loss : 242.14840698242188\n",
      "iteration : 1000, loss : 222.71002197265625\n",
      "iteration : 1100, loss : 205.4908905029297\n",
      "iteration : 1200, loss : 190.1983642578125\n",
      "iteration : 1300, loss : 176.58203125\n",
      "iteration : 1400, loss : 164.42666625976562\n",
      "iteration : 1500, loss : 153.54766845703125\n",
      "iteration : 1600, loss : 143.7860107421875\n",
      "iteration : 1700, loss : 135.00460815429688\n",
      "iteration : 1800, loss : 127.08521270751953\n",
      "iteration : 1900, loss : 119.92570495605469\n",
      "iteration : 2000, loss : 113.43770599365234\n",
      "iteration : 2100, loss : 107.54460906982422\n",
      "iteration : 2200, loss : 102.1797103881836\n",
      "iteration : 2300, loss : 97.28511047363281\n",
      "iteration : 2400, loss : 92.81031799316406\n",
      "iteration : 2500, loss : 88.71131896972656\n",
      "iteration : 2600, loss : 84.94940185546875\n",
      "iteration : 2700, loss : 81.49063873291016\n",
      "iteration : 2800, loss : 78.30526733398438\n",
      "iteration : 2900, loss : 75.36693572998047\n",
      "iteration : 3000, loss : 72.6524429321289\n",
      "iteration : 3100, loss : 70.1412124633789\n",
      "iteration : 3200, loss : 67.8149185180664\n",
      "iteration : 3300, loss : 65.65737915039062\n",
      "iteration : 3400, loss : 63.65385437011719\n",
      "iteration : 3500, loss : 61.791465759277344\n",
      "iteration : 3600, loss : 60.05847930908203\n",
      "iteration : 3700, loss : 58.444400787353516\n",
      "iteration : 3800, loss : 56.93977355957031\n",
      "iteration : 3900, loss : 55.536006927490234\n"
     ]
    }
   ],
   "source": [
    "Xdim = boston_x.size(1)\n",
    "EPOCHS = 4000\n",
    "\n",
    "w = torch.randn(1, Xdim, dtype=torch.float, requires_grad=True)\n",
    "b = torch.randn(1, dtype=torch.float, requires_grad=True)\n",
    "## on optimise selon w et b.  lr est le pas du gradient\n",
    "optim = torch.optim.SGD(params=[w,b],lr=EPS) \n",
    "for i in range(EPOCHS):\n",
    "  loss = MSE(flineaire(boston_x,w,b).view(-1,1),boston_y.view(-1,1))\n",
    "  optim.zero_grad()\n",
    "  loss.backward()\n",
    "  optim.step()  \n",
    "  if i % 100==0:  print(f\"iteration : {i}, loss : {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1rxv3ychdhD"
   },
   "source": [
    "## C. Architecture modulaires & réseaux de neurones\n",
    "Dans le framework pytorch (et dans la plupart des frameworks analogues), le module est la brique de base qui permet de construire un réseau de neurones.  Il permet de représenter en particulier :\n",
    "* une couche du réseau (linéaire : **torch.nn.Linear**, convolution : **torch.nn.convXd**, ...)\n",
    "* une fonction d'activation (tanh : **torch.nn.Tanh**, sigmoïde : **torch.nn.Sigmoid** , ReLu : **torch.nn.ReLu**, ...)\n",
    "* une fonction de coût (MSE : **torch.nn.MSELoss**, L1 :  **torch.nn.L1Loss**, CrossEntropy binaire: **torch.BCE**, CrossEntropy : **torch.nn.CrossEntropyLoss**, ...)\n",
    "* mais également des outils de régularisation (BatchNorm : **torch.nn.BatchNorm1d**, Dropout : **torch.nn.Dropout**, ...)\n",
    "* un ensemble de modules : en termes informatique, un module est un conteneur abstrait qui peut contenir d'autres conteneurs) : plusieurs modules peuvent être mis ensemble afin de former un nouveau module plus complexe.\n",
    "\n",
    "\n",
    "Le fonctionnement est très proche des fonctions que nous avons vu ci-dessus (un module encapsule en fait une fonction de **torch.nn.Function**), mais de manière à gérer automatiquement les paramètres à apprendre. Un module est ainsi muni :\n",
    "* d'une méthode **forward** qui permet de calculer la sortie du module à partir des entrées\n",
    "* d'une méthode **backward** qui permet d'effectuer la rétro-propagation (localement).\n",
    "* tous les paramètres sont automatiquement ajoutés dans une liste interne, accessible par la fonction **.parameters()** du module.\n",
    "\n",
    "Ci-dessous un exemple de régression linéaire en utilisant les modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "spguRLUjD60C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sortie du réseau tensor([[-7.1210],\n",
      "        [-6.2041],\n",
      "        [-7.4647],\n",
      "        [-9.3827],\n",
      "        [-8.7766],\n",
      "        [-8.0196],\n",
      "        [-9.0078],\n",
      "        [-6.3453],\n",
      "        [-8.0930],\n",
      "        [-7.1587]], grad_fn=<SliceBackward0>)\n",
      "Paramètres et noms des paramètres [(Parameter containing:\n",
      "tensor([[ 0.0073, -0.0030, -0.0440, -0.1497, -0.2121,  0.0322,  0.1483, -0.1514,\n",
      "          0.0532, -0.0063, -0.0911, -0.0302, -0.2411]], requires_grad=True), ('weight', Parameter containing:\n",
      "tensor([[ 0.0073, -0.0030, -0.0440, -0.1497, -0.2121,  0.0322,  0.1483, -0.1514,\n",
      "          0.0532, -0.0063, -0.0911, -0.0302, -0.2411]], requires_grad=True))), (Parameter containing:\n",
      "tensor([0.2724], requires_grad=True), ('bias', Parameter containing:\n",
      "tensor([0.2724], requires_grad=True)))]\n",
      "iteration : 0, loss : 24630.96875\n",
      "iteration : 1, loss : 4103.08251953125\n",
      "iteration : 2, loss : 1180.4862060546875\n",
      "iteration : 3, loss : 724.70458984375\n",
      "iteration : 4, loss : 618.2102661132812\n",
      "iteration : 5, loss : 564.4752197265625\n",
      "iteration : 6, loss : 521.6715698242188\n",
      "iteration : 7, loss : 483.6241760253906\n",
      "iteration : 8, loss : 449.1735534667969\n",
      "iteration : 9, loss : 417.88897705078125\n",
      "iteration : 10, loss : 389.46600341796875\n",
      "iteration : 11, loss : 363.640380859375\n",
      "iteration : 12, loss : 340.1737060546875\n",
      "iteration : 13, loss : 318.84979248046875\n",
      "iteration : 14, loss : 299.4723205566406\n",
      "iteration : 15, loss : 281.8627624511719\n",
      "iteration : 16, loss : 265.859130859375\n",
      "iteration : 17, loss : 251.314208984375\n",
      "iteration : 18, loss : 238.09429931640625\n",
      "iteration : 19, loss : 226.07803344726562\n",
      "iteration : 20, loss : 215.15504455566406\n",
      "iteration : 21, loss : 205.22512817382812\n",
      "iteration : 22, loss : 196.1973114013672\n",
      "iteration : 23, loss : 187.98895263671875\n",
      "iteration : 24, loss : 180.5248565673828\n",
      "iteration : 25, loss : 173.73690795898438\n",
      "iteration : 26, loss : 167.56317138671875\n",
      "iteration : 27, loss : 161.94729614257812\n",
      "iteration : 28, loss : 156.8382110595703\n",
      "iteration : 29, loss : 152.18946838378906\n",
      "iteration : 30, loss : 147.95889282226562\n",
      "iteration : 31, loss : 144.10816955566406\n",
      "iteration : 32, loss : 140.60255432128906\n",
      "iteration : 33, loss : 137.41038513183594\n",
      "iteration : 34, loss : 134.50296020507812\n",
      "iteration : 35, loss : 131.85423278808594\n",
      "iteration : 36, loss : 129.4404754638672\n",
      "iteration : 37, loss : 127.24020385742188\n",
      "iteration : 38, loss : 125.23384094238281\n",
      "iteration : 39, loss : 123.4036636352539\n",
      "iteration : 40, loss : 121.73353576660156\n",
      "iteration : 41, loss : 120.20879364013672\n",
      "iteration : 42, loss : 118.8161849975586\n",
      "iteration : 43, loss : 117.54354858398438\n",
      "iteration : 44, loss : 116.37997436523438\n",
      "iteration : 45, loss : 115.31544494628906\n",
      "iteration : 46, loss : 114.34093475341797\n",
      "iteration : 47, loss : 113.44821166992188\n",
      "iteration : 48, loss : 112.62977600097656\n",
      "iteration : 49, loss : 111.87886810302734\n",
      "iteration : 50, loss : 111.18931579589844\n",
      "iteration : 51, loss : 110.55551147460938\n",
      "iteration : 52, loss : 109.97238159179688\n",
      "iteration : 53, loss : 109.43529510498047\n",
      "iteration : 54, loss : 108.9400405883789\n",
      "iteration : 55, loss : 108.48284149169922\n",
      "iteration : 56, loss : 108.06019592285156\n",
      "iteration : 57, loss : 107.66898345947266\n",
      "iteration : 58, loss : 107.30633544921875\n",
      "iteration : 59, loss : 106.96963500976562\n",
      "iteration : 60, loss : 106.65660858154297\n",
      "iteration : 61, loss : 106.3650131225586\n",
      "iteration : 62, loss : 106.09293365478516\n",
      "iteration : 63, loss : 105.83863067626953\n",
      "iteration : 64, loss : 105.60047912597656\n",
      "iteration : 65, loss : 105.37701416015625\n",
      "iteration : 66, loss : 105.16692352294922\n",
      "iteration : 67, loss : 104.9690170288086\n",
      "iteration : 68, loss : 104.78216552734375\n",
      "iteration : 69, loss : 104.60539245605469\n",
      "iteration : 70, loss : 104.43780517578125\n",
      "iteration : 71, loss : 104.27857971191406\n",
      "iteration : 72, loss : 104.12696838378906\n",
      "iteration : 73, loss : 103.9822998046875\n",
      "iteration : 74, loss : 103.84394836425781\n",
      "iteration : 75, loss : 103.71136474609375\n",
      "iteration : 76, loss : 103.58405303955078\n",
      "iteration : 77, loss : 103.4615249633789\n",
      "iteration : 78, loss : 103.3433837890625\n",
      "iteration : 79, loss : 103.229248046875\n",
      "iteration : 80, loss : 103.11875915527344\n",
      "iteration : 81, loss : 103.0116195678711\n",
      "iteration : 82, loss : 102.90753173828125\n",
      "iteration : 83, loss : 102.8062515258789\n",
      "iteration : 84, loss : 102.70751953125\n",
      "iteration : 85, loss : 102.61113739013672\n",
      "iteration : 86, loss : 102.51692199707031\n",
      "iteration : 87, loss : 102.42466735839844\n",
      "iteration : 88, loss : 102.33423614501953\n",
      "iteration : 89, loss : 102.24547576904297\n",
      "iteration : 90, loss : 102.15825653076172\n",
      "iteration : 91, loss : 102.07245635986328\n",
      "iteration : 92, loss : 101.98798370361328\n",
      "iteration : 93, loss : 101.9046859741211\n",
      "iteration : 94, loss : 101.82251739501953\n",
      "iteration : 95, loss : 101.74140167236328\n",
      "iteration : 96, loss : 101.66123962402344\n",
      "iteration : 97, loss : 101.58197021484375\n",
      "iteration : 98, loss : 101.5035171508789\n",
      "iteration : 99, loss : 101.42585754394531\n",
      "iteration : 100, loss : 101.34893798828125\n",
      "iteration : 101, loss : 101.27267456054688\n",
      "iteration : 102, loss : 101.19705963134766\n",
      "iteration : 103, loss : 101.12201690673828\n",
      "iteration : 104, loss : 101.04756164550781\n",
      "iteration : 105, loss : 100.97364044189453\n",
      "iteration : 106, loss : 100.90020751953125\n",
      "iteration : 107, loss : 100.82726287841797\n",
      "iteration : 108, loss : 100.75475311279297\n",
      "iteration : 109, loss : 100.68267822265625\n",
      "iteration : 110, loss : 100.61102294921875\n",
      "iteration : 111, loss : 100.53974914550781\n",
      "iteration : 112, loss : 100.4688491821289\n",
      "iteration : 113, loss : 100.3983154296875\n",
      "iteration : 114, loss : 100.32811737060547\n",
      "iteration : 115, loss : 100.2582778930664\n",
      "iteration : 116, loss : 100.18873596191406\n",
      "iteration : 117, loss : 100.1195068359375\n",
      "iteration : 118, loss : 100.05057525634766\n",
      "iteration : 119, loss : 99.9819564819336\n",
      "iteration : 120, loss : 99.9135971069336\n",
      "iteration : 121, loss : 99.84555053710938\n",
      "iteration : 122, loss : 99.77774047851562\n",
      "iteration : 123, loss : 99.71019744873047\n",
      "iteration : 124, loss : 99.64292907714844\n",
      "iteration : 125, loss : 99.5759048461914\n",
      "iteration : 126, loss : 99.5091323852539\n",
      "iteration : 127, loss : 99.4426040649414\n",
      "iteration : 128, loss : 99.3763198852539\n",
      "iteration : 129, loss : 99.31025695800781\n",
      "iteration : 130, loss : 99.24443054199219\n",
      "iteration : 131, loss : 99.17884826660156\n",
      "iteration : 132, loss : 99.11347198486328\n",
      "iteration : 133, loss : 99.04833221435547\n",
      "iteration : 134, loss : 98.9834213256836\n",
      "iteration : 135, loss : 98.91870880126953\n",
      "iteration : 136, loss : 98.8542251586914\n",
      "iteration : 137, loss : 98.78994750976562\n",
      "iteration : 138, loss : 98.72590637207031\n",
      "iteration : 139, loss : 98.66204833984375\n",
      "iteration : 140, loss : 98.59841918945312\n",
      "iteration : 141, loss : 98.53497314453125\n",
      "iteration : 142, loss : 98.47176361083984\n",
      "iteration : 143, loss : 98.40873718261719\n",
      "iteration : 144, loss : 98.34590911865234\n",
      "iteration : 145, loss : 98.28329467773438\n",
      "iteration : 146, loss : 98.22087097167969\n",
      "iteration : 147, loss : 98.15866088867188\n",
      "iteration : 148, loss : 98.09663391113281\n",
      "iteration : 149, loss : 98.03479766845703\n",
      "iteration : 150, loss : 97.97319030761719\n",
      "iteration : 151, loss : 97.9117431640625\n",
      "iteration : 152, loss : 97.85050964355469\n",
      "iteration : 153, loss : 97.78946685791016\n",
      "iteration : 154, loss : 97.72862243652344\n",
      "iteration : 155, loss : 97.66795349121094\n",
      "iteration : 156, loss : 97.60748291015625\n",
      "iteration : 157, loss : 97.54719543457031\n",
      "iteration : 158, loss : 97.48711395263672\n",
      "iteration : 159, loss : 97.42720031738281\n",
      "iteration : 160, loss : 97.36748504638672\n",
      "iteration : 161, loss : 97.30794525146484\n",
      "iteration : 162, loss : 97.24859619140625\n",
      "iteration : 163, loss : 97.18944549560547\n",
      "iteration : 164, loss : 97.13046264648438\n",
      "iteration : 165, loss : 97.07167053222656\n",
      "iteration : 166, loss : 97.01306915283203\n",
      "iteration : 167, loss : 96.95462799072266\n",
      "iteration : 168, loss : 96.89639282226562\n",
      "iteration : 169, loss : 96.83832550048828\n",
      "iteration : 170, loss : 96.78044128417969\n",
      "iteration : 171, loss : 96.72273254394531\n",
      "iteration : 172, loss : 96.66520690917969\n",
      "iteration : 173, loss : 96.60785675048828\n",
      "iteration : 174, loss : 96.55068969726562\n",
      "iteration : 175, loss : 96.49368286132812\n",
      "iteration : 176, loss : 96.4368667602539\n",
      "iteration : 177, loss : 96.38023376464844\n",
      "iteration : 178, loss : 96.32376861572266\n",
      "iteration : 179, loss : 96.2674789428711\n",
      "iteration : 180, loss : 96.21135711669922\n",
      "iteration : 181, loss : 96.1554183959961\n",
      "iteration : 182, loss : 96.09963989257812\n",
      "iteration : 183, loss : 96.04405212402344\n",
      "iteration : 184, loss : 95.9886245727539\n",
      "iteration : 185, loss : 95.93336486816406\n",
      "iteration : 186, loss : 95.8782730102539\n",
      "iteration : 187, loss : 95.8233642578125\n",
      "iteration : 188, loss : 95.76861572265625\n",
      "iteration : 189, loss : 95.71403503417969\n",
      "iteration : 190, loss : 95.65962219238281\n",
      "iteration : 191, loss : 95.60538482666016\n",
      "iteration : 192, loss : 95.55131530761719\n",
      "iteration : 193, loss : 95.49739837646484\n",
      "iteration : 194, loss : 95.44366455078125\n",
      "iteration : 195, loss : 95.39008331298828\n",
      "iteration : 196, loss : 95.33668518066406\n",
      "iteration : 197, loss : 95.28343200683594\n",
      "iteration : 198, loss : 95.2303466796875\n",
      "iteration : 199, loss : 95.17742919921875\n",
      "iteration : 200, loss : 95.12468719482422\n",
      "iteration : 201, loss : 95.07207489013672\n",
      "iteration : 202, loss : 95.01964569091797\n",
      "iteration : 203, loss : 94.96737670898438\n",
      "iteration : 204, loss : 94.915283203125\n",
      "iteration : 205, loss : 94.86331939697266\n",
      "iteration : 206, loss : 94.81151580810547\n",
      "iteration : 207, loss : 94.75989532470703\n",
      "iteration : 208, loss : 94.70842742919922\n",
      "iteration : 209, loss : 94.65711212158203\n",
      "iteration : 210, loss : 94.60595703125\n",
      "iteration : 211, loss : 94.5549545288086\n",
      "iteration : 212, loss : 94.50410461425781\n",
      "iteration : 213, loss : 94.45342254638672\n",
      "iteration : 214, loss : 94.40289306640625\n",
      "iteration : 215, loss : 94.3525161743164\n",
      "iteration : 216, loss : 94.30229187011719\n",
      "iteration : 217, loss : 94.2522201538086\n",
      "iteration : 218, loss : 94.20231628417969\n",
      "iteration : 219, loss : 94.15254974365234\n",
      "iteration : 220, loss : 94.10295104980469\n",
      "iteration : 221, loss : 94.0534896850586\n",
      "iteration : 222, loss : 94.00418090820312\n",
      "iteration : 223, loss : 93.95503234863281\n",
      "iteration : 224, loss : 93.90602111816406\n",
      "iteration : 225, loss : 93.85717010498047\n",
      "iteration : 226, loss : 93.80846405029297\n",
      "iteration : 227, loss : 93.7599105834961\n",
      "iteration : 228, loss : 93.71150207519531\n",
      "iteration : 229, loss : 93.66324615478516\n",
      "iteration : 230, loss : 93.61514282226562\n",
      "iteration : 231, loss : 93.56717681884766\n",
      "iteration : 232, loss : 93.51935577392578\n",
      "iteration : 233, loss : 93.47168731689453\n",
      "iteration : 234, loss : 93.42416381835938\n",
      "iteration : 235, loss : 93.37679290771484\n",
      "iteration : 236, loss : 93.32954406738281\n",
      "iteration : 237, loss : 93.28246307373047\n",
      "iteration : 238, loss : 93.23550415039062\n",
      "iteration : 239, loss : 93.18871307373047\n",
      "iteration : 240, loss : 93.14203643798828\n",
      "iteration : 241, loss : 93.09552001953125\n",
      "iteration : 242, loss : 93.04913330078125\n",
      "iteration : 243, loss : 93.00291442871094\n",
      "iteration : 244, loss : 92.95680236816406\n",
      "iteration : 245, loss : 92.91085052490234\n",
      "iteration : 246, loss : 92.86505126953125\n",
      "iteration : 247, loss : 92.81937408447266\n",
      "iteration : 248, loss : 92.77384185791016\n",
      "iteration : 249, loss : 92.72843933105469\n",
      "iteration : 250, loss : 92.68317413330078\n",
      "iteration : 251, loss : 92.6380615234375\n",
      "iteration : 252, loss : 92.59308624267578\n",
      "iteration : 253, loss : 92.54823303222656\n",
      "iteration : 254, loss : 92.50350952148438\n",
      "iteration : 255, loss : 92.45893859863281\n",
      "iteration : 256, loss : 92.41451263427734\n",
      "iteration : 257, loss : 92.37020874023438\n",
      "iteration : 258, loss : 92.32604217529297\n",
      "iteration : 259, loss : 92.2820053100586\n",
      "iteration : 260, loss : 92.23809814453125\n",
      "iteration : 261, loss : 92.19432067871094\n",
      "iteration : 262, loss : 92.15068817138672\n",
      "iteration : 263, loss : 92.10719299316406\n",
      "iteration : 264, loss : 92.06382751464844\n",
      "iteration : 265, loss : 92.02059173583984\n",
      "iteration : 266, loss : 91.97747802734375\n",
      "iteration : 267, loss : 91.93449401855469\n",
      "iteration : 268, loss : 91.89166259765625\n",
      "iteration : 269, loss : 91.84894561767578\n",
      "iteration : 270, loss : 91.80635833740234\n",
      "iteration : 271, loss : 91.76388549804688\n",
      "iteration : 272, loss : 91.72156524658203\n",
      "iteration : 273, loss : 91.67938232421875\n",
      "iteration : 274, loss : 91.63729095458984\n",
      "iteration : 275, loss : 91.59535217285156\n",
      "iteration : 276, loss : 91.55353546142578\n",
      "iteration : 277, loss : 91.51183319091797\n",
      "iteration : 278, loss : 91.47027587890625\n",
      "iteration : 279, loss : 91.4288330078125\n",
      "iteration : 280, loss : 91.38751220703125\n",
      "iteration : 281, loss : 91.34632110595703\n",
      "iteration : 282, loss : 91.30525970458984\n",
      "iteration : 283, loss : 91.26432800292969\n",
      "iteration : 284, loss : 91.22349548339844\n",
      "iteration : 285, loss : 91.18282318115234\n",
      "iteration : 286, loss : 91.1422348022461\n",
      "iteration : 287, loss : 91.1017837524414\n",
      "iteration : 288, loss : 91.06145477294922\n",
      "iteration : 289, loss : 91.02125549316406\n",
      "iteration : 290, loss : 90.9811782836914\n",
      "iteration : 291, loss : 90.94120788574219\n",
      "iteration : 292, loss : 90.90137481689453\n",
      "iteration : 293, loss : 90.86164093017578\n",
      "iteration : 294, loss : 90.8220443725586\n",
      "iteration : 295, loss : 90.78255462646484\n",
      "iteration : 296, loss : 90.74319458007812\n",
      "iteration : 297, loss : 90.70394134521484\n",
      "iteration : 298, loss : 90.66482543945312\n",
      "iteration : 299, loss : 90.62580108642578\n",
      "iteration : 300, loss : 90.58690643310547\n",
      "iteration : 301, loss : 90.54813385009766\n",
      "iteration : 302, loss : 90.50948333740234\n",
      "iteration : 303, loss : 90.47093963623047\n",
      "iteration : 304, loss : 90.43251037597656\n",
      "iteration : 305, loss : 90.3941879272461\n",
      "iteration : 306, loss : 90.35601043701172\n",
      "iteration : 307, loss : 90.31791687011719\n",
      "iteration : 308, loss : 90.27994537353516\n",
      "iteration : 309, loss : 90.24210357666016\n",
      "iteration : 310, loss : 90.20436096191406\n",
      "iteration : 311, loss : 90.16671752929688\n",
      "iteration : 312, loss : 90.12921142578125\n",
      "iteration : 313, loss : 90.0918197631836\n",
      "iteration : 314, loss : 90.05451965332031\n",
      "iteration : 315, loss : 90.01734924316406\n",
      "iteration : 316, loss : 89.98028564453125\n",
      "iteration : 317, loss : 89.94332885742188\n",
      "iteration : 318, loss : 89.90647888183594\n",
      "iteration : 319, loss : 89.86974334716797\n",
      "iteration : 320, loss : 89.83312225341797\n",
      "iteration : 321, loss : 89.79661560058594\n",
      "iteration : 322, loss : 89.76020050048828\n",
      "iteration : 323, loss : 89.72390747070312\n",
      "iteration : 324, loss : 89.68771362304688\n",
      "iteration : 325, loss : 89.6516342163086\n",
      "iteration : 326, loss : 89.61566925048828\n",
      "iteration : 327, loss : 89.57981872558594\n",
      "iteration : 328, loss : 89.5440444946289\n",
      "iteration : 329, loss : 89.50839233398438\n",
      "iteration : 330, loss : 89.47284698486328\n",
      "iteration : 331, loss : 89.43741607666016\n",
      "iteration : 332, loss : 89.40208435058594\n",
      "iteration : 333, loss : 89.36685180664062\n",
      "iteration : 334, loss : 89.33174896240234\n",
      "iteration : 335, loss : 89.29671478271484\n",
      "iteration : 336, loss : 89.26180267333984\n",
      "iteration : 337, loss : 89.22699737548828\n",
      "iteration : 338, loss : 89.1922836303711\n",
      "iteration : 339, loss : 89.15768432617188\n",
      "iteration : 340, loss : 89.1231918334961\n",
      "iteration : 341, loss : 89.08879089355469\n",
      "iteration : 342, loss : 89.05449676513672\n",
      "iteration : 343, loss : 89.02030944824219\n",
      "iteration : 344, loss : 88.98621368408203\n",
      "iteration : 345, loss : 88.95222473144531\n",
      "iteration : 346, loss : 88.91834259033203\n",
      "iteration : 347, loss : 88.88455200195312\n",
      "iteration : 348, loss : 88.85086822509766\n",
      "iteration : 349, loss : 88.81727600097656\n",
      "iteration : 350, loss : 88.7837905883789\n",
      "iteration : 351, loss : 88.75040435791016\n",
      "iteration : 352, loss : 88.71710205078125\n",
      "iteration : 353, loss : 88.68391418457031\n",
      "iteration : 354, loss : 88.65084075927734\n",
      "iteration : 355, loss : 88.61784362792969\n",
      "iteration : 356, loss : 88.58493041992188\n",
      "iteration : 357, loss : 88.55213928222656\n",
      "iteration : 358, loss : 88.51944732666016\n",
      "iteration : 359, loss : 88.4868392944336\n",
      "iteration : 360, loss : 88.45433807373047\n",
      "iteration : 361, loss : 88.42190551757812\n",
      "iteration : 362, loss : 88.38959503173828\n",
      "iteration : 363, loss : 88.35738372802734\n",
      "iteration : 364, loss : 88.32525634765625\n",
      "iteration : 365, loss : 88.29320526123047\n",
      "iteration : 366, loss : 88.26128387451172\n",
      "iteration : 367, loss : 88.22944641113281\n",
      "iteration : 368, loss : 88.19770050048828\n",
      "iteration : 369, loss : 88.1660385131836\n",
      "iteration : 370, loss : 88.1344985961914\n",
      "iteration : 371, loss : 88.10302734375\n",
      "iteration : 372, loss : 88.07164764404297\n",
      "iteration : 373, loss : 88.04036712646484\n",
      "iteration : 374, loss : 88.00917053222656\n",
      "iteration : 375, loss : 87.97807312011719\n",
      "iteration : 376, loss : 87.94707489013672\n",
      "iteration : 377, loss : 87.9161605834961\n",
      "iteration : 378, loss : 87.88533020019531\n",
      "iteration : 379, loss : 87.85459899902344\n",
      "iteration : 380, loss : 87.8239517211914\n",
      "iteration : 381, loss : 87.79339599609375\n",
      "iteration : 382, loss : 87.762939453125\n",
      "iteration : 383, loss : 87.73255157470703\n",
      "iteration : 384, loss : 87.7022705078125\n",
      "iteration : 385, loss : 87.67207336425781\n",
      "iteration : 386, loss : 87.64197540283203\n",
      "iteration : 387, loss : 87.61196899414062\n",
      "iteration : 388, loss : 87.58202362060547\n",
      "iteration : 389, loss : 87.55217742919922\n",
      "iteration : 390, loss : 87.52243041992188\n",
      "iteration : 391, loss : 87.49275970458984\n",
      "iteration : 392, loss : 87.46318054199219\n",
      "iteration : 393, loss : 87.43367767333984\n",
      "iteration : 394, loss : 87.4042739868164\n",
      "iteration : 395, loss : 87.37495422363281\n",
      "iteration : 396, loss : 87.3457260131836\n",
      "iteration : 397, loss : 87.31656646728516\n",
      "iteration : 398, loss : 87.28751373291016\n",
      "iteration : 399, loss : 87.25852966308594\n",
      "iteration : 400, loss : 87.22962951660156\n",
      "iteration : 401, loss : 87.20081329345703\n",
      "iteration : 402, loss : 87.17208862304688\n",
      "iteration : 403, loss : 87.14344024658203\n",
      "iteration : 404, loss : 87.11487579345703\n",
      "iteration : 405, loss : 87.08641052246094\n",
      "iteration : 406, loss : 87.05801391601562\n",
      "iteration : 407, loss : 87.02971649169922\n",
      "iteration : 408, loss : 87.00146484375\n",
      "iteration : 409, loss : 86.97333526611328\n",
      "iteration : 410, loss : 86.94526672363281\n",
      "iteration : 411, loss : 86.91728973388672\n",
      "iteration : 412, loss : 86.8893814086914\n",
      "iteration : 413, loss : 86.86156463623047\n",
      "iteration : 414, loss : 86.83383178710938\n",
      "iteration : 415, loss : 86.80615997314453\n",
      "iteration : 416, loss : 86.77859497070312\n",
      "iteration : 417, loss : 86.75109100341797\n",
      "iteration : 418, loss : 86.72367095947266\n",
      "iteration : 419, loss : 86.69632720947266\n",
      "iteration : 420, loss : 86.66908264160156\n",
      "iteration : 421, loss : 86.64188385009766\n",
      "iteration : 422, loss : 86.61479187011719\n",
      "iteration : 423, loss : 86.5877685546875\n",
      "iteration : 424, loss : 86.5608139038086\n",
      "iteration : 425, loss : 86.53395080566406\n",
      "iteration : 426, loss : 86.50716400146484\n",
      "iteration : 427, loss : 86.48046112060547\n",
      "iteration : 428, loss : 86.45382690429688\n",
      "iteration : 429, loss : 86.42726135253906\n",
      "iteration : 430, loss : 86.40079498291016\n",
      "iteration : 431, loss : 86.37438201904297\n",
      "iteration : 432, loss : 86.34805297851562\n",
      "iteration : 433, loss : 86.3218002319336\n",
      "iteration : 434, loss : 86.2956314086914\n",
      "iteration : 435, loss : 86.26952362060547\n",
      "iteration : 436, loss : 86.2435073852539\n",
      "iteration : 437, loss : 86.21755981445312\n",
      "iteration : 438, loss : 86.1916732788086\n",
      "iteration : 439, loss : 86.16588592529297\n",
      "iteration : 440, loss : 86.14015197753906\n",
      "iteration : 441, loss : 86.11450958251953\n",
      "iteration : 442, loss : 86.08893585205078\n",
      "iteration : 443, loss : 86.06341552734375\n",
      "iteration : 444, loss : 86.03799438476562\n",
      "iteration : 445, loss : 86.01262664794922\n",
      "iteration : 446, loss : 85.98735809326172\n",
      "iteration : 447, loss : 85.96214294433594\n",
      "iteration : 448, loss : 85.93699645996094\n",
      "iteration : 449, loss : 85.91192626953125\n",
      "iteration : 450, loss : 85.88693237304688\n",
      "iteration : 451, loss : 85.86199951171875\n",
      "iteration : 452, loss : 85.837158203125\n",
      "iteration : 453, loss : 85.81238555908203\n",
      "iteration : 454, loss : 85.78765869140625\n",
      "iteration : 455, loss : 85.76303100585938\n",
      "iteration : 456, loss : 85.73845672607422\n",
      "iteration : 457, loss : 85.7139663696289\n",
      "iteration : 458, loss : 85.68953704833984\n",
      "iteration : 459, loss : 85.6651840209961\n",
      "iteration : 460, loss : 85.64088439941406\n",
      "iteration : 461, loss : 85.61666870117188\n",
      "iteration : 462, loss : 85.5925064086914\n",
      "iteration : 463, loss : 85.56842803955078\n",
      "iteration : 464, loss : 85.54440307617188\n",
      "iteration : 465, loss : 85.5204849243164\n",
      "iteration : 466, loss : 85.49659729003906\n",
      "iteration : 467, loss : 85.47280883789062\n",
      "iteration : 468, loss : 85.44905853271484\n",
      "iteration : 469, loss : 85.42537689208984\n",
      "iteration : 470, loss : 85.40177154541016\n",
      "iteration : 471, loss : 85.37824249267578\n",
      "iteration : 472, loss : 85.35476684570312\n",
      "iteration : 473, loss : 85.33136749267578\n",
      "iteration : 474, loss : 85.30802917480469\n",
      "iteration : 475, loss : 85.28475952148438\n",
      "iteration : 476, loss : 85.26155853271484\n",
      "iteration : 477, loss : 85.2384033203125\n",
      "iteration : 478, loss : 85.21534729003906\n",
      "iteration : 479, loss : 85.19233703613281\n",
      "iteration : 480, loss : 85.16938781738281\n",
      "iteration : 481, loss : 85.14651489257812\n",
      "iteration : 482, loss : 85.12370300292969\n",
      "iteration : 483, loss : 85.1009521484375\n",
      "iteration : 484, loss : 85.0782699584961\n",
      "iteration : 485, loss : 85.0556411743164\n",
      "iteration : 486, loss : 85.03309631347656\n",
      "iteration : 487, loss : 85.0105972290039\n",
      "iteration : 488, loss : 84.98817443847656\n",
      "iteration : 489, loss : 84.96580505371094\n",
      "iteration : 490, loss : 84.9435043334961\n",
      "iteration : 491, loss : 84.92127227783203\n",
      "iteration : 492, loss : 84.89910125732422\n",
      "iteration : 493, loss : 84.87698364257812\n",
      "iteration : 494, loss : 84.85491943359375\n",
      "iteration : 495, loss : 84.83293914794922\n",
      "iteration : 496, loss : 84.81101989746094\n",
      "iteration : 497, loss : 84.78914642333984\n",
      "iteration : 498, loss : 84.7673568725586\n",
      "iteration : 499, loss : 84.74559783935547\n",
      "iteration : 500, loss : 84.72392272949219\n",
      "iteration : 501, loss : 84.70230102539062\n",
      "iteration : 502, loss : 84.68074035644531\n",
      "iteration : 503, loss : 84.65924835205078\n",
      "iteration : 504, loss : 84.63780975341797\n",
      "iteration : 505, loss : 84.6164321899414\n",
      "iteration : 506, loss : 84.59510040283203\n",
      "iteration : 507, loss : 84.57384490966797\n",
      "iteration : 508, loss : 84.55265808105469\n",
      "iteration : 509, loss : 84.53150939941406\n",
      "iteration : 510, loss : 84.51042938232422\n",
      "iteration : 511, loss : 84.48941040039062\n",
      "iteration : 512, loss : 84.46844482421875\n",
      "iteration : 513, loss : 84.44754028320312\n",
      "iteration : 514, loss : 84.42669677734375\n",
      "iteration : 515, loss : 84.40589904785156\n",
      "iteration : 516, loss : 84.38517761230469\n",
      "iteration : 517, loss : 84.36450958251953\n",
      "iteration : 518, loss : 84.3438949584961\n",
      "iteration : 519, loss : 84.32334899902344\n",
      "iteration : 520, loss : 84.3028335571289\n",
      "iteration : 521, loss : 84.28240203857422\n",
      "iteration : 522, loss : 84.26200866699219\n",
      "iteration : 523, loss : 84.2416763305664\n",
      "iteration : 524, loss : 84.22142028808594\n",
      "iteration : 525, loss : 84.2011947631836\n",
      "iteration : 526, loss : 84.18103790283203\n",
      "iteration : 527, loss : 84.16092681884766\n",
      "iteration : 528, loss : 84.14088439941406\n",
      "iteration : 529, loss : 84.12089538574219\n",
      "iteration : 530, loss : 84.10095977783203\n",
      "iteration : 531, loss : 84.08108520507812\n",
      "iteration : 532, loss : 84.0612564086914\n",
      "iteration : 533, loss : 84.0414810180664\n",
      "iteration : 534, loss : 84.02176666259766\n",
      "iteration : 535, loss : 84.0020980834961\n",
      "iteration : 536, loss : 83.98248291015625\n",
      "iteration : 537, loss : 83.96294403076172\n",
      "iteration : 538, loss : 83.94344329833984\n",
      "iteration : 539, loss : 83.92399597167969\n",
      "iteration : 540, loss : 83.90460968017578\n",
      "iteration : 541, loss : 83.88526916503906\n",
      "iteration : 542, loss : 83.86598205566406\n",
      "iteration : 543, loss : 83.84676361083984\n",
      "iteration : 544, loss : 83.82757568359375\n",
      "iteration : 545, loss : 83.80845642089844\n",
      "iteration : 546, loss : 83.78937530517578\n",
      "iteration : 547, loss : 83.77035522460938\n",
      "iteration : 548, loss : 83.75138854980469\n",
      "iteration : 549, loss : 83.73246765136719\n",
      "iteration : 550, loss : 83.71360778808594\n",
      "iteration : 551, loss : 83.69479370117188\n",
      "iteration : 552, loss : 83.67603302001953\n",
      "iteration : 553, loss : 83.6573257446289\n",
      "iteration : 554, loss : 83.638671875\n",
      "iteration : 555, loss : 83.62004852294922\n",
      "iteration : 556, loss : 83.60150909423828\n",
      "iteration : 557, loss : 83.58300018310547\n",
      "iteration : 558, loss : 83.56454467773438\n",
      "iteration : 559, loss : 83.546142578125\n",
      "iteration : 560, loss : 83.52779388427734\n",
      "iteration : 561, loss : 83.5094985961914\n",
      "iteration : 562, loss : 83.4912338256836\n",
      "iteration : 563, loss : 83.47303009033203\n",
      "iteration : 564, loss : 83.45487213134766\n",
      "iteration : 565, loss : 83.436767578125\n",
      "iteration : 566, loss : 83.41870880126953\n",
      "iteration : 567, loss : 83.40070343017578\n",
      "iteration : 568, loss : 83.38274383544922\n",
      "iteration : 569, loss : 83.36483764648438\n",
      "iteration : 570, loss : 83.34699249267578\n",
      "iteration : 571, loss : 83.32917785644531\n",
      "iteration : 572, loss : 83.3114013671875\n",
      "iteration : 573, loss : 83.293701171875\n",
      "iteration : 574, loss : 83.2760238647461\n",
      "iteration : 575, loss : 83.25840759277344\n",
      "iteration : 576, loss : 83.2408447265625\n",
      "iteration : 577, loss : 83.22332000732422\n",
      "iteration : 578, loss : 83.20584106445312\n",
      "iteration : 579, loss : 83.18841552734375\n",
      "iteration : 580, loss : 83.1710433959961\n",
      "iteration : 581, loss : 83.15370178222656\n",
      "iteration : 582, loss : 83.13642120361328\n",
      "iteration : 583, loss : 83.11917877197266\n",
      "iteration : 584, loss : 83.10198211669922\n",
      "iteration : 585, loss : 83.0848388671875\n",
      "iteration : 586, loss : 83.06774139404297\n",
      "iteration : 587, loss : 83.0506820678711\n",
      "iteration : 588, loss : 83.0336685180664\n",
      "iteration : 589, loss : 83.01670837402344\n",
      "iteration : 590, loss : 82.99979400634766\n",
      "iteration : 591, loss : 82.98292541503906\n",
      "iteration : 592, loss : 82.9660873413086\n",
      "iteration : 593, loss : 82.94931030273438\n",
      "iteration : 594, loss : 82.93257141113281\n",
      "iteration : 595, loss : 82.91588592529297\n",
      "iteration : 596, loss : 82.89923095703125\n",
      "iteration : 597, loss : 82.88262939453125\n",
      "iteration : 598, loss : 82.86608123779297\n",
      "iteration : 599, loss : 82.84957122802734\n",
      "iteration : 600, loss : 82.83309173583984\n",
      "iteration : 601, loss : 82.8166732788086\n",
      "iteration : 602, loss : 82.80028533935547\n",
      "iteration : 603, loss : 82.78395080566406\n",
      "iteration : 604, loss : 82.76766204833984\n",
      "iteration : 605, loss : 82.75140380859375\n",
      "iteration : 606, loss : 82.73518371582031\n",
      "iteration : 607, loss : 82.71902465820312\n",
      "iteration : 608, loss : 82.70291137695312\n",
      "iteration : 609, loss : 82.68683624267578\n",
      "iteration : 610, loss : 82.67079162597656\n",
      "iteration : 611, loss : 82.6548080444336\n",
      "iteration : 612, loss : 82.63885498046875\n",
      "iteration : 613, loss : 82.62294006347656\n",
      "iteration : 614, loss : 82.6070785522461\n",
      "iteration : 615, loss : 82.59126281738281\n",
      "iteration : 616, loss : 82.57547760009766\n",
      "iteration : 617, loss : 82.55973815917969\n",
      "iteration : 618, loss : 82.54405212402344\n",
      "iteration : 619, loss : 82.52838897705078\n",
      "iteration : 620, loss : 82.51277923583984\n",
      "iteration : 621, loss : 82.49720001220703\n",
      "iteration : 622, loss : 82.4816665649414\n",
      "iteration : 623, loss : 82.46617889404297\n",
      "iteration : 624, loss : 82.45072937011719\n",
      "iteration : 625, loss : 82.43531799316406\n",
      "iteration : 626, loss : 82.4199447631836\n",
      "iteration : 627, loss : 82.40461730957031\n",
      "iteration : 628, loss : 82.38932800292969\n",
      "iteration : 629, loss : 82.37408447265625\n",
      "iteration : 630, loss : 82.35887908935547\n",
      "iteration : 631, loss : 82.34371185302734\n",
      "iteration : 632, loss : 82.3285903930664\n",
      "iteration : 633, loss : 82.31349182128906\n",
      "iteration : 634, loss : 82.2984390258789\n",
      "iteration : 635, loss : 82.2834243774414\n",
      "iteration : 636, loss : 82.26846313476562\n",
      "iteration : 637, loss : 82.25353240966797\n",
      "iteration : 638, loss : 82.2386474609375\n",
      "iteration : 639, loss : 82.22380065917969\n",
      "iteration : 640, loss : 82.208984375\n",
      "iteration : 641, loss : 82.19420623779297\n",
      "iteration : 642, loss : 82.17947387695312\n",
      "iteration : 643, loss : 82.16476440429688\n",
      "iteration : 644, loss : 82.15011596679688\n",
      "iteration : 645, loss : 82.13549041748047\n",
      "iteration : 646, loss : 82.12090301513672\n",
      "iteration : 647, loss : 82.10636901855469\n",
      "iteration : 648, loss : 82.09185028076172\n",
      "iteration : 649, loss : 82.07738494873047\n",
      "iteration : 650, loss : 82.06295776367188\n",
      "iteration : 651, loss : 82.0485610961914\n",
      "iteration : 652, loss : 82.0342025756836\n",
      "iteration : 653, loss : 82.01988220214844\n",
      "iteration : 654, loss : 82.00559997558594\n",
      "iteration : 655, loss : 81.9913558959961\n",
      "iteration : 656, loss : 81.9771499633789\n",
      "iteration : 657, loss : 81.96296691894531\n",
      "iteration : 658, loss : 81.9488296508789\n",
      "iteration : 659, loss : 81.93473815917969\n",
      "iteration : 660, loss : 81.92066955566406\n",
      "iteration : 661, loss : 81.90665435791016\n",
      "iteration : 662, loss : 81.89266204833984\n",
      "iteration : 663, loss : 81.87869262695312\n",
      "iteration : 664, loss : 81.86478424072266\n",
      "iteration : 665, loss : 81.85089111328125\n",
      "iteration : 666, loss : 81.83705139160156\n",
      "iteration : 667, loss : 81.82324981689453\n",
      "iteration : 668, loss : 81.80946350097656\n",
      "iteration : 669, loss : 81.79572296142578\n",
      "iteration : 670, loss : 81.78201293945312\n",
      "iteration : 671, loss : 81.76834869384766\n",
      "iteration : 672, loss : 81.75471496582031\n",
      "iteration : 673, loss : 81.7411117553711\n",
      "iteration : 674, loss : 81.72755432128906\n",
      "iteration : 675, loss : 81.71401977539062\n",
      "iteration : 676, loss : 81.70052337646484\n",
      "iteration : 677, loss : 81.68705749511719\n",
      "iteration : 678, loss : 81.67363739013672\n",
      "iteration : 679, loss : 81.66024780273438\n",
      "iteration : 680, loss : 81.64688110351562\n",
      "iteration : 681, loss : 81.63355255126953\n",
      "iteration : 682, loss : 81.6202621459961\n",
      "iteration : 683, loss : 81.60701751708984\n",
      "iteration : 684, loss : 81.59378051757812\n",
      "iteration : 685, loss : 81.58059692382812\n",
      "iteration : 686, loss : 81.56745147705078\n",
      "iteration : 687, loss : 81.5543212890625\n",
      "iteration : 688, loss : 81.54122161865234\n",
      "iteration : 689, loss : 81.52815246582031\n",
      "iteration : 690, loss : 81.51513671875\n",
      "iteration : 691, loss : 81.50215148925781\n",
      "iteration : 692, loss : 81.48919677734375\n",
      "iteration : 693, loss : 81.47627258300781\n",
      "iteration : 694, loss : 81.46337890625\n",
      "iteration : 695, loss : 81.45052337646484\n",
      "iteration : 696, loss : 81.43769073486328\n",
      "iteration : 697, loss : 81.4249038696289\n",
      "iteration : 698, loss : 81.4121322631836\n",
      "iteration : 699, loss : 81.39940643310547\n",
      "iteration : 700, loss : 81.38671112060547\n",
      "iteration : 701, loss : 81.3740463256836\n",
      "iteration : 702, loss : 81.36140441894531\n",
      "iteration : 703, loss : 81.34880065917969\n",
      "iteration : 704, loss : 81.33622741699219\n",
      "iteration : 705, loss : 81.32368469238281\n",
      "iteration : 706, loss : 81.31117248535156\n",
      "iteration : 707, loss : 81.29869842529297\n",
      "iteration : 708, loss : 81.28624725341797\n",
      "iteration : 709, loss : 81.27384185791016\n",
      "iteration : 710, loss : 81.2614517211914\n",
      "iteration : 711, loss : 81.24909973144531\n",
      "iteration : 712, loss : 81.23677062988281\n",
      "iteration : 713, loss : 81.22447967529297\n",
      "iteration : 714, loss : 81.21221923828125\n",
      "iteration : 715, loss : 81.19998168945312\n",
      "iteration : 716, loss : 81.18778991699219\n",
      "iteration : 717, loss : 81.17562103271484\n",
      "iteration : 718, loss : 81.1634750366211\n",
      "iteration : 719, loss : 81.15137481689453\n",
      "iteration : 720, loss : 81.13928985595703\n",
      "iteration : 721, loss : 81.12722778320312\n",
      "iteration : 722, loss : 81.1152114868164\n",
      "iteration : 723, loss : 81.10322570800781\n",
      "iteration : 724, loss : 81.09125518798828\n",
      "iteration : 725, loss : 81.07933807373047\n",
      "iteration : 726, loss : 81.06742858886719\n",
      "iteration : 727, loss : 81.05555725097656\n",
      "iteration : 728, loss : 81.04371643066406\n",
      "iteration : 729, loss : 81.03189086914062\n",
      "iteration : 730, loss : 81.02010345458984\n",
      "iteration : 731, loss : 81.00836181640625\n",
      "iteration : 732, loss : 80.99662780761719\n",
      "iteration : 733, loss : 80.98493194580078\n",
      "iteration : 734, loss : 80.97325897216797\n",
      "iteration : 735, loss : 80.96161651611328\n",
      "iteration : 736, loss : 80.95000457763672\n",
      "iteration : 737, loss : 80.93841552734375\n",
      "iteration : 738, loss : 80.92686462402344\n",
      "iteration : 739, loss : 80.91533660888672\n",
      "iteration : 740, loss : 80.9038314819336\n",
      "iteration : 741, loss : 80.8923568725586\n",
      "iteration : 742, loss : 80.88090515136719\n",
      "iteration : 743, loss : 80.86949920654297\n",
      "iteration : 744, loss : 80.85811614990234\n",
      "iteration : 745, loss : 80.84676361083984\n",
      "iteration : 746, loss : 80.83541870117188\n",
      "iteration : 747, loss : 80.82411193847656\n",
      "iteration : 748, loss : 80.81283569335938\n",
      "iteration : 749, loss : 80.80158996582031\n",
      "iteration : 750, loss : 80.79036712646484\n",
      "iteration : 751, loss : 80.77915954589844\n",
      "iteration : 752, loss : 80.76799774169922\n",
      "iteration : 753, loss : 80.7568588256836\n",
      "iteration : 754, loss : 80.7457275390625\n",
      "iteration : 755, loss : 80.7346420288086\n",
      "iteration : 756, loss : 80.72357940673828\n",
      "iteration : 757, loss : 80.7125473022461\n",
      "iteration : 758, loss : 80.70154571533203\n",
      "iteration : 759, loss : 80.69055938720703\n",
      "iteration : 760, loss : 80.67959594726562\n",
      "iteration : 761, loss : 80.66866302490234\n",
      "iteration : 762, loss : 80.65776062011719\n",
      "iteration : 763, loss : 80.64688873291016\n",
      "iteration : 764, loss : 80.63603210449219\n",
      "iteration : 765, loss : 80.62520599365234\n",
      "iteration : 766, loss : 80.61441040039062\n",
      "iteration : 767, loss : 80.60363006591797\n",
      "iteration : 768, loss : 80.59288787841797\n",
      "iteration : 769, loss : 80.58216094970703\n",
      "iteration : 770, loss : 80.57147979736328\n",
      "iteration : 771, loss : 80.56079864501953\n",
      "iteration : 772, loss : 80.55015563964844\n",
      "iteration : 773, loss : 80.5395278930664\n",
      "iteration : 774, loss : 80.52893829345703\n",
      "iteration : 775, loss : 80.51835632324219\n",
      "iteration : 776, loss : 80.5078125\n",
      "iteration : 777, loss : 80.49729919433594\n",
      "iteration : 778, loss : 80.48680114746094\n",
      "iteration : 779, loss : 80.47633361816406\n",
      "iteration : 780, loss : 80.46588897705078\n",
      "iteration : 781, loss : 80.4554672241211\n",
      "iteration : 782, loss : 80.445068359375\n",
      "iteration : 783, loss : 80.43470001220703\n",
      "iteration : 784, loss : 80.42434692382812\n",
      "iteration : 785, loss : 80.41402435302734\n",
      "iteration : 786, loss : 80.40371704101562\n",
      "iteration : 787, loss : 80.3934555053711\n",
      "iteration : 788, loss : 80.3832015991211\n",
      "iteration : 789, loss : 80.37297058105469\n",
      "iteration : 790, loss : 80.36276245117188\n",
      "iteration : 791, loss : 80.35258483886719\n",
      "iteration : 792, loss : 80.34243774414062\n",
      "iteration : 793, loss : 80.33230590820312\n",
      "iteration : 794, loss : 80.32218933105469\n",
      "iteration : 795, loss : 80.31210327148438\n",
      "iteration : 796, loss : 80.30204772949219\n",
      "iteration : 797, loss : 80.2920150756836\n",
      "iteration : 798, loss : 80.28199005126953\n",
      "iteration : 799, loss : 80.27201080322266\n",
      "iteration : 800, loss : 80.26203918457031\n",
      "iteration : 801, loss : 80.2520980834961\n",
      "iteration : 802, loss : 80.24217224121094\n",
      "iteration : 803, loss : 80.2322769165039\n",
      "iteration : 804, loss : 80.22239685058594\n",
      "iteration : 805, loss : 80.21255493164062\n",
      "iteration : 806, loss : 80.20270538330078\n",
      "iteration : 807, loss : 80.19291687011719\n",
      "iteration : 808, loss : 80.1831283569336\n",
      "iteration : 809, loss : 80.1733627319336\n",
      "iteration : 810, loss : 80.16361236572266\n",
      "iteration : 811, loss : 80.15390014648438\n",
      "iteration : 812, loss : 80.14421081542969\n",
      "iteration : 813, loss : 80.13453674316406\n",
      "iteration : 814, loss : 80.12488555908203\n",
      "iteration : 815, loss : 80.1152572631836\n",
      "iteration : 816, loss : 80.10565185546875\n",
      "iteration : 817, loss : 80.0960693359375\n",
      "iteration : 818, loss : 80.08650207519531\n",
      "iteration : 819, loss : 80.07696533203125\n",
      "iteration : 820, loss : 80.06743621826172\n",
      "iteration : 821, loss : 80.05795288085938\n",
      "iteration : 822, loss : 80.04847717285156\n",
      "iteration : 823, loss : 80.03902435302734\n",
      "iteration : 824, loss : 80.02958679199219\n",
      "iteration : 825, loss : 80.02017974853516\n",
      "iteration : 826, loss : 80.01078796386719\n",
      "iteration : 827, loss : 80.00141143798828\n",
      "iteration : 828, loss : 79.9920654296875\n",
      "iteration : 829, loss : 79.98275756835938\n",
      "iteration : 830, loss : 79.97344207763672\n",
      "iteration : 831, loss : 79.96415710449219\n",
      "iteration : 832, loss : 79.95488739013672\n",
      "iteration : 833, loss : 79.9456558227539\n",
      "iteration : 834, loss : 79.93641662597656\n",
      "iteration : 835, loss : 79.9272232055664\n",
      "iteration : 836, loss : 79.91804504394531\n",
      "iteration : 837, loss : 79.90888977050781\n",
      "iteration : 838, loss : 79.8997573852539\n",
      "iteration : 839, loss : 79.890625\n",
      "iteration : 840, loss : 79.88153076171875\n",
      "iteration : 841, loss : 79.87245178222656\n",
      "iteration : 842, loss : 79.86338806152344\n",
      "iteration : 843, loss : 79.85435485839844\n",
      "iteration : 844, loss : 79.8453369140625\n",
      "iteration : 845, loss : 79.83635711669922\n",
      "iteration : 846, loss : 79.82738494873047\n",
      "iteration : 847, loss : 79.81841278076172\n",
      "iteration : 848, loss : 79.80948638916016\n",
      "iteration : 849, loss : 79.8005599975586\n",
      "iteration : 850, loss : 79.79166412353516\n",
      "iteration : 851, loss : 79.78279113769531\n",
      "iteration : 852, loss : 79.77393341064453\n",
      "iteration : 853, loss : 79.76509094238281\n",
      "iteration : 854, loss : 79.75627136230469\n",
      "iteration : 855, loss : 79.74746704101562\n",
      "iteration : 856, loss : 79.73870086669922\n",
      "iteration : 857, loss : 79.72992706298828\n",
      "iteration : 858, loss : 79.72119140625\n",
      "iteration : 859, loss : 79.71246337890625\n",
      "iteration : 860, loss : 79.7037582397461\n",
      "iteration : 861, loss : 79.6950912475586\n",
      "iteration : 862, loss : 79.68641662597656\n",
      "iteration : 863, loss : 79.67777252197266\n",
      "iteration : 864, loss : 79.66915130615234\n",
      "iteration : 865, loss : 79.66053771972656\n",
      "iteration : 866, loss : 79.65194702148438\n",
      "iteration : 867, loss : 79.64337921142578\n",
      "iteration : 868, loss : 79.63482666015625\n",
      "iteration : 869, loss : 79.62629699707031\n",
      "iteration : 870, loss : 79.61778259277344\n",
      "iteration : 871, loss : 79.6092758178711\n",
      "iteration : 872, loss : 79.6008071899414\n",
      "iteration : 873, loss : 79.59233856201172\n",
      "iteration : 874, loss : 79.58390045166016\n",
      "iteration : 875, loss : 79.57547760009766\n",
      "iteration : 876, loss : 79.56707000732422\n",
      "iteration : 877, loss : 79.5586929321289\n",
      "iteration : 878, loss : 79.55030822753906\n",
      "iteration : 879, loss : 79.54196166992188\n",
      "iteration : 880, loss : 79.53363800048828\n",
      "iteration : 881, loss : 79.52532196044922\n",
      "iteration : 882, loss : 79.51702880859375\n",
      "iteration : 883, loss : 79.50874328613281\n",
      "iteration : 884, loss : 79.50048828125\n",
      "iteration : 885, loss : 79.49224853515625\n",
      "iteration : 886, loss : 79.48401641845703\n",
      "iteration : 887, loss : 79.47581481933594\n",
      "iteration : 888, loss : 79.46761322021484\n",
      "iteration : 889, loss : 79.4594497680664\n",
      "iteration : 890, loss : 79.4512939453125\n",
      "iteration : 891, loss : 79.4431381225586\n",
      "iteration : 892, loss : 79.43502807617188\n",
      "iteration : 893, loss : 79.42693328857422\n",
      "iteration : 894, loss : 79.4188461303711\n",
      "iteration : 895, loss : 79.4107666015625\n",
      "iteration : 896, loss : 79.40272521972656\n",
      "iteration : 897, loss : 79.39468383789062\n",
      "iteration : 898, loss : 79.38666534423828\n",
      "iteration : 899, loss : 79.378662109375\n",
      "iteration : 900, loss : 79.37068176269531\n",
      "iteration : 901, loss : 79.36270904541016\n",
      "iteration : 902, loss : 79.35475158691406\n",
      "iteration : 903, loss : 79.3468246459961\n",
      "iteration : 904, loss : 79.33890533447266\n",
      "iteration : 905, loss : 79.33100891113281\n",
      "iteration : 906, loss : 79.3231201171875\n",
      "iteration : 907, loss : 79.31524658203125\n",
      "iteration : 908, loss : 79.3073959350586\n",
      "iteration : 909, loss : 79.29956817626953\n",
      "iteration : 910, loss : 79.291748046875\n",
      "iteration : 911, loss : 79.28395080566406\n",
      "iteration : 912, loss : 79.27615356445312\n",
      "iteration : 913, loss : 79.26839447021484\n",
      "iteration : 914, loss : 79.26063537597656\n",
      "iteration : 915, loss : 79.25289916992188\n",
      "iteration : 916, loss : 79.24517059326172\n",
      "iteration : 917, loss : 79.23747253417969\n",
      "iteration : 918, loss : 79.22978210449219\n",
      "iteration : 919, loss : 79.22210693359375\n",
      "iteration : 920, loss : 79.21444702148438\n",
      "iteration : 921, loss : 79.2068099975586\n",
      "iteration : 922, loss : 79.19917297363281\n",
      "iteration : 923, loss : 79.19156646728516\n",
      "iteration : 924, loss : 79.1839828491211\n",
      "iteration : 925, loss : 79.1763916015625\n",
      "iteration : 926, loss : 79.1688232421875\n",
      "iteration : 927, loss : 79.16128540039062\n",
      "iteration : 928, loss : 79.15373992919922\n",
      "iteration : 929, loss : 79.14622497558594\n",
      "iteration : 930, loss : 79.13873291015625\n",
      "iteration : 931, loss : 79.13124084472656\n",
      "iteration : 932, loss : 79.12376403808594\n",
      "iteration : 933, loss : 79.11629486083984\n",
      "iteration : 934, loss : 79.1088638305664\n",
      "iteration : 935, loss : 79.1014404296875\n",
      "iteration : 936, loss : 79.09402465820312\n",
      "iteration : 937, loss : 79.08663177490234\n",
      "iteration : 938, loss : 79.0792465209961\n",
      "iteration : 939, loss : 79.07188415527344\n",
      "iteration : 940, loss : 79.06453704833984\n",
      "iteration : 941, loss : 79.05718994140625\n",
      "iteration : 942, loss : 79.04987335205078\n",
      "iteration : 943, loss : 79.04256439208984\n",
      "iteration : 944, loss : 79.03527069091797\n",
      "iteration : 945, loss : 79.02799224853516\n",
      "iteration : 946, loss : 79.0207290649414\n",
      "iteration : 947, loss : 79.01348114013672\n",
      "iteration : 948, loss : 79.0062484741211\n",
      "iteration : 949, loss : 78.99901580810547\n",
      "iteration : 950, loss : 78.99181365966797\n",
      "iteration : 951, loss : 78.98462677001953\n",
      "iteration : 952, loss : 78.97744750976562\n",
      "iteration : 953, loss : 78.97028350830078\n",
      "iteration : 954, loss : 78.96312713623047\n",
      "iteration : 955, loss : 78.95599365234375\n",
      "iteration : 956, loss : 78.94888305664062\n",
      "iteration : 957, loss : 78.94178009033203\n",
      "iteration : 958, loss : 78.93467712402344\n",
      "iteration : 959, loss : 78.92760467529297\n",
      "iteration : 960, loss : 78.92053985595703\n",
      "iteration : 961, loss : 78.91349029541016\n",
      "iteration : 962, loss : 78.90644836425781\n",
      "iteration : 963, loss : 78.8994369506836\n",
      "iteration : 964, loss : 78.89242553710938\n",
      "iteration : 965, loss : 78.88542938232422\n",
      "iteration : 966, loss : 78.8784408569336\n",
      "iteration : 967, loss : 78.87147521972656\n",
      "iteration : 968, loss : 78.86451721191406\n",
      "iteration : 969, loss : 78.85757446289062\n",
      "iteration : 970, loss : 78.85064697265625\n",
      "iteration : 971, loss : 78.84373474121094\n",
      "iteration : 972, loss : 78.83683013916016\n",
      "iteration : 973, loss : 78.82994842529297\n",
      "iteration : 974, loss : 78.82306671142578\n",
      "iteration : 975, loss : 78.81620788574219\n",
      "iteration : 976, loss : 78.80937194824219\n",
      "iteration : 977, loss : 78.80253601074219\n",
      "iteration : 978, loss : 78.79570770263672\n",
      "iteration : 979, loss : 78.78890228271484\n",
      "iteration : 980, loss : 78.78211212158203\n",
      "iteration : 981, loss : 78.77532196044922\n",
      "iteration : 982, loss : 78.76856231689453\n",
      "iteration : 983, loss : 78.76181030273438\n",
      "iteration : 984, loss : 78.75505828857422\n",
      "iteration : 985, loss : 78.74832916259766\n",
      "iteration : 986, loss : 78.74162292480469\n",
      "iteration : 987, loss : 78.73490905761719\n",
      "iteration : 988, loss : 78.72821044921875\n",
      "iteration : 989, loss : 78.7215347290039\n",
      "iteration : 990, loss : 78.71487426757812\n",
      "iteration : 991, loss : 78.70822143554688\n",
      "iteration : 992, loss : 78.70157623291016\n",
      "iteration : 993, loss : 78.69495391845703\n",
      "iteration : 994, loss : 78.6883316040039\n",
      "iteration : 995, loss : 78.68173217773438\n",
      "iteration : 996, loss : 78.67514038085938\n",
      "iteration : 997, loss : 78.66856384277344\n",
      "iteration : 998, loss : 78.66200256347656\n",
      "iteration : 999, loss : 78.65544891357422\n",
      "iteration : 1000, loss : 78.6489028930664\n",
      "iteration : 1001, loss : 78.64238739013672\n",
      "iteration : 1002, loss : 78.6358642578125\n",
      "iteration : 1003, loss : 78.62935638427734\n",
      "iteration : 1004, loss : 78.62286376953125\n",
      "iteration : 1005, loss : 78.61637878417969\n",
      "iteration : 1006, loss : 78.60991668701172\n",
      "iteration : 1007, loss : 78.60345458984375\n",
      "iteration : 1008, loss : 78.5970230102539\n",
      "iteration : 1009, loss : 78.59058380126953\n",
      "iteration : 1010, loss : 78.58415985107422\n",
      "iteration : 1011, loss : 78.57775115966797\n",
      "iteration : 1012, loss : 78.57135772705078\n",
      "iteration : 1013, loss : 78.56497955322266\n",
      "iteration : 1014, loss : 78.55860137939453\n",
      "iteration : 1015, loss : 78.55223083496094\n",
      "iteration : 1016, loss : 78.54588317871094\n",
      "iteration : 1017, loss : 78.53955841064453\n",
      "iteration : 1018, loss : 78.5332260131836\n",
      "iteration : 1019, loss : 78.52690887451172\n",
      "iteration : 1020, loss : 78.52061462402344\n",
      "iteration : 1021, loss : 78.51432037353516\n",
      "iteration : 1022, loss : 78.50802612304688\n",
      "iteration : 1023, loss : 78.50176239013672\n",
      "iteration : 1024, loss : 78.4955062866211\n",
      "iteration : 1025, loss : 78.48927307128906\n",
      "iteration : 1026, loss : 78.4830322265625\n",
      "iteration : 1027, loss : 78.47679138183594\n",
      "iteration : 1028, loss : 78.47059631347656\n",
      "iteration : 1029, loss : 78.46439361572266\n",
      "iteration : 1030, loss : 78.45819854736328\n",
      "iteration : 1031, loss : 78.45201873779297\n",
      "iteration : 1032, loss : 78.44586181640625\n",
      "iteration : 1033, loss : 78.439697265625\n",
      "iteration : 1034, loss : 78.43355560302734\n",
      "iteration : 1035, loss : 78.42741394042969\n",
      "iteration : 1036, loss : 78.42130279541016\n",
      "iteration : 1037, loss : 78.41517639160156\n",
      "iteration : 1038, loss : 78.40907287597656\n",
      "iteration : 1039, loss : 78.40299224853516\n",
      "iteration : 1040, loss : 78.39691925048828\n",
      "iteration : 1041, loss : 78.39083862304688\n",
      "iteration : 1042, loss : 78.38478088378906\n",
      "iteration : 1043, loss : 78.37874603271484\n",
      "iteration : 1044, loss : 78.3727035522461\n",
      "iteration : 1045, loss : 78.36666870117188\n",
      "iteration : 1046, loss : 78.36066436767578\n",
      "iteration : 1047, loss : 78.35464477539062\n",
      "iteration : 1048, loss : 78.34866333007812\n",
      "iteration : 1049, loss : 78.3426742553711\n",
      "iteration : 1050, loss : 78.33670806884766\n",
      "iteration : 1051, loss : 78.33074951171875\n",
      "iteration : 1052, loss : 78.32479095458984\n",
      "iteration : 1053, loss : 78.31885528564453\n",
      "iteration : 1054, loss : 78.31291961669922\n",
      "iteration : 1055, loss : 78.30699920654297\n",
      "iteration : 1056, loss : 78.30109405517578\n",
      "iteration : 1057, loss : 78.2951889038086\n",
      "iteration : 1058, loss : 78.28929901123047\n",
      "iteration : 1059, loss : 78.28341674804688\n",
      "iteration : 1060, loss : 78.27754211425781\n",
      "iteration : 1061, loss : 78.27168273925781\n",
      "iteration : 1062, loss : 78.26583862304688\n",
      "iteration : 1063, loss : 78.26000213623047\n",
      "iteration : 1064, loss : 78.2541732788086\n",
      "iteration : 1065, loss : 78.24834442138672\n",
      "iteration : 1066, loss : 78.24254608154297\n",
      "iteration : 1067, loss : 78.23675537109375\n",
      "iteration : 1068, loss : 78.23094940185547\n",
      "iteration : 1069, loss : 78.22518157958984\n",
      "iteration : 1070, loss : 78.21940612792969\n",
      "iteration : 1071, loss : 78.2136459350586\n",
      "iteration : 1072, loss : 78.20789337158203\n",
      "iteration : 1073, loss : 78.20214080810547\n",
      "iteration : 1074, loss : 78.19641876220703\n",
      "iteration : 1075, loss : 78.19070434570312\n",
      "iteration : 1076, loss : 78.18498229980469\n",
      "iteration : 1077, loss : 78.1792984008789\n",
      "iteration : 1078, loss : 78.17359924316406\n",
      "iteration : 1079, loss : 78.16792297363281\n",
      "iteration : 1080, loss : 78.16224670410156\n",
      "iteration : 1081, loss : 78.1565933227539\n",
      "iteration : 1082, loss : 78.15093231201172\n",
      "iteration : 1083, loss : 78.1452865600586\n",
      "iteration : 1084, loss : 78.13965606689453\n",
      "iteration : 1085, loss : 78.13401794433594\n",
      "iteration : 1086, loss : 78.12841033935547\n",
      "iteration : 1087, loss : 78.12281036376953\n",
      "iteration : 1088, loss : 78.11720275878906\n",
      "iteration : 1089, loss : 78.11161804199219\n",
      "iteration : 1090, loss : 78.10604858398438\n",
      "iteration : 1091, loss : 78.10047149658203\n",
      "iteration : 1092, loss : 78.09491729736328\n",
      "iteration : 1093, loss : 78.08936309814453\n",
      "iteration : 1094, loss : 78.08380889892578\n",
      "iteration : 1095, loss : 78.07829284667969\n",
      "iteration : 1096, loss : 78.07276916503906\n",
      "iteration : 1097, loss : 78.06725311279297\n",
      "iteration : 1098, loss : 78.0617446899414\n",
      "iteration : 1099, loss : 78.0562515258789\n",
      "iteration : 1100, loss : 78.0507583618164\n",
      "iteration : 1101, loss : 78.04529571533203\n",
      "iteration : 1102, loss : 78.0398178100586\n",
      "iteration : 1103, loss : 78.03435516357422\n",
      "iteration : 1104, loss : 78.02890014648438\n",
      "iteration : 1105, loss : 78.0234603881836\n",
      "iteration : 1106, loss : 78.01802825927734\n",
      "iteration : 1107, loss : 78.01261138916016\n",
      "iteration : 1108, loss : 78.00719451904297\n",
      "iteration : 1109, loss : 78.00177764892578\n",
      "iteration : 1110, loss : 77.99638366699219\n",
      "iteration : 1111, loss : 77.99099731445312\n",
      "iteration : 1112, loss : 77.98561096191406\n",
      "iteration : 1113, loss : 77.98023986816406\n",
      "iteration : 1114, loss : 77.9748764038086\n",
      "iteration : 1115, loss : 77.96952819824219\n",
      "iteration : 1116, loss : 77.96417236328125\n",
      "iteration : 1117, loss : 77.9588394165039\n",
      "iteration : 1118, loss : 77.9535140991211\n",
      "iteration : 1119, loss : 77.94818115234375\n",
      "iteration : 1120, loss : 77.94286346435547\n",
      "iteration : 1121, loss : 77.93757629394531\n",
      "iteration : 1122, loss : 77.9322738647461\n",
      "iteration : 1123, loss : 77.92698669433594\n",
      "iteration : 1124, loss : 77.92170715332031\n",
      "iteration : 1125, loss : 77.91642761230469\n",
      "iteration : 1126, loss : 77.91117095947266\n",
      "iteration : 1127, loss : 77.90591430664062\n",
      "iteration : 1128, loss : 77.90066528320312\n",
      "iteration : 1129, loss : 77.89543151855469\n",
      "iteration : 1130, loss : 77.89020538330078\n",
      "iteration : 1131, loss : 77.8849868774414\n",
      "iteration : 1132, loss : 77.87976837158203\n",
      "iteration : 1133, loss : 77.87456512451172\n",
      "iteration : 1134, loss : 77.86936950683594\n",
      "iteration : 1135, loss : 77.86417388916016\n",
      "iteration : 1136, loss : 77.85900115966797\n",
      "iteration : 1137, loss : 77.85382080078125\n",
      "iteration : 1138, loss : 77.84866333007812\n",
      "iteration : 1139, loss : 77.843505859375\n",
      "iteration : 1140, loss : 77.8383560180664\n",
      "iteration : 1141, loss : 77.83321380615234\n",
      "iteration : 1142, loss : 77.82808685302734\n",
      "iteration : 1143, loss : 77.82295227050781\n",
      "iteration : 1144, loss : 77.81783294677734\n",
      "iteration : 1145, loss : 77.81272888183594\n",
      "iteration : 1146, loss : 77.8076400756836\n",
      "iteration : 1147, loss : 77.80253601074219\n",
      "iteration : 1148, loss : 77.79744720458984\n",
      "iteration : 1149, loss : 77.79236602783203\n",
      "iteration : 1150, loss : 77.78730010986328\n",
      "iteration : 1151, loss : 77.78224182128906\n",
      "iteration : 1152, loss : 77.77717590332031\n",
      "iteration : 1153, loss : 77.77212524414062\n",
      "iteration : 1154, loss : 77.76708984375\n",
      "iteration : 1155, loss : 77.7620620727539\n",
      "iteration : 1156, loss : 77.75704193115234\n",
      "iteration : 1157, loss : 77.75200653076172\n",
      "iteration : 1158, loss : 77.74700164794922\n",
      "iteration : 1159, loss : 77.74199676513672\n",
      "iteration : 1160, loss : 77.73700714111328\n",
      "iteration : 1161, loss : 77.73200988769531\n",
      "iteration : 1162, loss : 77.72704315185547\n",
      "iteration : 1163, loss : 77.7220687866211\n",
      "iteration : 1164, loss : 77.71709442138672\n",
      "iteration : 1165, loss : 77.71214294433594\n",
      "iteration : 1166, loss : 77.70719909667969\n",
      "iteration : 1167, loss : 77.7022476196289\n",
      "iteration : 1168, loss : 77.69731903076172\n",
      "iteration : 1169, loss : 77.69239044189453\n",
      "iteration : 1170, loss : 77.6874771118164\n",
      "iteration : 1171, loss : 77.68254852294922\n",
      "iteration : 1172, loss : 77.67765045166016\n",
      "iteration : 1173, loss : 77.67273712158203\n",
      "iteration : 1174, loss : 77.66785430908203\n",
      "iteration : 1175, loss : 77.6629638671875\n",
      "iteration : 1176, loss : 77.65808868408203\n",
      "iteration : 1177, loss : 77.65321350097656\n",
      "iteration : 1178, loss : 77.64835357666016\n",
      "iteration : 1179, loss : 77.64348602294922\n",
      "iteration : 1180, loss : 77.6386489868164\n",
      "iteration : 1181, loss : 77.63380432128906\n",
      "iteration : 1182, loss : 77.62896728515625\n",
      "iteration : 1183, loss : 77.62413787841797\n",
      "iteration : 1184, loss : 77.61931610107422\n",
      "iteration : 1185, loss : 77.614501953125\n",
      "iteration : 1186, loss : 77.60969543457031\n",
      "iteration : 1187, loss : 77.6048812866211\n",
      "iteration : 1188, loss : 77.60009765625\n",
      "iteration : 1189, loss : 77.5953140258789\n",
      "iteration : 1190, loss : 77.59052276611328\n",
      "iteration : 1191, loss : 77.58575439453125\n",
      "iteration : 1192, loss : 77.58098602294922\n",
      "iteration : 1193, loss : 77.57622528076172\n",
      "iteration : 1194, loss : 77.57147979736328\n",
      "iteration : 1195, loss : 77.56672668457031\n",
      "iteration : 1196, loss : 77.5619888305664\n",
      "iteration : 1197, loss : 77.55726623535156\n",
      "iteration : 1198, loss : 77.55252838134766\n",
      "iteration : 1199, loss : 77.54779815673828\n",
      "iteration : 1200, loss : 77.5430908203125\n",
      "iteration : 1201, loss : 77.53838348388672\n",
      "iteration : 1202, loss : 77.53368377685547\n",
      "iteration : 1203, loss : 77.52899169921875\n",
      "iteration : 1204, loss : 77.52429962158203\n",
      "iteration : 1205, loss : 77.51962280273438\n",
      "iteration : 1206, loss : 77.51494598388672\n",
      "iteration : 1207, loss : 77.51028442382812\n",
      "iteration : 1208, loss : 77.50562286376953\n",
      "iteration : 1209, loss : 77.5009536743164\n",
      "iteration : 1210, loss : 77.49630737304688\n",
      "iteration : 1211, loss : 77.4916763305664\n",
      "iteration : 1212, loss : 77.4870376586914\n",
      "iteration : 1213, loss : 77.48240661621094\n",
      "iteration : 1214, loss : 77.47776794433594\n",
      "iteration : 1215, loss : 77.4731674194336\n",
      "iteration : 1216, loss : 77.46855926513672\n",
      "iteration : 1217, loss : 77.46395111083984\n",
      "iteration : 1218, loss : 77.45936584472656\n",
      "iteration : 1219, loss : 77.45476531982422\n",
      "iteration : 1220, loss : 77.45018005371094\n",
      "iteration : 1221, loss : 77.44559478759766\n",
      "iteration : 1222, loss : 77.44100952148438\n",
      "iteration : 1223, loss : 77.43646240234375\n",
      "iteration : 1224, loss : 77.43190002441406\n",
      "iteration : 1225, loss : 77.42733001708984\n",
      "iteration : 1226, loss : 77.42279052734375\n",
      "iteration : 1227, loss : 77.41825103759766\n",
      "iteration : 1228, loss : 77.41370391845703\n",
      "iteration : 1229, loss : 77.40918731689453\n",
      "iteration : 1230, loss : 77.4046630859375\n",
      "iteration : 1231, loss : 77.400146484375\n",
      "iteration : 1232, loss : 77.3956298828125\n",
      "iteration : 1233, loss : 77.39112854003906\n",
      "iteration : 1234, loss : 77.38662719726562\n",
      "iteration : 1235, loss : 77.38211822509766\n",
      "iteration : 1236, loss : 77.37763977050781\n",
      "iteration : 1237, loss : 77.37315368652344\n",
      "iteration : 1238, loss : 77.3686752319336\n",
      "iteration : 1239, loss : 77.36421966552734\n",
      "iteration : 1240, loss : 77.3597412109375\n",
      "iteration : 1241, loss : 77.35528564453125\n",
      "iteration : 1242, loss : 77.350830078125\n",
      "iteration : 1243, loss : 77.34638977050781\n",
      "iteration : 1244, loss : 77.34194946289062\n",
      "iteration : 1245, loss : 77.33750915527344\n",
      "iteration : 1246, loss : 77.33308410644531\n",
      "iteration : 1247, loss : 77.32865905761719\n",
      "iteration : 1248, loss : 77.32423400878906\n",
      "iteration : 1249, loss : 77.31981658935547\n",
      "iteration : 1250, loss : 77.31541442871094\n",
      "iteration : 1251, loss : 77.3110122680664\n",
      "iteration : 1252, loss : 77.3066177368164\n",
      "iteration : 1253, loss : 77.3022232055664\n",
      "iteration : 1254, loss : 77.29784393310547\n",
      "iteration : 1255, loss : 77.29345703125\n",
      "iteration : 1256, loss : 77.2890853881836\n",
      "iteration : 1257, loss : 77.28472137451172\n",
      "iteration : 1258, loss : 77.28036499023438\n",
      "iteration : 1259, loss : 77.2760009765625\n",
      "iteration : 1260, loss : 77.27164459228516\n",
      "iteration : 1261, loss : 77.2673110961914\n",
      "iteration : 1262, loss : 77.26296997070312\n",
      "iteration : 1263, loss : 77.25862121582031\n",
      "iteration : 1264, loss : 77.25430297851562\n",
      "iteration : 1265, loss : 77.2499771118164\n",
      "iteration : 1266, loss : 77.24565887451172\n",
      "iteration : 1267, loss : 77.24134063720703\n",
      "iteration : 1268, loss : 77.23704528808594\n",
      "iteration : 1269, loss : 77.23273468017578\n",
      "iteration : 1270, loss : 77.22844696044922\n",
      "iteration : 1271, loss : 77.22415161132812\n",
      "iteration : 1272, loss : 77.2198715209961\n",
      "iteration : 1273, loss : 77.215576171875\n",
      "iteration : 1274, loss : 77.2113037109375\n",
      "iteration : 1275, loss : 77.20703125\n",
      "iteration : 1276, loss : 77.2027587890625\n",
      "iteration : 1277, loss : 77.1985092163086\n",
      "iteration : 1278, loss : 77.19425964355469\n",
      "iteration : 1279, loss : 77.19000244140625\n",
      "iteration : 1280, loss : 77.18575286865234\n",
      "iteration : 1281, loss : 77.18151092529297\n",
      "iteration : 1282, loss : 77.17728424072266\n",
      "iteration : 1283, loss : 77.17305755615234\n",
      "iteration : 1284, loss : 77.16883087158203\n",
      "iteration : 1285, loss : 77.16461181640625\n",
      "iteration : 1286, loss : 77.16039276123047\n",
      "iteration : 1287, loss : 77.15619659423828\n",
      "iteration : 1288, loss : 77.15198516845703\n",
      "iteration : 1289, loss : 77.14778900146484\n",
      "iteration : 1290, loss : 77.14360046386719\n",
      "iteration : 1291, loss : 77.139404296875\n",
      "iteration : 1292, loss : 77.13523864746094\n",
      "iteration : 1293, loss : 77.13104248046875\n",
      "iteration : 1294, loss : 77.12686920166016\n",
      "iteration : 1295, loss : 77.1227035522461\n",
      "iteration : 1296, loss : 77.11854553222656\n",
      "iteration : 1297, loss : 77.11438751220703\n",
      "iteration : 1298, loss : 77.1102294921875\n",
      "iteration : 1299, loss : 77.1060791015625\n",
      "iteration : 1300, loss : 77.10194396972656\n",
      "iteration : 1301, loss : 77.0978012084961\n",
      "iteration : 1302, loss : 77.09367370605469\n",
      "iteration : 1303, loss : 77.08954620361328\n",
      "iteration : 1304, loss : 77.08541870117188\n",
      "iteration : 1305, loss : 77.081298828125\n",
      "iteration : 1306, loss : 77.07718658447266\n",
      "iteration : 1307, loss : 77.07307434082031\n",
      "iteration : 1308, loss : 77.06896209716797\n",
      "iteration : 1309, loss : 77.06486511230469\n",
      "iteration : 1310, loss : 77.06077575683594\n",
      "iteration : 1311, loss : 77.05667877197266\n",
      "iteration : 1312, loss : 77.05259704589844\n",
      "iteration : 1313, loss : 77.04851531982422\n",
      "iteration : 1314, loss : 77.04444122314453\n",
      "iteration : 1315, loss : 77.04036712646484\n",
      "iteration : 1316, loss : 77.03630065917969\n",
      "iteration : 1317, loss : 77.03224182128906\n",
      "iteration : 1318, loss : 77.02816009521484\n",
      "iteration : 1319, loss : 77.02412414550781\n",
      "iteration : 1320, loss : 77.02007293701172\n",
      "iteration : 1321, loss : 77.01602935791016\n",
      "iteration : 1322, loss : 77.01197814941406\n",
      "iteration : 1323, loss : 77.0079574584961\n",
      "iteration : 1324, loss : 77.0039291381836\n",
      "iteration : 1325, loss : 76.99989318847656\n",
      "iteration : 1326, loss : 76.9958724975586\n",
      "iteration : 1327, loss : 76.99185180664062\n",
      "iteration : 1328, loss : 76.98783874511719\n",
      "iteration : 1329, loss : 76.98383331298828\n",
      "iteration : 1330, loss : 76.9798355102539\n",
      "iteration : 1331, loss : 76.97583770751953\n",
      "iteration : 1332, loss : 76.97184753417969\n",
      "iteration : 1333, loss : 76.96785736083984\n",
      "iteration : 1334, loss : 76.96387481689453\n",
      "iteration : 1335, loss : 76.95988464355469\n",
      "iteration : 1336, loss : 76.9559097290039\n",
      "iteration : 1337, loss : 76.95194244384766\n",
      "iteration : 1338, loss : 76.9479751586914\n",
      "iteration : 1339, loss : 76.94400024414062\n",
      "iteration : 1340, loss : 76.94004821777344\n",
      "iteration : 1341, loss : 76.93608856201172\n",
      "iteration : 1342, loss : 76.93214416503906\n",
      "iteration : 1343, loss : 76.92820739746094\n",
      "iteration : 1344, loss : 76.92425537109375\n",
      "iteration : 1345, loss : 76.92032623291016\n",
      "iteration : 1346, loss : 76.91638946533203\n",
      "iteration : 1347, loss : 76.91246032714844\n",
      "iteration : 1348, loss : 76.90852355957031\n",
      "iteration : 1349, loss : 76.90460968017578\n",
      "iteration : 1350, loss : 76.90069580078125\n",
      "iteration : 1351, loss : 76.89677429199219\n",
      "iteration : 1352, loss : 76.89286804199219\n",
      "iteration : 1353, loss : 76.88896179199219\n",
      "iteration : 1354, loss : 76.88507080078125\n",
      "iteration : 1355, loss : 76.88117218017578\n",
      "iteration : 1356, loss : 76.87727355957031\n",
      "iteration : 1357, loss : 76.87340545654297\n",
      "iteration : 1358, loss : 76.86952209472656\n",
      "iteration : 1359, loss : 76.86563110351562\n",
      "iteration : 1360, loss : 76.86176300048828\n",
      "iteration : 1361, loss : 76.8578872680664\n",
      "iteration : 1362, loss : 76.8540267944336\n",
      "iteration : 1363, loss : 76.85015869140625\n",
      "iteration : 1364, loss : 76.84629821777344\n",
      "iteration : 1365, loss : 76.84243774414062\n",
      "iteration : 1366, loss : 76.8386001586914\n",
      "iteration : 1367, loss : 76.83474731445312\n",
      "iteration : 1368, loss : 76.83090209960938\n",
      "iteration : 1369, loss : 76.82706451416016\n",
      "iteration : 1370, loss : 76.82322692871094\n",
      "iteration : 1371, loss : 76.81940460205078\n",
      "iteration : 1372, loss : 76.81558227539062\n",
      "iteration : 1373, loss : 76.81175231933594\n",
      "iteration : 1374, loss : 76.80792236328125\n",
      "iteration : 1375, loss : 76.80411529541016\n",
      "iteration : 1376, loss : 76.80030059814453\n",
      "iteration : 1377, loss : 76.79650115966797\n",
      "iteration : 1378, loss : 76.79269409179688\n",
      "iteration : 1379, loss : 76.78889465332031\n",
      "iteration : 1380, loss : 76.78509521484375\n",
      "iteration : 1381, loss : 76.78131103515625\n",
      "iteration : 1382, loss : 76.77752685546875\n",
      "iteration : 1383, loss : 76.77374267578125\n",
      "iteration : 1384, loss : 76.76995849609375\n",
      "iteration : 1385, loss : 76.76617431640625\n",
      "iteration : 1386, loss : 76.76241302490234\n",
      "iteration : 1387, loss : 76.75863647460938\n",
      "iteration : 1388, loss : 76.75489044189453\n",
      "iteration : 1389, loss : 76.7511215209961\n",
      "iteration : 1390, loss : 76.74736022949219\n",
      "iteration : 1391, loss : 76.74359893798828\n",
      "iteration : 1392, loss : 76.73985290527344\n",
      "iteration : 1393, loss : 76.73611450195312\n",
      "iteration : 1394, loss : 76.73236846923828\n",
      "iteration : 1395, loss : 76.72862243652344\n",
      "iteration : 1396, loss : 76.72488403320312\n",
      "iteration : 1397, loss : 76.7211685180664\n",
      "iteration : 1398, loss : 76.7174301147461\n",
      "iteration : 1399, loss : 76.71370697021484\n",
      "iteration : 1400, loss : 76.7099838256836\n",
      "iteration : 1401, loss : 76.7062759399414\n",
      "iteration : 1402, loss : 76.70256042480469\n",
      "iteration : 1403, loss : 76.69884490966797\n",
      "iteration : 1404, loss : 76.69514465332031\n",
      "iteration : 1405, loss : 76.69144439697266\n",
      "iteration : 1406, loss : 76.68773651123047\n",
      "iteration : 1407, loss : 76.68404388427734\n",
      "iteration : 1408, loss : 76.68035125732422\n",
      "iteration : 1409, loss : 76.6766586303711\n",
      "iteration : 1410, loss : 76.67298889160156\n",
      "iteration : 1411, loss : 76.66930389404297\n",
      "iteration : 1412, loss : 76.66561889648438\n",
      "iteration : 1413, loss : 76.66195678710938\n",
      "iteration : 1414, loss : 76.65827941894531\n",
      "iteration : 1415, loss : 76.65460968017578\n",
      "iteration : 1416, loss : 76.65095520019531\n",
      "iteration : 1417, loss : 76.64729309082031\n",
      "iteration : 1418, loss : 76.64363861083984\n",
      "iteration : 1419, loss : 76.63997650146484\n",
      "iteration : 1420, loss : 76.6363296508789\n",
      "iteration : 1421, loss : 76.6326904296875\n",
      "iteration : 1422, loss : 76.62903594970703\n",
      "iteration : 1423, loss : 76.62540435791016\n",
      "iteration : 1424, loss : 76.62175750732422\n",
      "iteration : 1425, loss : 76.61813354492188\n",
      "iteration : 1426, loss : 76.61450958251953\n",
      "iteration : 1427, loss : 76.61087799072266\n",
      "iteration : 1428, loss : 76.60726928710938\n",
      "iteration : 1429, loss : 76.6036376953125\n",
      "iteration : 1430, loss : 76.60002136230469\n",
      "iteration : 1431, loss : 76.5964126586914\n",
      "iteration : 1432, loss : 76.59280395507812\n",
      "iteration : 1433, loss : 76.58919525146484\n",
      "iteration : 1434, loss : 76.5855941772461\n",
      "iteration : 1435, loss : 76.58199310302734\n",
      "iteration : 1436, loss : 76.57839965820312\n",
      "iteration : 1437, loss : 76.5748062133789\n",
      "iteration : 1438, loss : 76.57121276855469\n",
      "iteration : 1439, loss : 76.567626953125\n",
      "iteration : 1440, loss : 76.56404876708984\n",
      "iteration : 1441, loss : 76.56047058105469\n",
      "iteration : 1442, loss : 76.55689239501953\n",
      "iteration : 1443, loss : 76.55332946777344\n",
      "iteration : 1444, loss : 76.54974365234375\n",
      "iteration : 1445, loss : 76.54618072509766\n",
      "iteration : 1446, loss : 76.5426254272461\n",
      "iteration : 1447, loss : 76.5390625\n",
      "iteration : 1448, loss : 76.53550720214844\n",
      "iteration : 1449, loss : 76.53194427490234\n",
      "iteration : 1450, loss : 76.52840423583984\n",
      "iteration : 1451, loss : 76.52484893798828\n",
      "iteration : 1452, loss : 76.52130889892578\n",
      "iteration : 1453, loss : 76.51776885986328\n",
      "iteration : 1454, loss : 76.51422882080078\n",
      "iteration : 1455, loss : 76.51068878173828\n",
      "iteration : 1456, loss : 76.50716400146484\n",
      "iteration : 1457, loss : 76.5036392211914\n",
      "iteration : 1458, loss : 76.50010681152344\n",
      "iteration : 1459, loss : 76.49658203125\n",
      "iteration : 1460, loss : 76.49305725097656\n",
      "iteration : 1461, loss : 76.48954010009766\n",
      "iteration : 1462, loss : 76.48604583740234\n",
      "iteration : 1463, loss : 76.4825210571289\n",
      "iteration : 1464, loss : 76.47901916503906\n",
      "iteration : 1465, loss : 76.47550964355469\n",
      "iteration : 1466, loss : 76.4720230102539\n",
      "iteration : 1467, loss : 76.46851348876953\n",
      "iteration : 1468, loss : 76.46501922607422\n",
      "iteration : 1469, loss : 76.4615249633789\n",
      "iteration : 1470, loss : 76.45804595947266\n",
      "iteration : 1471, loss : 76.45454406738281\n",
      "iteration : 1472, loss : 76.4510726928711\n",
      "iteration : 1473, loss : 76.44757843017578\n",
      "iteration : 1474, loss : 76.44410705566406\n",
      "iteration : 1475, loss : 76.44063568115234\n",
      "iteration : 1476, loss : 76.43714904785156\n",
      "iteration : 1477, loss : 76.4336929321289\n",
      "iteration : 1478, loss : 76.43022918701172\n",
      "iteration : 1479, loss : 76.42677307128906\n",
      "iteration : 1480, loss : 76.42330932617188\n",
      "iteration : 1481, loss : 76.41984558105469\n",
      "iteration : 1482, loss : 76.41639709472656\n",
      "iteration : 1483, loss : 76.41294860839844\n",
      "iteration : 1484, loss : 76.40949249267578\n",
      "iteration : 1485, loss : 76.40605163574219\n",
      "iteration : 1486, loss : 76.40260314941406\n",
      "iteration : 1487, loss : 76.39915466308594\n",
      "iteration : 1488, loss : 76.3957290649414\n",
      "iteration : 1489, loss : 76.39229583740234\n",
      "iteration : 1490, loss : 76.38886260986328\n",
      "iteration : 1491, loss : 76.38544464111328\n",
      "iteration : 1492, loss : 76.38201904296875\n",
      "iteration : 1493, loss : 76.37859344482422\n",
      "iteration : 1494, loss : 76.37516784667969\n",
      "iteration : 1495, loss : 76.37174224853516\n",
      "iteration : 1496, loss : 76.36833953857422\n",
      "iteration : 1497, loss : 76.36492919921875\n",
      "iteration : 1498, loss : 76.36151123046875\n",
      "iteration : 1499, loss : 76.35810852050781\n",
      "iteration : 1500, loss : 76.35469818115234\n",
      "iteration : 1501, loss : 76.35130310058594\n",
      "iteration : 1502, loss : 76.34790802001953\n",
      "iteration : 1503, loss : 76.34451293945312\n",
      "iteration : 1504, loss : 76.34111785888672\n",
      "iteration : 1505, loss : 76.33773803710938\n",
      "iteration : 1506, loss : 76.3343505859375\n",
      "iteration : 1507, loss : 76.33094787597656\n",
      "iteration : 1508, loss : 76.32757568359375\n",
      "iteration : 1509, loss : 76.32418823242188\n",
      "iteration : 1510, loss : 76.3208236694336\n",
      "iteration : 1511, loss : 76.31744384765625\n",
      "iteration : 1512, loss : 76.31407165527344\n",
      "iteration : 1513, loss : 76.31070709228516\n",
      "iteration : 1514, loss : 76.30733489990234\n",
      "iteration : 1515, loss : 76.3039779663086\n",
      "iteration : 1516, loss : 76.30062103271484\n",
      "iteration : 1517, loss : 76.29725646972656\n",
      "iteration : 1518, loss : 76.29389953613281\n",
      "iteration : 1519, loss : 76.2905502319336\n",
      "iteration : 1520, loss : 76.28719329833984\n",
      "iteration : 1521, loss : 76.28385162353516\n",
      "iteration : 1522, loss : 76.28050994873047\n",
      "iteration : 1523, loss : 76.27716827392578\n",
      "iteration : 1524, loss : 76.27381896972656\n",
      "iteration : 1525, loss : 76.2704849243164\n",
      "iteration : 1526, loss : 76.26715087890625\n",
      "iteration : 1527, loss : 76.2638168334961\n",
      "iteration : 1528, loss : 76.26049041748047\n",
      "iteration : 1529, loss : 76.25716400146484\n",
      "iteration : 1530, loss : 76.25383758544922\n",
      "iteration : 1531, loss : 76.2505111694336\n",
      "iteration : 1532, loss : 76.24720001220703\n",
      "iteration : 1533, loss : 76.24388885498047\n",
      "iteration : 1534, loss : 76.24057006835938\n",
      "iteration : 1535, loss : 76.23724365234375\n",
      "iteration : 1536, loss : 76.23394012451172\n",
      "iteration : 1537, loss : 76.23063659667969\n",
      "iteration : 1538, loss : 76.22733306884766\n",
      "iteration : 1539, loss : 76.2240219116211\n",
      "iteration : 1540, loss : 76.2207260131836\n",
      "iteration : 1541, loss : 76.2174301147461\n",
      "iteration : 1542, loss : 76.21414184570312\n",
      "iteration : 1543, loss : 76.21084594726562\n",
      "iteration : 1544, loss : 76.20755004882812\n",
      "iteration : 1545, loss : 76.20426940917969\n",
      "iteration : 1546, loss : 76.20098876953125\n",
      "iteration : 1547, loss : 76.19770050048828\n",
      "iteration : 1548, loss : 76.19442749023438\n",
      "iteration : 1549, loss : 76.19114685058594\n",
      "iteration : 1550, loss : 76.1878662109375\n",
      "iteration : 1551, loss : 76.18460845947266\n",
      "iteration : 1552, loss : 76.18132781982422\n",
      "iteration : 1553, loss : 76.17804718017578\n",
      "iteration : 1554, loss : 76.1747817993164\n",
      "iteration : 1555, loss : 76.17152404785156\n",
      "iteration : 1556, loss : 76.16825866699219\n",
      "iteration : 1557, loss : 76.16500091552734\n",
      "iteration : 1558, loss : 76.16173553466797\n",
      "iteration : 1559, loss : 76.15848541259766\n",
      "iteration : 1560, loss : 76.15523529052734\n",
      "iteration : 1561, loss : 76.15198516845703\n",
      "iteration : 1562, loss : 76.14874267578125\n",
      "iteration : 1563, loss : 76.14549255371094\n",
      "iteration : 1564, loss : 76.14226531982422\n",
      "iteration : 1565, loss : 76.13902282714844\n",
      "iteration : 1566, loss : 76.13578033447266\n",
      "iteration : 1567, loss : 76.13253784179688\n",
      "iteration : 1568, loss : 76.12931060791016\n",
      "iteration : 1569, loss : 76.1260757446289\n",
      "iteration : 1570, loss : 76.12284088134766\n",
      "iteration : 1571, loss : 76.11962890625\n",
      "iteration : 1572, loss : 76.11640167236328\n",
      "iteration : 1573, loss : 76.11317443847656\n",
      "iteration : 1574, loss : 76.10995483398438\n",
      "iteration : 1575, loss : 76.10672760009766\n",
      "iteration : 1576, loss : 76.10353088378906\n",
      "iteration : 1577, loss : 76.10030364990234\n",
      "iteration : 1578, loss : 76.09709167480469\n",
      "iteration : 1579, loss : 76.09387969970703\n",
      "iteration : 1580, loss : 76.0906753540039\n",
      "iteration : 1581, loss : 76.08747100830078\n",
      "iteration : 1582, loss : 76.08426666259766\n",
      "iteration : 1583, loss : 76.0810775756836\n",
      "iteration : 1584, loss : 76.077880859375\n",
      "iteration : 1585, loss : 76.07467651367188\n",
      "iteration : 1586, loss : 76.07148742675781\n",
      "iteration : 1587, loss : 76.06829833984375\n",
      "iteration : 1588, loss : 76.06510162353516\n",
      "iteration : 1589, loss : 76.0619125366211\n",
      "iteration : 1590, loss : 76.05872344970703\n",
      "iteration : 1591, loss : 76.0555419921875\n",
      "iteration : 1592, loss : 76.0523681640625\n",
      "iteration : 1593, loss : 76.04920196533203\n",
      "iteration : 1594, loss : 76.0460205078125\n",
      "iteration : 1595, loss : 76.04283905029297\n",
      "iteration : 1596, loss : 76.03966522216797\n",
      "iteration : 1597, loss : 76.03649139404297\n",
      "iteration : 1598, loss : 76.0333251953125\n",
      "iteration : 1599, loss : 76.03015899658203\n",
      "iteration : 1600, loss : 76.02699279785156\n",
      "iteration : 1601, loss : 76.02383422851562\n",
      "iteration : 1602, loss : 76.02067565917969\n",
      "iteration : 1603, loss : 76.01751708984375\n",
      "iteration : 1604, loss : 76.01435852050781\n",
      "iteration : 1605, loss : 76.01119995117188\n",
      "iteration : 1606, loss : 76.008056640625\n",
      "iteration : 1607, loss : 76.00491333007812\n",
      "iteration : 1608, loss : 76.00176239013672\n",
      "iteration : 1609, loss : 75.99861145019531\n",
      "iteration : 1610, loss : 75.99546813964844\n",
      "iteration : 1611, loss : 75.99232482910156\n",
      "iteration : 1612, loss : 75.98918914794922\n",
      "iteration : 1613, loss : 75.9860610961914\n",
      "iteration : 1614, loss : 75.98292541503906\n",
      "iteration : 1615, loss : 75.97978210449219\n",
      "iteration : 1616, loss : 75.97665405273438\n",
      "iteration : 1617, loss : 75.97352600097656\n",
      "iteration : 1618, loss : 75.97040557861328\n",
      "iteration : 1619, loss : 75.96727752685547\n",
      "iteration : 1620, loss : 75.96414184570312\n",
      "iteration : 1621, loss : 75.96102142333984\n",
      "iteration : 1622, loss : 75.95790100097656\n",
      "iteration : 1623, loss : 75.95479583740234\n",
      "iteration : 1624, loss : 75.95167541503906\n",
      "iteration : 1625, loss : 75.94856262207031\n",
      "iteration : 1626, loss : 75.94544982910156\n",
      "iteration : 1627, loss : 75.94233703613281\n",
      "iteration : 1628, loss : 75.93923950195312\n",
      "iteration : 1629, loss : 75.93612670898438\n",
      "iteration : 1630, loss : 75.93302154541016\n",
      "iteration : 1631, loss : 75.92991638183594\n",
      "iteration : 1632, loss : 75.92682647705078\n",
      "iteration : 1633, loss : 75.92373657226562\n",
      "iteration : 1634, loss : 75.92062377929688\n",
      "iteration : 1635, loss : 75.91753387451172\n",
      "iteration : 1636, loss : 75.91444396972656\n",
      "iteration : 1637, loss : 75.9113540649414\n",
      "iteration : 1638, loss : 75.90825653076172\n",
      "iteration : 1639, loss : 75.90518188476562\n",
      "iteration : 1640, loss : 75.902099609375\n",
      "iteration : 1641, loss : 75.89901733398438\n",
      "iteration : 1642, loss : 75.89591979980469\n",
      "iteration : 1643, loss : 75.8928451538086\n",
      "iteration : 1644, loss : 75.8897705078125\n",
      "iteration : 1645, loss : 75.8866958618164\n",
      "iteration : 1646, loss : 75.88362121582031\n",
      "iteration : 1647, loss : 75.88054656982422\n",
      "iteration : 1648, loss : 75.87747955322266\n",
      "iteration : 1649, loss : 75.87440490722656\n",
      "iteration : 1650, loss : 75.871337890625\n",
      "iteration : 1651, loss : 75.86827850341797\n",
      "iteration : 1652, loss : 75.8652114868164\n",
      "iteration : 1653, loss : 75.86215209960938\n",
      "iteration : 1654, loss : 75.85910034179688\n",
      "iteration : 1655, loss : 75.85604095458984\n",
      "iteration : 1656, loss : 75.85298919677734\n",
      "iteration : 1657, loss : 75.84992218017578\n",
      "iteration : 1658, loss : 75.84687042236328\n",
      "iteration : 1659, loss : 75.84382629394531\n",
      "iteration : 1660, loss : 75.84077453613281\n",
      "iteration : 1661, loss : 75.83773803710938\n",
      "iteration : 1662, loss : 75.83470153808594\n",
      "iteration : 1663, loss : 75.8316421508789\n",
      "iteration : 1664, loss : 75.82859802246094\n",
      "iteration : 1665, loss : 75.82556915283203\n",
      "iteration : 1666, loss : 75.82252502441406\n",
      "iteration : 1667, loss : 75.81948852539062\n",
      "iteration : 1668, loss : 75.81645202636719\n",
      "iteration : 1669, loss : 75.81342315673828\n",
      "iteration : 1670, loss : 75.8104019165039\n",
      "iteration : 1671, loss : 75.80736541748047\n",
      "iteration : 1672, loss : 75.80433654785156\n",
      "iteration : 1673, loss : 75.80131530761719\n",
      "iteration : 1674, loss : 75.79829406738281\n",
      "iteration : 1675, loss : 75.79528045654297\n",
      "iteration : 1676, loss : 75.79224395751953\n",
      "iteration : 1677, loss : 75.78923034667969\n",
      "iteration : 1678, loss : 75.78622436523438\n",
      "iteration : 1679, loss : 75.78319549560547\n",
      "iteration : 1680, loss : 75.7801742553711\n",
      "iteration : 1681, loss : 75.77717590332031\n",
      "iteration : 1682, loss : 75.77416229248047\n",
      "iteration : 1683, loss : 75.77114868164062\n",
      "iteration : 1684, loss : 75.76814270019531\n",
      "iteration : 1685, loss : 75.76512908935547\n",
      "iteration : 1686, loss : 75.76214599609375\n",
      "iteration : 1687, loss : 75.75914001464844\n",
      "iteration : 1688, loss : 75.75613403320312\n",
      "iteration : 1689, loss : 75.75314331054688\n",
      "iteration : 1690, loss : 75.7501449584961\n",
      "iteration : 1691, loss : 75.74714660644531\n",
      "iteration : 1692, loss : 75.74415588378906\n",
      "iteration : 1693, loss : 75.74114990234375\n",
      "iteration : 1694, loss : 75.73816680908203\n",
      "iteration : 1695, loss : 75.73518371582031\n",
      "iteration : 1696, loss : 75.7322006225586\n",
      "iteration : 1697, loss : 75.72921752929688\n",
      "iteration : 1698, loss : 75.7262191772461\n",
      "iteration : 1699, loss : 75.72323608398438\n",
      "iteration : 1700, loss : 75.72025299072266\n",
      "iteration : 1701, loss : 75.71728515625\n",
      "iteration : 1702, loss : 75.71430206298828\n",
      "iteration : 1703, loss : 75.71133422851562\n",
      "iteration : 1704, loss : 75.70835876464844\n",
      "iteration : 1705, loss : 75.70538330078125\n",
      "iteration : 1706, loss : 75.7024154663086\n",
      "iteration : 1707, loss : 75.69944763183594\n",
      "iteration : 1708, loss : 75.69647216796875\n",
      "iteration : 1709, loss : 75.6935043334961\n",
      "iteration : 1710, loss : 75.69054412841797\n",
      "iteration : 1711, loss : 75.68757629394531\n",
      "iteration : 1712, loss : 75.68462371826172\n",
      "iteration : 1713, loss : 75.6816635131836\n",
      "iteration : 1714, loss : 75.67869567871094\n",
      "iteration : 1715, loss : 75.67574310302734\n",
      "iteration : 1716, loss : 75.67279052734375\n",
      "iteration : 1717, loss : 75.66983032226562\n",
      "iteration : 1718, loss : 75.66687774658203\n",
      "iteration : 1719, loss : 75.66393280029297\n",
      "iteration : 1720, loss : 75.6609878540039\n",
      "iteration : 1721, loss : 75.65804290771484\n",
      "iteration : 1722, loss : 75.65508270263672\n",
      "iteration : 1723, loss : 75.65214538574219\n",
      "iteration : 1724, loss : 75.64920043945312\n",
      "iteration : 1725, loss : 75.6462631225586\n",
      "iteration : 1726, loss : 75.64331817626953\n",
      "iteration : 1727, loss : 75.64038848876953\n",
      "iteration : 1728, loss : 75.63743591308594\n",
      "iteration : 1729, loss : 75.63451385498047\n",
      "iteration : 1730, loss : 75.6315689086914\n",
      "iteration : 1731, loss : 75.62864685058594\n",
      "iteration : 1732, loss : 75.62571716308594\n",
      "iteration : 1733, loss : 75.6227798461914\n",
      "iteration : 1734, loss : 75.6198501586914\n",
      "iteration : 1735, loss : 75.61692810058594\n",
      "iteration : 1736, loss : 75.61400604248047\n",
      "iteration : 1737, loss : 75.611083984375\n",
      "iteration : 1738, loss : 75.608154296875\n",
      "iteration : 1739, loss : 75.60523986816406\n",
      "iteration : 1740, loss : 75.6023178100586\n",
      "iteration : 1741, loss : 75.59941101074219\n",
      "iteration : 1742, loss : 75.59648132324219\n",
      "iteration : 1743, loss : 75.59356689453125\n",
      "iteration : 1744, loss : 75.59064483642578\n",
      "iteration : 1745, loss : 75.58773803710938\n",
      "iteration : 1746, loss : 75.58482360839844\n",
      "iteration : 1747, loss : 75.58192443847656\n",
      "iteration : 1748, loss : 75.5790023803711\n",
      "iteration : 1749, loss : 75.57610321044922\n",
      "iteration : 1750, loss : 75.57319641113281\n",
      "iteration : 1751, loss : 75.57029724121094\n",
      "iteration : 1752, loss : 75.5674057006836\n",
      "iteration : 1753, loss : 75.56449127197266\n",
      "iteration : 1754, loss : 75.56159973144531\n",
      "iteration : 1755, loss : 75.55870056152344\n",
      "iteration : 1756, loss : 75.55580139160156\n",
      "iteration : 1757, loss : 75.55290222167969\n",
      "iteration : 1758, loss : 75.55001068115234\n",
      "iteration : 1759, loss : 75.547119140625\n",
      "iteration : 1760, loss : 75.54421997070312\n",
      "iteration : 1761, loss : 75.54135131835938\n",
      "iteration : 1762, loss : 75.5384521484375\n",
      "iteration : 1763, loss : 75.53556823730469\n",
      "iteration : 1764, loss : 75.53266906738281\n",
      "iteration : 1765, loss : 75.52980041503906\n",
      "iteration : 1766, loss : 75.52690887451172\n",
      "iteration : 1767, loss : 75.52403259277344\n",
      "iteration : 1768, loss : 75.5211410522461\n",
      "iteration : 1769, loss : 75.51828002929688\n",
      "iteration : 1770, loss : 75.51539611816406\n",
      "iteration : 1771, loss : 75.51251983642578\n",
      "iteration : 1772, loss : 75.5096435546875\n",
      "iteration : 1773, loss : 75.50676727294922\n",
      "iteration : 1774, loss : 75.50389099121094\n",
      "iteration : 1775, loss : 75.50102996826172\n",
      "iteration : 1776, loss : 75.49816131591797\n",
      "iteration : 1777, loss : 75.49528503417969\n",
      "iteration : 1778, loss : 75.49242401123047\n",
      "iteration : 1779, loss : 75.48955535888672\n",
      "iteration : 1780, loss : 75.48670196533203\n",
      "iteration : 1781, loss : 75.48383331298828\n",
      "iteration : 1782, loss : 75.48097229003906\n",
      "iteration : 1783, loss : 75.47810363769531\n",
      "iteration : 1784, loss : 75.47525787353516\n",
      "iteration : 1785, loss : 75.47239685058594\n",
      "iteration : 1786, loss : 75.46954345703125\n",
      "iteration : 1787, loss : 75.4666976928711\n",
      "iteration : 1788, loss : 75.46383666992188\n",
      "iteration : 1789, loss : 75.46098327636719\n",
      "iteration : 1790, loss : 75.4581298828125\n",
      "iteration : 1791, loss : 75.45527648925781\n",
      "iteration : 1792, loss : 75.45243072509766\n",
      "iteration : 1793, loss : 75.4495849609375\n",
      "iteration : 1794, loss : 75.44673156738281\n",
      "iteration : 1795, loss : 75.44389343261719\n",
      "iteration : 1796, loss : 75.44104766845703\n",
      "iteration : 1797, loss : 75.43820190429688\n",
      "iteration : 1798, loss : 75.43537139892578\n",
      "iteration : 1799, loss : 75.43252563476562\n",
      "iteration : 1800, loss : 75.4296875\n",
      "iteration : 1801, loss : 75.42684936523438\n",
      "iteration : 1802, loss : 75.42401885986328\n",
      "iteration : 1803, loss : 75.42118835449219\n",
      "iteration : 1804, loss : 75.4183578491211\n",
      "iteration : 1805, loss : 75.41551971435547\n",
      "iteration : 1806, loss : 75.41268920898438\n",
      "iteration : 1807, loss : 75.40986633300781\n",
      "iteration : 1808, loss : 75.40703582763672\n",
      "iteration : 1809, loss : 75.40420532226562\n",
      "iteration : 1810, loss : 75.40138244628906\n",
      "iteration : 1811, loss : 75.39854431152344\n",
      "iteration : 1812, loss : 75.39573669433594\n",
      "iteration : 1813, loss : 75.39291381835938\n",
      "iteration : 1814, loss : 75.39008331298828\n",
      "iteration : 1815, loss : 75.38726806640625\n",
      "iteration : 1816, loss : 75.38445281982422\n",
      "iteration : 1817, loss : 75.38162994384766\n",
      "iteration : 1818, loss : 75.37881469726562\n",
      "iteration : 1819, loss : 75.37600708007812\n",
      "iteration : 1820, loss : 75.3731918334961\n",
      "iteration : 1821, loss : 75.3703842163086\n",
      "iteration : 1822, loss : 75.36756896972656\n",
      "iteration : 1823, loss : 75.36476135253906\n",
      "iteration : 1824, loss : 75.36195373535156\n",
      "iteration : 1825, loss : 75.35914611816406\n",
      "iteration : 1826, loss : 75.35633087158203\n",
      "iteration : 1827, loss : 75.35353088378906\n",
      "iteration : 1828, loss : 75.35072326660156\n",
      "iteration : 1829, loss : 75.34791564941406\n",
      "iteration : 1830, loss : 75.34513092041016\n",
      "iteration : 1831, loss : 75.34231567382812\n",
      "iteration : 1832, loss : 75.33953094482422\n",
      "iteration : 1833, loss : 75.33670806884766\n",
      "iteration : 1834, loss : 75.33392333984375\n",
      "iteration : 1835, loss : 75.33113098144531\n",
      "iteration : 1836, loss : 75.32833099365234\n",
      "iteration : 1837, loss : 75.32554626464844\n",
      "iteration : 1838, loss : 75.32274627685547\n",
      "iteration : 1839, loss : 75.31995391845703\n",
      "iteration : 1840, loss : 75.31716918945312\n",
      "iteration : 1841, loss : 75.31438446044922\n",
      "iteration : 1842, loss : 75.31159210205078\n",
      "iteration : 1843, loss : 75.30879211425781\n",
      "iteration : 1844, loss : 75.3060073852539\n",
      "iteration : 1845, loss : 75.30323791503906\n",
      "iteration : 1846, loss : 75.30044555664062\n",
      "iteration : 1847, loss : 75.29766845703125\n",
      "iteration : 1848, loss : 75.29488372802734\n",
      "iteration : 1849, loss : 75.29210662841797\n",
      "iteration : 1850, loss : 75.28932189941406\n",
      "iteration : 1851, loss : 75.28655242919922\n",
      "iteration : 1852, loss : 75.28378295898438\n",
      "iteration : 1853, loss : 75.28099822998047\n",
      "iteration : 1854, loss : 75.27822875976562\n",
      "iteration : 1855, loss : 75.27545166015625\n",
      "iteration : 1856, loss : 75.2726821899414\n",
      "iteration : 1857, loss : 75.2699203491211\n",
      "iteration : 1858, loss : 75.26713562011719\n",
      "iteration : 1859, loss : 75.26437377929688\n",
      "iteration : 1860, loss : 75.26160430908203\n",
      "iteration : 1861, loss : 75.25884246826172\n",
      "iteration : 1862, loss : 75.25607299804688\n",
      "iteration : 1863, loss : 75.25330352783203\n",
      "iteration : 1864, loss : 75.25054168701172\n",
      "iteration : 1865, loss : 75.24778747558594\n",
      "iteration : 1866, loss : 75.2450180053711\n",
      "iteration : 1867, loss : 75.24226379394531\n",
      "iteration : 1868, loss : 75.23949432373047\n",
      "iteration : 1869, loss : 75.23674774169922\n",
      "iteration : 1870, loss : 75.2339859008789\n",
      "iteration : 1871, loss : 75.23123168945312\n",
      "iteration : 1872, loss : 75.22848510742188\n",
      "iteration : 1873, loss : 75.22572326660156\n",
      "iteration : 1874, loss : 75.22297668457031\n",
      "iteration : 1875, loss : 75.22022247314453\n",
      "iteration : 1876, loss : 75.21746826171875\n",
      "iteration : 1877, loss : 75.21472930908203\n",
      "iteration : 1878, loss : 75.21196746826172\n",
      "iteration : 1879, loss : 75.209228515625\n",
      "iteration : 1880, loss : 75.20648956298828\n",
      "iteration : 1881, loss : 75.20374298095703\n",
      "iteration : 1882, loss : 75.20099639892578\n",
      "iteration : 1883, loss : 75.19824981689453\n",
      "iteration : 1884, loss : 75.19550323486328\n",
      "iteration : 1885, loss : 75.19276428222656\n",
      "iteration : 1886, loss : 75.19002532958984\n",
      "iteration : 1887, loss : 75.18729400634766\n",
      "iteration : 1888, loss : 75.18453979492188\n",
      "iteration : 1889, loss : 75.18180084228516\n",
      "iteration : 1890, loss : 75.1790771484375\n",
      "iteration : 1891, loss : 75.17634582519531\n",
      "iteration : 1892, loss : 75.17361450195312\n",
      "iteration : 1893, loss : 75.17086791992188\n",
      "iteration : 1894, loss : 75.16813659667969\n",
      "iteration : 1895, loss : 75.16541290283203\n",
      "iteration : 1896, loss : 75.16268157958984\n",
      "iteration : 1897, loss : 75.15995788574219\n",
      "iteration : 1898, loss : 75.15721893310547\n",
      "iteration : 1899, loss : 75.15449523925781\n",
      "iteration : 1900, loss : 75.15177154541016\n",
      "iteration : 1901, loss : 75.1490478515625\n",
      "iteration : 1902, loss : 75.14632415771484\n",
      "iteration : 1903, loss : 75.14360809326172\n",
      "iteration : 1904, loss : 75.14087677001953\n",
      "iteration : 1905, loss : 75.1381607055664\n",
      "iteration : 1906, loss : 75.13544464111328\n",
      "iteration : 1907, loss : 75.13272094726562\n",
      "iteration : 1908, loss : 75.1300048828125\n",
      "iteration : 1909, loss : 75.1272964477539\n",
      "iteration : 1910, loss : 75.12457275390625\n",
      "iteration : 1911, loss : 75.12185668945312\n",
      "iteration : 1912, loss : 75.119140625\n",
      "iteration : 1913, loss : 75.11642456054688\n",
      "iteration : 1914, loss : 75.11371612548828\n",
      "iteration : 1915, loss : 75.11100769042969\n",
      "iteration : 1916, loss : 75.1082992553711\n",
      "iteration : 1917, loss : 75.1055908203125\n",
      "iteration : 1918, loss : 75.1028823852539\n",
      "iteration : 1919, loss : 75.10018157958984\n",
      "iteration : 1920, loss : 75.09747314453125\n",
      "iteration : 1921, loss : 75.09477233886719\n",
      "iteration : 1922, loss : 75.09205627441406\n",
      "iteration : 1923, loss : 75.08935546875\n",
      "iteration : 1924, loss : 75.08665466308594\n",
      "iteration : 1925, loss : 75.0839614868164\n",
      "iteration : 1926, loss : 75.08126068115234\n",
      "iteration : 1927, loss : 75.07855987548828\n",
      "iteration : 1928, loss : 75.07585906982422\n",
      "iteration : 1929, loss : 75.07315826416016\n",
      "iteration : 1930, loss : 75.0704574584961\n",
      "iteration : 1931, loss : 75.06776428222656\n",
      "iteration : 1932, loss : 75.06507873535156\n",
      "iteration : 1933, loss : 75.0623779296875\n",
      "iteration : 1934, loss : 75.05968475341797\n",
      "iteration : 1935, loss : 75.05699157714844\n",
      "iteration : 1936, loss : 75.05430603027344\n",
      "iteration : 1937, loss : 75.0516128540039\n",
      "iteration : 1938, loss : 75.04891967773438\n",
      "iteration : 1939, loss : 75.0462417602539\n",
      "iteration : 1940, loss : 75.04354858398438\n",
      "iteration : 1941, loss : 75.04085540771484\n",
      "iteration : 1942, loss : 75.0381851196289\n",
      "iteration : 1943, loss : 75.0354995727539\n",
      "iteration : 1944, loss : 75.03280639648438\n",
      "iteration : 1945, loss : 75.03012084960938\n",
      "iteration : 1946, loss : 75.0274429321289\n",
      "iteration : 1947, loss : 75.02477264404297\n",
      "iteration : 1948, loss : 75.02207946777344\n",
      "iteration : 1949, loss : 75.0194091796875\n",
      "iteration : 1950, loss : 75.0167236328125\n",
      "iteration : 1951, loss : 75.01404571533203\n",
      "iteration : 1952, loss : 75.01136779785156\n",
      "iteration : 1953, loss : 75.0086898803711\n",
      "iteration : 1954, loss : 75.00601959228516\n",
      "iteration : 1955, loss : 75.00334930419922\n",
      "iteration : 1956, loss : 75.00067901611328\n",
      "iteration : 1957, loss : 74.99800872802734\n",
      "iteration : 1958, loss : 74.9953384399414\n",
      "iteration : 1959, loss : 74.99266815185547\n",
      "iteration : 1960, loss : 74.98999786376953\n",
      "iteration : 1961, loss : 74.98733520507812\n",
      "iteration : 1962, loss : 74.98464965820312\n",
      "iteration : 1963, loss : 74.98199462890625\n",
      "iteration : 1964, loss : 74.97933197021484\n",
      "iteration : 1965, loss : 74.9766616821289\n",
      "iteration : 1966, loss : 74.97399139404297\n",
      "iteration : 1967, loss : 74.9713363647461\n",
      "iteration : 1968, loss : 74.96866607666016\n",
      "iteration : 1969, loss : 74.96601104736328\n",
      "iteration : 1970, loss : 74.9633560180664\n",
      "iteration : 1971, loss : 74.960693359375\n",
      "iteration : 1972, loss : 74.9580307006836\n",
      "iteration : 1973, loss : 74.95537567138672\n",
      "iteration : 1974, loss : 74.95271301269531\n",
      "iteration : 1975, loss : 74.95006561279297\n",
      "iteration : 1976, loss : 74.94740295410156\n",
      "iteration : 1977, loss : 74.94475555419922\n",
      "iteration : 1978, loss : 74.94210052490234\n",
      "iteration : 1979, loss : 74.939453125\n",
      "iteration : 1980, loss : 74.93680572509766\n",
      "iteration : 1981, loss : 74.93413543701172\n",
      "iteration : 1982, loss : 74.9314956665039\n",
      "iteration : 1983, loss : 74.92884826660156\n",
      "iteration : 1984, loss : 74.92619323730469\n",
      "iteration : 1985, loss : 74.92355346679688\n",
      "iteration : 1986, loss : 74.92090606689453\n",
      "iteration : 1987, loss : 74.91825103759766\n",
      "iteration : 1988, loss : 74.91561126708984\n",
      "iteration : 1989, loss : 74.91297149658203\n",
      "iteration : 1990, loss : 74.91032409667969\n",
      "iteration : 1991, loss : 74.90768432617188\n",
      "iteration : 1992, loss : 74.90504455566406\n",
      "iteration : 1993, loss : 74.90238952636719\n",
      "iteration : 1994, loss : 74.8997573852539\n",
      "iteration : 1995, loss : 74.89712524414062\n",
      "iteration : 1996, loss : 74.89448547363281\n",
      "iteration : 1997, loss : 74.891845703125\n",
      "iteration : 1998, loss : 74.88920593261719\n",
      "iteration : 1999, loss : 74.8865737915039\n",
      "iteration : 2000, loss : 74.88394165039062\n",
      "iteration : 2001, loss : 74.88130187988281\n",
      "iteration : 2002, loss : 74.87866973876953\n",
      "iteration : 2003, loss : 74.87603759765625\n",
      "iteration : 2004, loss : 74.87340545654297\n",
      "iteration : 2005, loss : 74.87077331542969\n",
      "iteration : 2006, loss : 74.8681411743164\n",
      "iteration : 2007, loss : 74.86550903320312\n",
      "iteration : 2008, loss : 74.86288452148438\n",
      "iteration : 2009, loss : 74.86024475097656\n",
      "iteration : 2010, loss : 74.85762786865234\n",
      "iteration : 2011, loss : 74.8550033569336\n",
      "iteration : 2012, loss : 74.85237884521484\n",
      "iteration : 2013, loss : 74.8497543334961\n",
      "iteration : 2014, loss : 74.84712982177734\n",
      "iteration : 2015, loss : 74.8445053100586\n",
      "iteration : 2016, loss : 74.84188079833984\n",
      "iteration : 2017, loss : 74.83926391601562\n",
      "iteration : 2018, loss : 74.8366470336914\n",
      "iteration : 2019, loss : 74.83401489257812\n",
      "iteration : 2020, loss : 74.83140563964844\n",
      "iteration : 2021, loss : 74.82878875732422\n",
      "iteration : 2022, loss : 74.82616424560547\n",
      "iteration : 2023, loss : 74.82355499267578\n",
      "iteration : 2024, loss : 74.82093811035156\n",
      "iteration : 2025, loss : 74.81831359863281\n",
      "iteration : 2026, loss : 74.81570434570312\n",
      "iteration : 2027, loss : 74.8130874633789\n",
      "iteration : 2028, loss : 74.81047058105469\n",
      "iteration : 2029, loss : 74.80786895751953\n",
      "iteration : 2030, loss : 74.80525207519531\n",
      "iteration : 2031, loss : 74.80264282226562\n",
      "iteration : 2032, loss : 74.80003356933594\n",
      "iteration : 2033, loss : 74.79743194580078\n",
      "iteration : 2034, loss : 74.79481506347656\n",
      "iteration : 2035, loss : 74.79220581054688\n",
      "iteration : 2036, loss : 74.78960418701172\n",
      "iteration : 2037, loss : 74.78699493408203\n",
      "iteration : 2038, loss : 74.78439331054688\n",
      "iteration : 2039, loss : 74.78178405761719\n",
      "iteration : 2040, loss : 74.77919006347656\n",
      "iteration : 2041, loss : 74.77658081054688\n",
      "iteration : 2042, loss : 74.77397155761719\n",
      "iteration : 2043, loss : 74.77137756347656\n",
      "iteration : 2044, loss : 74.7687759399414\n",
      "iteration : 2045, loss : 74.76616668701172\n",
      "iteration : 2046, loss : 74.7635726928711\n",
      "iteration : 2047, loss : 74.76097106933594\n",
      "iteration : 2048, loss : 74.75837707519531\n",
      "iteration : 2049, loss : 74.75578308105469\n",
      "iteration : 2050, loss : 74.7531967163086\n",
      "iteration : 2051, loss : 74.7505874633789\n",
      "iteration : 2052, loss : 74.74799346923828\n",
      "iteration : 2053, loss : 74.74540710449219\n",
      "iteration : 2054, loss : 74.74280548095703\n",
      "iteration : 2055, loss : 74.74021911621094\n",
      "iteration : 2056, loss : 74.73761749267578\n",
      "iteration : 2057, loss : 74.73502349853516\n",
      "iteration : 2058, loss : 74.7324447631836\n",
      "iteration : 2059, loss : 74.7298583984375\n",
      "iteration : 2060, loss : 74.72725677490234\n",
      "iteration : 2061, loss : 74.72467041015625\n",
      "iteration : 2062, loss : 74.72208404541016\n",
      "iteration : 2063, loss : 74.71949768066406\n",
      "iteration : 2064, loss : 74.71691131591797\n",
      "iteration : 2065, loss : 74.71432495117188\n",
      "iteration : 2066, loss : 74.71173858642578\n",
      "iteration : 2067, loss : 74.70915222167969\n",
      "iteration : 2068, loss : 74.70658111572266\n",
      "iteration : 2069, loss : 74.70398712158203\n",
      "iteration : 2070, loss : 74.70140838623047\n",
      "iteration : 2071, loss : 74.6988296508789\n",
      "iteration : 2072, loss : 74.69624328613281\n",
      "iteration : 2073, loss : 74.69366455078125\n",
      "iteration : 2074, loss : 74.69109344482422\n",
      "iteration : 2075, loss : 74.68850708007812\n",
      "iteration : 2076, loss : 74.6859359741211\n",
      "iteration : 2077, loss : 74.68335723876953\n",
      "iteration : 2078, loss : 74.6807861328125\n",
      "iteration : 2079, loss : 74.67820739746094\n",
      "iteration : 2080, loss : 74.6756362915039\n",
      "iteration : 2081, loss : 74.67306518554688\n",
      "iteration : 2082, loss : 74.67047882080078\n",
      "iteration : 2083, loss : 74.66790771484375\n",
      "iteration : 2084, loss : 74.66534423828125\n",
      "iteration : 2085, loss : 74.66276550292969\n",
      "iteration : 2086, loss : 74.66020202636719\n",
      "iteration : 2087, loss : 74.65762329101562\n",
      "iteration : 2088, loss : 74.65505981445312\n",
      "iteration : 2089, loss : 74.6524887084961\n",
      "iteration : 2090, loss : 74.64991760253906\n",
      "iteration : 2091, loss : 74.64735412597656\n",
      "iteration : 2092, loss : 74.64479064941406\n",
      "iteration : 2093, loss : 74.64221954345703\n",
      "iteration : 2094, loss : 74.63966369628906\n",
      "iteration : 2095, loss : 74.63709259033203\n",
      "iteration : 2096, loss : 74.63452911376953\n",
      "iteration : 2097, loss : 74.63196563720703\n",
      "iteration : 2098, loss : 74.62940979003906\n",
      "iteration : 2099, loss : 74.62684631347656\n",
      "iteration : 2100, loss : 74.62428283691406\n",
      "iteration : 2101, loss : 74.6217269897461\n",
      "iteration : 2102, loss : 74.6191635131836\n",
      "iteration : 2103, loss : 74.61660766601562\n",
      "iteration : 2104, loss : 74.61404418945312\n",
      "iteration : 2105, loss : 74.61148071289062\n",
      "iteration : 2106, loss : 74.60892486572266\n",
      "iteration : 2107, loss : 74.60636901855469\n",
      "iteration : 2108, loss : 74.60381317138672\n",
      "iteration : 2109, loss : 74.60127258300781\n",
      "iteration : 2110, loss : 74.59871673583984\n",
      "iteration : 2111, loss : 74.59616088867188\n",
      "iteration : 2112, loss : 74.5936050415039\n",
      "iteration : 2113, loss : 74.59104919433594\n",
      "iteration : 2114, loss : 74.5885009765625\n",
      "iteration : 2115, loss : 74.5859375\n",
      "iteration : 2116, loss : 74.5833969116211\n",
      "iteration : 2117, loss : 74.58084869384766\n",
      "iteration : 2118, loss : 74.57829284667969\n",
      "iteration : 2119, loss : 74.57574462890625\n",
      "iteration : 2120, loss : 74.57320404052734\n",
      "iteration : 2121, loss : 74.57066345214844\n",
      "iteration : 2122, loss : 74.56810760498047\n",
      "iteration : 2123, loss : 74.5655746459961\n",
      "iteration : 2124, loss : 74.56301879882812\n",
      "iteration : 2125, loss : 74.56047058105469\n",
      "iteration : 2126, loss : 74.55792999267578\n",
      "iteration : 2127, loss : 74.55538940429688\n",
      "iteration : 2128, loss : 74.55284881591797\n",
      "iteration : 2129, loss : 74.55030059814453\n",
      "iteration : 2130, loss : 74.54776000976562\n",
      "iteration : 2131, loss : 74.54521942138672\n",
      "iteration : 2132, loss : 74.54269409179688\n",
      "iteration : 2133, loss : 74.54014587402344\n",
      "iteration : 2134, loss : 74.53760528564453\n",
      "iteration : 2135, loss : 74.53506469726562\n",
      "iteration : 2136, loss : 74.53253173828125\n",
      "iteration : 2137, loss : 74.52999877929688\n",
      "iteration : 2138, loss : 74.52745819091797\n",
      "iteration : 2139, loss : 74.52491760253906\n",
      "iteration : 2140, loss : 74.52238464355469\n",
      "iteration : 2141, loss : 74.51984405517578\n",
      "iteration : 2142, loss : 74.51731872558594\n",
      "iteration : 2143, loss : 74.51478576660156\n",
      "iteration : 2144, loss : 74.51226043701172\n",
      "iteration : 2145, loss : 74.50971984863281\n",
      "iteration : 2146, loss : 74.50719451904297\n",
      "iteration : 2147, loss : 74.50465393066406\n",
      "iteration : 2148, loss : 74.50212860107422\n",
      "iteration : 2149, loss : 74.49959564208984\n",
      "iteration : 2150, loss : 74.49707794189453\n",
      "iteration : 2151, loss : 74.49453735351562\n",
      "iteration : 2152, loss : 74.49201965332031\n",
      "iteration : 2153, loss : 74.48948669433594\n",
      "iteration : 2154, loss : 74.4869613647461\n",
      "iteration : 2155, loss : 74.48443603515625\n",
      "iteration : 2156, loss : 74.4819107055664\n",
      "iteration : 2157, loss : 74.47938537597656\n",
      "iteration : 2158, loss : 74.47686767578125\n",
      "iteration : 2159, loss : 74.4743423461914\n",
      "iteration : 2160, loss : 74.4718246459961\n",
      "iteration : 2161, loss : 74.46929931640625\n",
      "iteration : 2162, loss : 74.46678161621094\n",
      "iteration : 2163, loss : 74.4642562866211\n",
      "iteration : 2164, loss : 74.46173858642578\n",
      "iteration : 2165, loss : 74.45922088623047\n",
      "iteration : 2166, loss : 74.45669555664062\n",
      "iteration : 2167, loss : 74.45417785644531\n",
      "iteration : 2168, loss : 74.45166778564453\n",
      "iteration : 2169, loss : 74.44915008544922\n",
      "iteration : 2170, loss : 74.44662475585938\n",
      "iteration : 2171, loss : 74.4441146850586\n",
      "iteration : 2172, loss : 74.44159698486328\n",
      "iteration : 2173, loss : 74.43907928466797\n",
      "iteration : 2174, loss : 74.43657684326172\n",
      "iteration : 2175, loss : 74.43406677246094\n",
      "iteration : 2176, loss : 74.43154907226562\n",
      "iteration : 2177, loss : 74.42902374267578\n",
      "iteration : 2178, loss : 74.42652130126953\n",
      "iteration : 2179, loss : 74.42400360107422\n",
      "iteration : 2180, loss : 74.42150115966797\n",
      "iteration : 2181, loss : 74.41899108886719\n",
      "iteration : 2182, loss : 74.4164810180664\n",
      "iteration : 2183, loss : 74.41397857666016\n",
      "iteration : 2184, loss : 74.41146087646484\n",
      "iteration : 2185, loss : 74.40895080566406\n",
      "iteration : 2186, loss : 74.40644836425781\n",
      "iteration : 2187, loss : 74.40394592285156\n",
      "iteration : 2188, loss : 74.40144348144531\n",
      "iteration : 2189, loss : 74.39893341064453\n",
      "iteration : 2190, loss : 74.39643096923828\n",
      "iteration : 2191, loss : 74.3939208984375\n",
      "iteration : 2192, loss : 74.39141845703125\n",
      "iteration : 2193, loss : 74.38890838623047\n",
      "iteration : 2194, loss : 74.38641357421875\n",
      "iteration : 2195, loss : 74.38391876220703\n",
      "iteration : 2196, loss : 74.38140869140625\n",
      "iteration : 2197, loss : 74.37891387939453\n",
      "iteration : 2198, loss : 74.37641143798828\n",
      "iteration : 2199, loss : 74.37391662597656\n",
      "iteration : 2200, loss : 74.37141418457031\n",
      "iteration : 2201, loss : 74.36892700195312\n",
      "iteration : 2202, loss : 74.36641693115234\n",
      "iteration : 2203, loss : 74.36392211914062\n",
      "iteration : 2204, loss : 74.3614273071289\n",
      "iteration : 2205, loss : 74.35891723632812\n",
      "iteration : 2206, loss : 74.35643005371094\n",
      "iteration : 2207, loss : 74.35394287109375\n",
      "iteration : 2208, loss : 74.3514404296875\n",
      "iteration : 2209, loss : 74.34894561767578\n",
      "iteration : 2210, loss : 74.34646606445312\n",
      "iteration : 2211, loss : 74.34395599365234\n",
      "iteration : 2212, loss : 74.34147644042969\n",
      "iteration : 2213, loss : 74.33898162841797\n",
      "iteration : 2214, loss : 74.33648681640625\n",
      "iteration : 2215, loss : 74.33399963378906\n",
      "iteration : 2216, loss : 74.33150482177734\n",
      "iteration : 2217, loss : 74.32901763916016\n",
      "iteration : 2218, loss : 74.3265380859375\n",
      "iteration : 2219, loss : 74.32405090332031\n",
      "iteration : 2220, loss : 74.32156372070312\n",
      "iteration : 2221, loss : 74.31907653808594\n",
      "iteration : 2222, loss : 74.31658172607422\n",
      "iteration : 2223, loss : 74.31410217285156\n",
      "iteration : 2224, loss : 74.31161499023438\n",
      "iteration : 2225, loss : 74.30913543701172\n",
      "iteration : 2226, loss : 74.30664825439453\n",
      "iteration : 2227, loss : 74.30415344238281\n",
      "iteration : 2228, loss : 74.30167388916016\n",
      "iteration : 2229, loss : 74.29920959472656\n",
      "iteration : 2230, loss : 74.29672241210938\n",
      "iteration : 2231, loss : 74.29423522949219\n",
      "iteration : 2232, loss : 74.29175567626953\n",
      "iteration : 2233, loss : 74.28926849365234\n",
      "iteration : 2234, loss : 74.28679656982422\n",
      "iteration : 2235, loss : 74.28430938720703\n",
      "iteration : 2236, loss : 74.2818374633789\n",
      "iteration : 2237, loss : 74.27935791015625\n",
      "iteration : 2238, loss : 74.2768783569336\n",
      "iteration : 2239, loss : 74.2743911743164\n",
      "iteration : 2240, loss : 74.27191925048828\n",
      "iteration : 2241, loss : 74.26944732666016\n",
      "iteration : 2242, loss : 74.26697540283203\n",
      "iteration : 2243, loss : 74.26449584960938\n",
      "iteration : 2244, loss : 74.26202392578125\n",
      "iteration : 2245, loss : 74.2595443725586\n",
      "iteration : 2246, loss : 74.257080078125\n",
      "iteration : 2247, loss : 74.25460052490234\n",
      "iteration : 2248, loss : 74.25213623046875\n",
      "iteration : 2249, loss : 74.2496566772461\n",
      "iteration : 2250, loss : 74.2471923828125\n",
      "iteration : 2251, loss : 74.24472045898438\n",
      "iteration : 2252, loss : 74.24224090576172\n",
      "iteration : 2253, loss : 74.23978424072266\n",
      "iteration : 2254, loss : 74.23731231689453\n",
      "iteration : 2255, loss : 74.2348403930664\n",
      "iteration : 2256, loss : 74.23237609863281\n",
      "iteration : 2257, loss : 74.22991180419922\n",
      "iteration : 2258, loss : 74.22743225097656\n",
      "iteration : 2259, loss : 74.22498321533203\n",
      "iteration : 2260, loss : 74.22250366210938\n",
      "iteration : 2261, loss : 74.22003173828125\n",
      "iteration : 2262, loss : 74.21757507324219\n",
      "iteration : 2263, loss : 74.21510314941406\n",
      "iteration : 2264, loss : 74.212646484375\n",
      "iteration : 2265, loss : 74.2101821899414\n",
      "iteration : 2266, loss : 74.20771789550781\n",
      "iteration : 2267, loss : 74.20525360107422\n",
      "iteration : 2268, loss : 74.20280456542969\n",
      "iteration : 2269, loss : 74.20033264160156\n",
      "iteration : 2270, loss : 74.1978759765625\n",
      "iteration : 2271, loss : 74.1954116821289\n",
      "iteration : 2272, loss : 74.19295501708984\n",
      "iteration : 2273, loss : 74.19049835205078\n",
      "iteration : 2274, loss : 74.18804168701172\n",
      "iteration : 2275, loss : 74.18557739257812\n",
      "iteration : 2276, loss : 74.18310546875\n",
      "iteration : 2277, loss : 74.18065643310547\n",
      "iteration : 2278, loss : 74.17820739746094\n",
      "iteration : 2279, loss : 74.17575073242188\n",
      "iteration : 2280, loss : 74.17328643798828\n",
      "iteration : 2281, loss : 74.17082977294922\n",
      "iteration : 2282, loss : 74.16838073730469\n",
      "iteration : 2283, loss : 74.16593933105469\n",
      "iteration : 2284, loss : 74.1634750366211\n",
      "iteration : 2285, loss : 74.16101837158203\n",
      "iteration : 2286, loss : 74.15857696533203\n",
      "iteration : 2287, loss : 74.1561279296875\n",
      "iteration : 2288, loss : 74.1536636352539\n",
      "iteration : 2289, loss : 74.15121459960938\n",
      "iteration : 2290, loss : 74.14875793457031\n",
      "iteration : 2291, loss : 74.14632415771484\n",
      "iteration : 2292, loss : 74.14385986328125\n",
      "iteration : 2293, loss : 74.14142608642578\n",
      "iteration : 2294, loss : 74.13896179199219\n",
      "iteration : 2295, loss : 74.13652038574219\n",
      "iteration : 2296, loss : 74.13407135009766\n",
      "iteration : 2297, loss : 74.13162994384766\n",
      "iteration : 2298, loss : 74.1291732788086\n",
      "iteration : 2299, loss : 74.1267318725586\n",
      "iteration : 2300, loss : 74.1242904663086\n",
      "iteration : 2301, loss : 74.12183380126953\n",
      "iteration : 2302, loss : 74.11939239501953\n",
      "iteration : 2303, loss : 74.11695861816406\n",
      "iteration : 2304, loss : 74.11451721191406\n",
      "iteration : 2305, loss : 74.11206817626953\n",
      "iteration : 2306, loss : 74.109619140625\n",
      "iteration : 2307, loss : 74.10718536376953\n",
      "iteration : 2308, loss : 74.104736328125\n",
      "iteration : 2309, loss : 74.102294921875\n",
      "iteration : 2310, loss : 74.09986114501953\n",
      "iteration : 2311, loss : 74.09741973876953\n",
      "iteration : 2312, loss : 74.09497833251953\n",
      "iteration : 2313, loss : 74.09253692626953\n",
      "iteration : 2314, loss : 74.09009552001953\n",
      "iteration : 2315, loss : 74.08766174316406\n",
      "iteration : 2316, loss : 74.0852279663086\n",
      "iteration : 2317, loss : 74.0827865600586\n",
      "iteration : 2318, loss : 74.08035278320312\n",
      "iteration : 2319, loss : 74.07791900634766\n",
      "iteration : 2320, loss : 74.07547760009766\n",
      "iteration : 2321, loss : 74.07304382324219\n",
      "iteration : 2322, loss : 74.07060241699219\n",
      "iteration : 2323, loss : 74.06817626953125\n",
      "iteration : 2324, loss : 74.06574249267578\n",
      "iteration : 2325, loss : 74.06330871582031\n",
      "iteration : 2326, loss : 74.06086730957031\n",
      "iteration : 2327, loss : 74.0584487915039\n",
      "iteration : 2328, loss : 74.0560073852539\n",
      "iteration : 2329, loss : 74.05357360839844\n",
      "iteration : 2330, loss : 74.0511474609375\n",
      "iteration : 2331, loss : 74.04871368408203\n",
      "iteration : 2332, loss : 74.0462875366211\n",
      "iteration : 2333, loss : 74.04385375976562\n",
      "iteration : 2334, loss : 74.04142761230469\n",
      "iteration : 2335, loss : 74.03899383544922\n",
      "iteration : 2336, loss : 74.03656768798828\n",
      "iteration : 2337, loss : 74.03414154052734\n",
      "iteration : 2338, loss : 74.0317153930664\n",
      "iteration : 2339, loss : 74.02928924560547\n",
      "iteration : 2340, loss : 74.02685546875\n",
      "iteration : 2341, loss : 74.02444458007812\n",
      "iteration : 2342, loss : 74.02201843261719\n",
      "iteration : 2343, loss : 74.01958465576172\n",
      "iteration : 2344, loss : 74.01716613769531\n",
      "iteration : 2345, loss : 74.01473999023438\n",
      "iteration : 2346, loss : 74.0123062133789\n",
      "iteration : 2347, loss : 74.0098876953125\n",
      "iteration : 2348, loss : 74.00746154785156\n",
      "iteration : 2349, loss : 74.00504302978516\n",
      "iteration : 2350, loss : 74.00263214111328\n",
      "iteration : 2351, loss : 74.00019836425781\n",
      "iteration : 2352, loss : 73.99778747558594\n",
      "iteration : 2353, loss : 73.995361328125\n",
      "iteration : 2354, loss : 73.9929428100586\n",
      "iteration : 2355, loss : 73.99051666259766\n",
      "iteration : 2356, loss : 73.98810577392578\n",
      "iteration : 2357, loss : 73.98567962646484\n",
      "iteration : 2358, loss : 73.98326110839844\n",
      "iteration : 2359, loss : 73.98085021972656\n",
      "iteration : 2360, loss : 73.97843170166016\n",
      "iteration : 2361, loss : 73.97602081298828\n",
      "iteration : 2362, loss : 73.97359466552734\n",
      "iteration : 2363, loss : 73.97119140625\n",
      "iteration : 2364, loss : 73.9687728881836\n",
      "iteration : 2365, loss : 73.96636199951172\n",
      "iteration : 2366, loss : 73.96395111083984\n",
      "iteration : 2367, loss : 73.9615249633789\n",
      "iteration : 2368, loss : 73.95911407470703\n",
      "iteration : 2369, loss : 73.95669555664062\n",
      "iteration : 2370, loss : 73.95429992675781\n",
      "iteration : 2371, loss : 73.9518814086914\n",
      "iteration : 2372, loss : 73.949462890625\n",
      "iteration : 2373, loss : 73.94705200195312\n",
      "iteration : 2374, loss : 73.94464111328125\n",
      "iteration : 2375, loss : 73.94222259521484\n",
      "iteration : 2376, loss : 73.93982696533203\n",
      "iteration : 2377, loss : 73.93741607666016\n",
      "iteration : 2378, loss : 73.93499755859375\n",
      "iteration : 2379, loss : 73.9325942993164\n",
      "iteration : 2380, loss : 73.93019104003906\n",
      "iteration : 2381, loss : 73.92777252197266\n",
      "iteration : 2382, loss : 73.92536926269531\n",
      "iteration : 2383, loss : 73.92296600341797\n",
      "iteration : 2384, loss : 73.9205551147461\n",
      "iteration : 2385, loss : 73.91814422607422\n",
      "iteration : 2386, loss : 73.9157485961914\n",
      "iteration : 2387, loss : 73.91334533691406\n",
      "iteration : 2388, loss : 73.91094207763672\n",
      "iteration : 2389, loss : 73.90853881835938\n",
      "iteration : 2390, loss : 73.9061279296875\n",
      "iteration : 2391, loss : 73.90373229980469\n",
      "iteration : 2392, loss : 73.90132141113281\n",
      "iteration : 2393, loss : 73.89891815185547\n",
      "iteration : 2394, loss : 73.89651489257812\n",
      "iteration : 2395, loss : 73.89411163330078\n",
      "iteration : 2396, loss : 73.8917236328125\n",
      "iteration : 2397, loss : 73.88932037353516\n",
      "iteration : 2398, loss : 73.88691711425781\n",
      "iteration : 2399, loss : 73.884521484375\n",
      "iteration : 2400, loss : 73.88211822509766\n",
      "iteration : 2401, loss : 73.87971496582031\n",
      "iteration : 2402, loss : 73.87733459472656\n",
      "iteration : 2403, loss : 73.87492370605469\n",
      "iteration : 2404, loss : 73.87252044677734\n",
      "iteration : 2405, loss : 73.87013244628906\n",
      "iteration : 2406, loss : 73.86773681640625\n",
      "iteration : 2407, loss : 73.8653335571289\n",
      "iteration : 2408, loss : 73.86294555664062\n",
      "iteration : 2409, loss : 73.86054992675781\n",
      "iteration : 2410, loss : 73.85814666748047\n",
      "iteration : 2411, loss : 73.85575866699219\n",
      "iteration : 2412, loss : 73.8533706665039\n",
      "iteration : 2413, loss : 73.8509750366211\n",
      "iteration : 2414, loss : 73.84857177734375\n",
      "iteration : 2415, loss : 73.84618377685547\n",
      "iteration : 2416, loss : 73.84378814697266\n",
      "iteration : 2417, loss : 73.84140014648438\n",
      "iteration : 2418, loss : 73.8390121459961\n",
      "iteration : 2419, loss : 73.83661651611328\n",
      "iteration : 2420, loss : 73.83422088623047\n",
      "iteration : 2421, loss : 73.83184051513672\n",
      "iteration : 2422, loss : 73.8294448852539\n",
      "iteration : 2423, loss : 73.82705688476562\n",
      "iteration : 2424, loss : 73.82466888427734\n",
      "iteration : 2425, loss : 73.82227325439453\n",
      "iteration : 2426, loss : 73.81989288330078\n",
      "iteration : 2427, loss : 73.8175048828125\n",
      "iteration : 2428, loss : 73.81512451171875\n",
      "iteration : 2429, loss : 73.81272888183594\n",
      "iteration : 2430, loss : 73.81034851074219\n",
      "iteration : 2431, loss : 73.8079605102539\n",
      "iteration : 2432, loss : 73.8055648803711\n",
      "iteration : 2433, loss : 73.80318450927734\n",
      "iteration : 2434, loss : 73.80079650878906\n",
      "iteration : 2435, loss : 73.79841613769531\n",
      "iteration : 2436, loss : 73.79603576660156\n",
      "iteration : 2437, loss : 73.79364013671875\n",
      "iteration : 2438, loss : 73.791259765625\n",
      "iteration : 2439, loss : 73.78888702392578\n",
      "iteration : 2440, loss : 73.7864990234375\n",
      "iteration : 2441, loss : 73.78411865234375\n",
      "iteration : 2442, loss : 73.78173828125\n",
      "iteration : 2443, loss : 73.77935028076172\n",
      "iteration : 2444, loss : 73.77696990966797\n",
      "iteration : 2445, loss : 73.77459716796875\n",
      "iteration : 2446, loss : 73.772216796875\n",
      "iteration : 2447, loss : 73.76983642578125\n",
      "iteration : 2448, loss : 73.76746368408203\n",
      "iteration : 2449, loss : 73.76508331298828\n",
      "iteration : 2450, loss : 73.76270294189453\n",
      "iteration : 2451, loss : 73.76033782958984\n",
      "iteration : 2452, loss : 73.75794219970703\n",
      "iteration : 2453, loss : 73.75557708740234\n",
      "iteration : 2454, loss : 73.7531967163086\n",
      "iteration : 2455, loss : 73.75081634521484\n",
      "iteration : 2456, loss : 73.74844360351562\n",
      "iteration : 2457, loss : 73.7460708618164\n",
      "iteration : 2458, loss : 73.74369049072266\n",
      "iteration : 2459, loss : 73.74131774902344\n",
      "iteration : 2460, loss : 73.73894500732422\n",
      "iteration : 2461, loss : 73.736572265625\n",
      "iteration : 2462, loss : 73.73420715332031\n",
      "iteration : 2463, loss : 73.73182678222656\n",
      "iteration : 2464, loss : 73.72944641113281\n",
      "iteration : 2465, loss : 73.72708129882812\n",
      "iteration : 2466, loss : 73.7247085571289\n",
      "iteration : 2467, loss : 73.72234344482422\n",
      "iteration : 2468, loss : 73.71996307373047\n",
      "iteration : 2469, loss : 73.71759796142578\n",
      "iteration : 2470, loss : 73.71522521972656\n",
      "iteration : 2471, loss : 73.71286010742188\n",
      "iteration : 2472, loss : 73.71048736572266\n",
      "iteration : 2473, loss : 73.70812225341797\n",
      "iteration : 2474, loss : 73.70574951171875\n",
      "iteration : 2475, loss : 73.70338439941406\n",
      "iteration : 2476, loss : 73.70101928710938\n",
      "iteration : 2477, loss : 73.69865417480469\n",
      "iteration : 2478, loss : 73.69628143310547\n",
      "iteration : 2479, loss : 73.69392395019531\n",
      "iteration : 2480, loss : 73.69154357910156\n",
      "iteration : 2481, loss : 73.6891860961914\n",
      "iteration : 2482, loss : 73.68682098388672\n",
      "iteration : 2483, loss : 73.6844482421875\n",
      "iteration : 2484, loss : 73.68209838867188\n",
      "iteration : 2485, loss : 73.67972564697266\n",
      "iteration : 2486, loss : 73.67736053466797\n",
      "iteration : 2487, loss : 73.67499542236328\n",
      "iteration : 2488, loss : 73.67263793945312\n",
      "iteration : 2489, loss : 73.67027282714844\n",
      "iteration : 2490, loss : 73.66791534423828\n",
      "iteration : 2491, loss : 73.6655502319336\n",
      "iteration : 2492, loss : 73.6631851196289\n",
      "iteration : 2493, loss : 73.66083526611328\n",
      "iteration : 2494, loss : 73.6584701538086\n",
      "iteration : 2495, loss : 73.65611267089844\n",
      "iteration : 2496, loss : 73.65374755859375\n",
      "iteration : 2497, loss : 73.65139770507812\n",
      "iteration : 2498, loss : 73.64903259277344\n",
      "iteration : 2499, loss : 73.64668273925781\n",
      "iteration : 2500, loss : 73.64431762695312\n",
      "iteration : 2501, loss : 73.64196014404297\n",
      "iteration : 2502, loss : 73.63960266113281\n",
      "iteration : 2503, loss : 73.63724517822266\n",
      "iteration : 2504, loss : 73.6348876953125\n",
      "iteration : 2505, loss : 73.63253784179688\n",
      "iteration : 2506, loss : 73.63018035888672\n",
      "iteration : 2507, loss : 73.62781524658203\n",
      "iteration : 2508, loss : 73.6254653930664\n",
      "iteration : 2509, loss : 73.62310791015625\n",
      "iteration : 2510, loss : 73.62075805664062\n",
      "iteration : 2511, loss : 73.618408203125\n",
      "iteration : 2512, loss : 73.61604309082031\n",
      "iteration : 2513, loss : 73.61370086669922\n",
      "iteration : 2514, loss : 73.61133575439453\n",
      "iteration : 2515, loss : 73.6089859008789\n",
      "iteration : 2516, loss : 73.60664367675781\n",
      "iteration : 2517, loss : 73.60429382324219\n",
      "iteration : 2518, loss : 73.60194396972656\n",
      "iteration : 2519, loss : 73.5995864868164\n",
      "iteration : 2520, loss : 73.59723663330078\n",
      "iteration : 2521, loss : 73.59489440917969\n",
      "iteration : 2522, loss : 73.59253692626953\n",
      "iteration : 2523, loss : 73.59019470214844\n",
      "iteration : 2524, loss : 73.58784484863281\n",
      "iteration : 2525, loss : 73.58548736572266\n",
      "iteration : 2526, loss : 73.58314514160156\n",
      "iteration : 2527, loss : 73.58080291748047\n",
      "iteration : 2528, loss : 73.57846069335938\n",
      "iteration : 2529, loss : 73.57610321044922\n",
      "iteration : 2530, loss : 73.57376098632812\n",
      "iteration : 2531, loss : 73.5714111328125\n",
      "iteration : 2532, loss : 73.5690689086914\n",
      "iteration : 2533, loss : 73.56671142578125\n",
      "iteration : 2534, loss : 73.56436920166016\n",
      "iteration : 2535, loss : 73.56202697753906\n",
      "iteration : 2536, loss : 73.55968475341797\n",
      "iteration : 2537, loss : 73.5573501586914\n",
      "iteration : 2538, loss : 73.55498504638672\n",
      "iteration : 2539, loss : 73.55265045166016\n",
      "iteration : 2540, loss : 73.5503158569336\n",
      "iteration : 2541, loss : 73.54798126220703\n",
      "iteration : 2542, loss : 73.5456314086914\n",
      "iteration : 2543, loss : 73.54328155517578\n",
      "iteration : 2544, loss : 73.54094696044922\n",
      "iteration : 2545, loss : 73.5385971069336\n",
      "iteration : 2546, loss : 73.53626251220703\n",
      "iteration : 2547, loss : 73.53392791748047\n",
      "iteration : 2548, loss : 73.5315933227539\n",
      "iteration : 2549, loss : 73.52923583984375\n",
      "iteration : 2550, loss : 73.52690124511719\n",
      "iteration : 2551, loss : 73.52456665039062\n",
      "iteration : 2552, loss : 73.52223205566406\n",
      "iteration : 2553, loss : 73.5198974609375\n",
      "iteration : 2554, loss : 73.5175552368164\n",
      "iteration : 2555, loss : 73.51522064208984\n",
      "iteration : 2556, loss : 73.51287841796875\n",
      "iteration : 2557, loss : 73.51053619384766\n",
      "iteration : 2558, loss : 73.50820922851562\n",
      "iteration : 2559, loss : 73.50587463378906\n",
      "iteration : 2560, loss : 73.50353240966797\n",
      "iteration : 2561, loss : 73.50121307373047\n",
      "iteration : 2562, loss : 73.4988784790039\n",
      "iteration : 2563, loss : 73.49654388427734\n",
      "iteration : 2564, loss : 73.49419403076172\n",
      "iteration : 2565, loss : 73.49187469482422\n",
      "iteration : 2566, loss : 73.48953247070312\n",
      "iteration : 2567, loss : 73.48719787597656\n",
      "iteration : 2568, loss : 73.48487091064453\n",
      "iteration : 2569, loss : 73.48253631591797\n",
      "iteration : 2570, loss : 73.48020935058594\n",
      "iteration : 2571, loss : 73.47787475585938\n",
      "iteration : 2572, loss : 73.47554779052734\n",
      "iteration : 2573, loss : 73.47322082519531\n",
      "iteration : 2574, loss : 73.47088623046875\n",
      "iteration : 2575, loss : 73.46856689453125\n",
      "iteration : 2576, loss : 73.46621704101562\n",
      "iteration : 2577, loss : 73.4638900756836\n",
      "iteration : 2578, loss : 73.46156311035156\n",
      "iteration : 2579, loss : 73.45923614501953\n",
      "iteration : 2580, loss : 73.4569091796875\n",
      "iteration : 2581, loss : 73.45457458496094\n",
      "iteration : 2582, loss : 73.45225524902344\n",
      "iteration : 2583, loss : 73.4499282836914\n",
      "iteration : 2584, loss : 73.44760131835938\n",
      "iteration : 2585, loss : 73.44527435302734\n",
      "iteration : 2586, loss : 73.44294738769531\n",
      "iteration : 2587, loss : 73.44062805175781\n",
      "iteration : 2588, loss : 73.43830108642578\n",
      "iteration : 2589, loss : 73.43598175048828\n",
      "iteration : 2590, loss : 73.43364715576172\n",
      "iteration : 2591, loss : 73.43133544921875\n",
      "iteration : 2592, loss : 73.42900848388672\n",
      "iteration : 2593, loss : 73.42667388916016\n",
      "iteration : 2594, loss : 73.42436218261719\n",
      "iteration : 2595, loss : 73.42202758789062\n",
      "iteration : 2596, loss : 73.41971588134766\n",
      "iteration : 2597, loss : 73.41739654541016\n",
      "iteration : 2598, loss : 73.41507720947266\n",
      "iteration : 2599, loss : 73.41275024414062\n",
      "iteration : 2600, loss : 73.41043090820312\n",
      "iteration : 2601, loss : 73.40811157226562\n",
      "iteration : 2602, loss : 73.4057846069336\n",
      "iteration : 2603, loss : 73.40347290039062\n",
      "iteration : 2604, loss : 73.40115356445312\n",
      "iteration : 2605, loss : 73.39884185791016\n",
      "iteration : 2606, loss : 73.39651489257812\n",
      "iteration : 2607, loss : 73.3941879272461\n",
      "iteration : 2608, loss : 73.39187622070312\n",
      "iteration : 2609, loss : 73.38956451416016\n",
      "iteration : 2610, loss : 73.38725280761719\n",
      "iteration : 2611, loss : 73.38492584228516\n",
      "iteration : 2612, loss : 73.38260650634766\n",
      "iteration : 2613, loss : 73.38028717041016\n",
      "iteration : 2614, loss : 73.37798309326172\n",
      "iteration : 2615, loss : 73.37567138671875\n",
      "iteration : 2616, loss : 73.37334442138672\n",
      "iteration : 2617, loss : 73.37104034423828\n",
      "iteration : 2618, loss : 73.36872100830078\n",
      "iteration : 2619, loss : 73.36640167236328\n",
      "iteration : 2620, loss : 73.36408996582031\n",
      "iteration : 2621, loss : 73.36177825927734\n",
      "iteration : 2622, loss : 73.35945892333984\n",
      "iteration : 2623, loss : 73.3571548461914\n",
      "iteration : 2624, loss : 73.35485076904297\n",
      "iteration : 2625, loss : 73.35252380371094\n",
      "iteration : 2626, loss : 73.3502197265625\n",
      "iteration : 2627, loss : 73.347900390625\n",
      "iteration : 2628, loss : 73.3456039428711\n",
      "iteration : 2629, loss : 73.34327697753906\n",
      "iteration : 2630, loss : 73.34098052978516\n",
      "iteration : 2631, loss : 73.33866882324219\n",
      "iteration : 2632, loss : 73.33636474609375\n",
      "iteration : 2633, loss : 73.33404541015625\n",
      "iteration : 2634, loss : 73.33174133300781\n",
      "iteration : 2635, loss : 73.32942199707031\n",
      "iteration : 2636, loss : 73.32711791992188\n",
      "iteration : 2637, loss : 73.32481384277344\n",
      "iteration : 2638, loss : 73.32250213623047\n",
      "iteration : 2639, loss : 73.32019805908203\n",
      "iteration : 2640, loss : 73.31788635253906\n",
      "iteration : 2641, loss : 73.31558990478516\n",
      "iteration : 2642, loss : 73.31328582763672\n",
      "iteration : 2643, loss : 73.31097412109375\n",
      "iteration : 2644, loss : 73.30867004394531\n",
      "iteration : 2645, loss : 73.30636596679688\n",
      "iteration : 2646, loss : 73.3040542602539\n",
      "iteration : 2647, loss : 73.30175018310547\n",
      "iteration : 2648, loss : 73.29944610595703\n",
      "iteration : 2649, loss : 73.2971420288086\n",
      "iteration : 2650, loss : 73.29484558105469\n",
      "iteration : 2651, loss : 73.29254150390625\n",
      "iteration : 2652, loss : 73.29023742675781\n",
      "iteration : 2653, loss : 73.2879409790039\n",
      "iteration : 2654, loss : 73.28563690185547\n",
      "iteration : 2655, loss : 73.28334045410156\n",
      "iteration : 2656, loss : 73.28103637695312\n",
      "iteration : 2657, loss : 73.27872467041016\n",
      "iteration : 2658, loss : 73.27642822265625\n",
      "iteration : 2659, loss : 73.27413177490234\n",
      "iteration : 2660, loss : 73.27183532714844\n",
      "iteration : 2661, loss : 73.26952362060547\n",
      "iteration : 2662, loss : 73.26721954345703\n",
      "iteration : 2663, loss : 73.26493835449219\n",
      "iteration : 2664, loss : 73.26262664794922\n",
      "iteration : 2665, loss : 73.26033020019531\n",
      "iteration : 2666, loss : 73.25804138183594\n",
      "iteration : 2667, loss : 73.25575256347656\n",
      "iteration : 2668, loss : 73.25344848632812\n",
      "iteration : 2669, loss : 73.25114440917969\n",
      "iteration : 2670, loss : 73.24885559082031\n",
      "iteration : 2671, loss : 73.2465591430664\n",
      "iteration : 2672, loss : 73.24424743652344\n",
      "iteration : 2673, loss : 73.24197387695312\n",
      "iteration : 2674, loss : 73.23966979980469\n",
      "iteration : 2675, loss : 73.23737335205078\n",
      "iteration : 2676, loss : 73.2350845336914\n",
      "iteration : 2677, loss : 73.2327880859375\n",
      "iteration : 2678, loss : 73.23048400878906\n",
      "iteration : 2679, loss : 73.22820281982422\n",
      "iteration : 2680, loss : 73.22590637207031\n",
      "iteration : 2681, loss : 73.22360229492188\n",
      "iteration : 2682, loss : 73.22132110595703\n",
      "iteration : 2683, loss : 73.21902465820312\n",
      "iteration : 2684, loss : 73.21673583984375\n",
      "iteration : 2685, loss : 73.21443939208984\n",
      "iteration : 2686, loss : 73.21215057373047\n",
      "iteration : 2687, loss : 73.20985412597656\n",
      "iteration : 2688, loss : 73.20757293701172\n",
      "iteration : 2689, loss : 73.20527648925781\n",
      "iteration : 2690, loss : 73.20298767089844\n",
      "iteration : 2691, loss : 73.20069885253906\n",
      "iteration : 2692, loss : 73.19841003417969\n",
      "iteration : 2693, loss : 73.19612121582031\n",
      "iteration : 2694, loss : 73.19384002685547\n",
      "iteration : 2695, loss : 73.19154357910156\n",
      "iteration : 2696, loss : 73.18926239013672\n",
      "iteration : 2697, loss : 73.18696594238281\n",
      "iteration : 2698, loss : 73.1846923828125\n",
      "iteration : 2699, loss : 73.18240356445312\n",
      "iteration : 2700, loss : 73.18012237548828\n",
      "iteration : 2701, loss : 73.17782592773438\n",
      "iteration : 2702, loss : 73.17554473876953\n",
      "iteration : 2703, loss : 73.17325592041016\n",
      "iteration : 2704, loss : 73.17095947265625\n",
      "iteration : 2705, loss : 73.1686782836914\n",
      "iteration : 2706, loss : 73.1664047241211\n",
      "iteration : 2707, loss : 73.16411590576172\n",
      "iteration : 2708, loss : 73.16183471679688\n",
      "iteration : 2709, loss : 73.1595458984375\n",
      "iteration : 2710, loss : 73.15727233886719\n",
      "iteration : 2711, loss : 73.15498352050781\n",
      "iteration : 2712, loss : 73.15270233154297\n",
      "iteration : 2713, loss : 73.15042114257812\n",
      "iteration : 2714, loss : 73.14813232421875\n",
      "iteration : 2715, loss : 73.1458511352539\n",
      "iteration : 2716, loss : 73.1435775756836\n",
      "iteration : 2717, loss : 73.14128112792969\n",
      "iteration : 2718, loss : 73.1390151977539\n",
      "iteration : 2719, loss : 73.13673400878906\n",
      "iteration : 2720, loss : 73.13445281982422\n",
      "iteration : 2721, loss : 73.13217163085938\n",
      "iteration : 2722, loss : 73.12989807128906\n",
      "iteration : 2723, loss : 73.12761688232422\n",
      "iteration : 2724, loss : 73.12533569335938\n",
      "iteration : 2725, loss : 73.12306213378906\n",
      "iteration : 2726, loss : 73.12078094482422\n",
      "iteration : 2727, loss : 73.11849975585938\n",
      "iteration : 2728, loss : 73.11622619628906\n",
      "iteration : 2729, loss : 73.11395263671875\n",
      "iteration : 2730, loss : 73.1116714477539\n",
      "iteration : 2731, loss : 73.10939025878906\n",
      "iteration : 2732, loss : 73.10711669921875\n",
      "iteration : 2733, loss : 73.10484313964844\n",
      "iteration : 2734, loss : 73.10257720947266\n",
      "iteration : 2735, loss : 73.10028839111328\n",
      "iteration : 2736, loss : 73.09801483154297\n",
      "iteration : 2737, loss : 73.09574127197266\n",
      "iteration : 2738, loss : 73.09346771240234\n",
      "iteration : 2739, loss : 73.09119415283203\n",
      "iteration : 2740, loss : 73.08891296386719\n",
      "iteration : 2741, loss : 73.08663940429688\n",
      "iteration : 2742, loss : 73.08438110351562\n",
      "iteration : 2743, loss : 73.08209228515625\n",
      "iteration : 2744, loss : 73.079833984375\n",
      "iteration : 2745, loss : 73.07755279541016\n",
      "iteration : 2746, loss : 73.0752944946289\n",
      "iteration : 2747, loss : 73.07300567626953\n",
      "iteration : 2748, loss : 73.07073974609375\n",
      "iteration : 2749, loss : 73.0684585571289\n",
      "iteration : 2750, loss : 73.06620788574219\n",
      "iteration : 2751, loss : 73.06393432617188\n",
      "iteration : 2752, loss : 73.06166076660156\n",
      "iteration : 2753, loss : 73.05939483642578\n",
      "iteration : 2754, loss : 73.05712127685547\n",
      "iteration : 2755, loss : 73.05485534667969\n",
      "iteration : 2756, loss : 73.0525894165039\n",
      "iteration : 2757, loss : 73.05030822753906\n",
      "iteration : 2758, loss : 73.04804992675781\n",
      "iteration : 2759, loss : 73.0457763671875\n",
      "iteration : 2760, loss : 73.04351806640625\n",
      "iteration : 2761, loss : 73.04125213623047\n",
      "iteration : 2762, loss : 73.03898620605469\n",
      "iteration : 2763, loss : 73.03671264648438\n",
      "iteration : 2764, loss : 73.0344467163086\n",
      "iteration : 2765, loss : 73.03218078613281\n",
      "iteration : 2766, loss : 73.02992248535156\n",
      "iteration : 2767, loss : 73.02764892578125\n",
      "iteration : 2768, loss : 73.02538299560547\n",
      "iteration : 2769, loss : 73.02313232421875\n",
      "iteration : 2770, loss : 73.02085876464844\n",
      "iteration : 2771, loss : 73.01860046386719\n",
      "iteration : 2772, loss : 73.01632690429688\n",
      "iteration : 2773, loss : 73.01407623291016\n",
      "iteration : 2774, loss : 73.01181030273438\n",
      "iteration : 2775, loss : 73.0095443725586\n",
      "iteration : 2776, loss : 73.00728607177734\n",
      "iteration : 2777, loss : 73.00502014160156\n",
      "iteration : 2778, loss : 73.00275421142578\n",
      "iteration : 2779, loss : 73.00050354003906\n",
      "iteration : 2780, loss : 72.99823760986328\n",
      "iteration : 2781, loss : 72.9959716796875\n",
      "iteration : 2782, loss : 72.99371337890625\n",
      "iteration : 2783, loss : 72.991455078125\n",
      "iteration : 2784, loss : 72.98919677734375\n",
      "iteration : 2785, loss : 72.9869384765625\n",
      "iteration : 2786, loss : 72.98468017578125\n",
      "iteration : 2787, loss : 72.98241424560547\n",
      "iteration : 2788, loss : 72.98015594482422\n",
      "iteration : 2789, loss : 72.9779052734375\n",
      "iteration : 2790, loss : 72.97565460205078\n",
      "iteration : 2791, loss : 72.973388671875\n",
      "iteration : 2792, loss : 72.97113037109375\n",
      "iteration : 2793, loss : 72.96887969970703\n",
      "iteration : 2794, loss : 72.96662139892578\n",
      "iteration : 2795, loss : 72.96436309814453\n",
      "iteration : 2796, loss : 72.96211242675781\n",
      "iteration : 2797, loss : 72.95985412597656\n",
      "iteration : 2798, loss : 72.95759582519531\n",
      "iteration : 2799, loss : 72.95533752441406\n",
      "iteration : 2800, loss : 72.95308685302734\n",
      "iteration : 2801, loss : 72.9508285522461\n",
      "iteration : 2802, loss : 72.94857788085938\n",
      "iteration : 2803, loss : 72.94633483886719\n",
      "iteration : 2804, loss : 72.9440689086914\n",
      "iteration : 2805, loss : 72.94182586669922\n",
      "iteration : 2806, loss : 72.93956756591797\n",
      "iteration : 2807, loss : 72.93731689453125\n",
      "iteration : 2808, loss : 72.93505859375\n",
      "iteration : 2809, loss : 72.93281555175781\n",
      "iteration : 2810, loss : 72.93054962158203\n",
      "iteration : 2811, loss : 72.92830657958984\n",
      "iteration : 2812, loss : 72.92605590820312\n",
      "iteration : 2813, loss : 72.9238052368164\n",
      "iteration : 2814, loss : 72.92156219482422\n",
      "iteration : 2815, loss : 72.91930389404297\n",
      "iteration : 2816, loss : 72.91705322265625\n",
      "iteration : 2817, loss : 72.91480255126953\n",
      "iteration : 2818, loss : 72.91255950927734\n",
      "iteration : 2819, loss : 72.9103012084961\n",
      "iteration : 2820, loss : 72.9080581665039\n",
      "iteration : 2821, loss : 72.90580749511719\n",
      "iteration : 2822, loss : 72.903564453125\n",
      "iteration : 2823, loss : 72.90130615234375\n",
      "iteration : 2824, loss : 72.89906311035156\n",
      "iteration : 2825, loss : 72.89682006835938\n",
      "iteration : 2826, loss : 72.89457702636719\n",
      "iteration : 2827, loss : 72.89232635498047\n",
      "iteration : 2828, loss : 72.89007568359375\n",
      "iteration : 2829, loss : 72.88783264160156\n",
      "iteration : 2830, loss : 72.88558197021484\n",
      "iteration : 2831, loss : 72.88334655761719\n",
      "iteration : 2832, loss : 72.88109588623047\n",
      "iteration : 2833, loss : 72.87885284423828\n",
      "iteration : 2834, loss : 72.87660217285156\n",
      "iteration : 2835, loss : 72.8743667602539\n",
      "iteration : 2836, loss : 72.87211608886719\n",
      "iteration : 2837, loss : 72.86988067626953\n",
      "iteration : 2838, loss : 72.86763000488281\n",
      "iteration : 2839, loss : 72.86538696289062\n",
      "iteration : 2840, loss : 72.8631591796875\n",
      "iteration : 2841, loss : 72.86091613769531\n",
      "iteration : 2842, loss : 72.8586654663086\n",
      "iteration : 2843, loss : 72.85641479492188\n",
      "iteration : 2844, loss : 72.85418701171875\n",
      "iteration : 2845, loss : 72.85194396972656\n",
      "iteration : 2846, loss : 72.84970092773438\n",
      "iteration : 2847, loss : 72.84746551513672\n",
      "iteration : 2848, loss : 72.84521484375\n",
      "iteration : 2849, loss : 72.84297943115234\n",
      "iteration : 2850, loss : 72.84072875976562\n",
      "iteration : 2851, loss : 72.8385009765625\n",
      "iteration : 2852, loss : 72.83625793457031\n",
      "iteration : 2853, loss : 72.83402252197266\n",
      "iteration : 2854, loss : 72.831787109375\n",
      "iteration : 2855, loss : 72.82954406738281\n",
      "iteration : 2856, loss : 72.82731628417969\n",
      "iteration : 2857, loss : 72.82506561279297\n",
      "iteration : 2858, loss : 72.82284545898438\n",
      "iteration : 2859, loss : 72.82058715820312\n",
      "iteration : 2860, loss : 72.818359375\n",
      "iteration : 2861, loss : 72.81612396240234\n",
      "iteration : 2862, loss : 72.81388854980469\n",
      "iteration : 2863, loss : 72.81165313720703\n",
      "iteration : 2864, loss : 72.80941772460938\n",
      "iteration : 2865, loss : 72.80718231201172\n",
      "iteration : 2866, loss : 72.80494689941406\n",
      "iteration : 2867, loss : 72.80271911621094\n",
      "iteration : 2868, loss : 72.80048370361328\n",
      "iteration : 2869, loss : 72.79824829101562\n",
      "iteration : 2870, loss : 72.7960205078125\n",
      "iteration : 2871, loss : 72.79377746582031\n",
      "iteration : 2872, loss : 72.79154205322266\n",
      "iteration : 2873, loss : 72.789306640625\n",
      "iteration : 2874, loss : 72.78707122802734\n",
      "iteration : 2875, loss : 72.78485107421875\n",
      "iteration : 2876, loss : 72.7826156616211\n",
      "iteration : 2877, loss : 72.78038787841797\n",
      "iteration : 2878, loss : 72.77814483642578\n",
      "iteration : 2879, loss : 72.77591705322266\n",
      "iteration : 2880, loss : 72.77368927001953\n",
      "iteration : 2881, loss : 72.7714614868164\n",
      "iteration : 2882, loss : 72.76923370361328\n",
      "iteration : 2883, loss : 72.76699829101562\n",
      "iteration : 2884, loss : 72.7647705078125\n",
      "iteration : 2885, loss : 72.76254272460938\n",
      "iteration : 2886, loss : 72.76031494140625\n",
      "iteration : 2887, loss : 72.75807189941406\n",
      "iteration : 2888, loss : 72.75585174560547\n",
      "iteration : 2889, loss : 72.75363159179688\n",
      "iteration : 2890, loss : 72.75139617919922\n",
      "iteration : 2891, loss : 72.74917602539062\n",
      "iteration : 2892, loss : 72.7469482421875\n",
      "iteration : 2893, loss : 72.74472045898438\n",
      "iteration : 2894, loss : 72.74249267578125\n",
      "iteration : 2895, loss : 72.74026489257812\n",
      "iteration : 2896, loss : 72.73804473876953\n",
      "iteration : 2897, loss : 72.73580932617188\n",
      "iteration : 2898, loss : 72.73358917236328\n",
      "iteration : 2899, loss : 72.73136138916016\n",
      "iteration : 2900, loss : 72.72913360595703\n",
      "iteration : 2901, loss : 72.72691345214844\n",
      "iteration : 2902, loss : 72.72468566894531\n",
      "iteration : 2903, loss : 72.72247314453125\n",
      "iteration : 2904, loss : 72.72024536132812\n",
      "iteration : 2905, loss : 72.71802520751953\n",
      "iteration : 2906, loss : 72.7157974243164\n",
      "iteration : 2907, loss : 72.71357727050781\n",
      "iteration : 2908, loss : 72.71135711669922\n",
      "iteration : 2909, loss : 72.7091293334961\n",
      "iteration : 2910, loss : 72.7069091796875\n",
      "iteration : 2911, loss : 72.7046890258789\n",
      "iteration : 2912, loss : 72.70246124267578\n",
      "iteration : 2913, loss : 72.70024871826172\n",
      "iteration : 2914, loss : 72.69801330566406\n",
      "iteration : 2915, loss : 72.69580841064453\n",
      "iteration : 2916, loss : 72.6935806274414\n",
      "iteration : 2917, loss : 72.69136047363281\n",
      "iteration : 2918, loss : 72.68913269042969\n",
      "iteration : 2919, loss : 72.68692779541016\n",
      "iteration : 2920, loss : 72.68470764160156\n",
      "iteration : 2921, loss : 72.68247985839844\n",
      "iteration : 2922, loss : 72.6802749633789\n",
      "iteration : 2923, loss : 72.67804718017578\n",
      "iteration : 2924, loss : 72.67583465576172\n",
      "iteration : 2925, loss : 72.67361450195312\n",
      "iteration : 2926, loss : 72.67138671875\n",
      "iteration : 2927, loss : 72.66917419433594\n",
      "iteration : 2928, loss : 72.66695404052734\n",
      "iteration : 2929, loss : 72.66474151611328\n",
      "iteration : 2930, loss : 72.66253662109375\n",
      "iteration : 2931, loss : 72.66030883789062\n",
      "iteration : 2932, loss : 72.6581039428711\n",
      "iteration : 2933, loss : 72.65587615966797\n",
      "iteration : 2934, loss : 72.6536636352539\n",
      "iteration : 2935, loss : 72.65145111083984\n",
      "iteration : 2936, loss : 72.64923095703125\n",
      "iteration : 2937, loss : 72.64702606201172\n",
      "iteration : 2938, loss : 72.64480590820312\n",
      "iteration : 2939, loss : 72.64259338378906\n",
      "iteration : 2940, loss : 72.64038848876953\n",
      "iteration : 2941, loss : 72.63816833496094\n",
      "iteration : 2942, loss : 72.63594818115234\n",
      "iteration : 2943, loss : 72.63373565673828\n",
      "iteration : 2944, loss : 72.63153076171875\n",
      "iteration : 2945, loss : 72.62932586669922\n",
      "iteration : 2946, loss : 72.62710571289062\n",
      "iteration : 2947, loss : 72.62489318847656\n",
      "iteration : 2948, loss : 72.6226806640625\n",
      "iteration : 2949, loss : 72.62046813964844\n",
      "iteration : 2950, loss : 72.6182632446289\n",
      "iteration : 2951, loss : 72.61605072021484\n",
      "iteration : 2952, loss : 72.61384582519531\n",
      "iteration : 2953, loss : 72.61162567138672\n",
      "iteration : 2954, loss : 72.60942077636719\n",
      "iteration : 2955, loss : 72.60721588134766\n",
      "iteration : 2956, loss : 72.6050033569336\n",
      "iteration : 2957, loss : 72.6028060913086\n",
      "iteration : 2958, loss : 72.60057830810547\n",
      "iteration : 2959, loss : 72.59838104248047\n",
      "iteration : 2960, loss : 72.5961685180664\n",
      "iteration : 2961, loss : 72.59396362304688\n",
      "iteration : 2962, loss : 72.59175872802734\n",
      "iteration : 2963, loss : 72.58955383300781\n",
      "iteration : 2964, loss : 72.58734130859375\n",
      "iteration : 2965, loss : 72.58513641357422\n",
      "iteration : 2966, loss : 72.58292388916016\n",
      "iteration : 2967, loss : 72.58072662353516\n",
      "iteration : 2968, loss : 72.57852172851562\n",
      "iteration : 2969, loss : 72.5763168334961\n",
      "iteration : 2970, loss : 72.57411193847656\n",
      "iteration : 2971, loss : 72.57190704345703\n",
      "iteration : 2972, loss : 72.5697021484375\n",
      "iteration : 2973, loss : 72.56749725341797\n",
      "iteration : 2974, loss : 72.5652847290039\n",
      "iteration : 2975, loss : 72.56309509277344\n",
      "iteration : 2976, loss : 72.56089782714844\n",
      "iteration : 2977, loss : 72.55868530273438\n",
      "iteration : 2978, loss : 72.55648803710938\n",
      "iteration : 2979, loss : 72.55426788330078\n",
      "iteration : 2980, loss : 72.55207824707031\n",
      "iteration : 2981, loss : 72.54988098144531\n",
      "iteration : 2982, loss : 72.54766845703125\n",
      "iteration : 2983, loss : 72.54547882080078\n",
      "iteration : 2984, loss : 72.54327392578125\n",
      "iteration : 2985, loss : 72.54106903076172\n",
      "iteration : 2986, loss : 72.53886413574219\n",
      "iteration : 2987, loss : 72.53666687011719\n",
      "iteration : 2988, loss : 72.53446960449219\n",
      "iteration : 2989, loss : 72.53227233886719\n",
      "iteration : 2990, loss : 72.53007507324219\n",
      "iteration : 2991, loss : 72.52787017822266\n",
      "iteration : 2992, loss : 72.52567291259766\n",
      "iteration : 2993, loss : 72.52347564697266\n",
      "iteration : 2994, loss : 72.52128601074219\n",
      "iteration : 2995, loss : 72.51907348632812\n",
      "iteration : 2996, loss : 72.51688385009766\n",
      "iteration : 2997, loss : 72.51469421386719\n",
      "iteration : 2998, loss : 72.51248931884766\n",
      "iteration : 2999, loss : 72.51029205322266\n",
      "iteration : 3000, loss : 72.50808715820312\n",
      "iteration : 3001, loss : 72.50589752197266\n",
      "iteration : 3002, loss : 72.50370025634766\n",
      "iteration : 3003, loss : 72.50150299072266\n",
      "iteration : 3004, loss : 72.49931335449219\n",
      "iteration : 3005, loss : 72.49711608886719\n",
      "iteration : 3006, loss : 72.49491119384766\n",
      "iteration : 3007, loss : 72.49272918701172\n",
      "iteration : 3008, loss : 72.49052429199219\n",
      "iteration : 3009, loss : 72.48833465576172\n",
      "iteration : 3010, loss : 72.48614501953125\n",
      "iteration : 3011, loss : 72.48395538330078\n",
      "iteration : 3012, loss : 72.48175811767578\n",
      "iteration : 3013, loss : 72.47956848144531\n",
      "iteration : 3014, loss : 72.47737121582031\n",
      "iteration : 3015, loss : 72.47517395019531\n",
      "iteration : 3016, loss : 72.47299194335938\n",
      "iteration : 3017, loss : 72.47079467773438\n",
      "iteration : 3018, loss : 72.46859741210938\n",
      "iteration : 3019, loss : 72.4664077758789\n",
      "iteration : 3020, loss : 72.46421813964844\n",
      "iteration : 3021, loss : 72.46202087402344\n",
      "iteration : 3022, loss : 72.4598388671875\n",
      "iteration : 3023, loss : 72.45764923095703\n",
      "iteration : 3024, loss : 72.45545959472656\n",
      "iteration : 3025, loss : 72.45326232910156\n",
      "iteration : 3026, loss : 72.45108032226562\n",
      "iteration : 3027, loss : 72.44889831542969\n",
      "iteration : 3028, loss : 72.44670104980469\n",
      "iteration : 3029, loss : 72.44451141357422\n",
      "iteration : 3030, loss : 72.44232177734375\n",
      "iteration : 3031, loss : 72.44013214111328\n",
      "iteration : 3032, loss : 72.43795776367188\n",
      "iteration : 3033, loss : 72.4357681274414\n",
      "iteration : 3034, loss : 72.4335708618164\n",
      "iteration : 3035, loss : 72.431396484375\n",
      "iteration : 3036, loss : 72.42919158935547\n",
      "iteration : 3037, loss : 72.4270248413086\n",
      "iteration : 3038, loss : 72.42483520507812\n",
      "iteration : 3039, loss : 72.42263793945312\n",
      "iteration : 3040, loss : 72.42045593261719\n",
      "iteration : 3041, loss : 72.41826629638672\n",
      "iteration : 3042, loss : 72.41609191894531\n",
      "iteration : 3043, loss : 72.41390228271484\n",
      "iteration : 3044, loss : 72.41171264648438\n",
      "iteration : 3045, loss : 72.40953826904297\n",
      "iteration : 3046, loss : 72.4073486328125\n",
      "iteration : 3047, loss : 72.40515899658203\n",
      "iteration : 3048, loss : 72.40298461914062\n",
      "iteration : 3049, loss : 72.40080261230469\n",
      "iteration : 3050, loss : 72.39860534667969\n",
      "iteration : 3051, loss : 72.39642333984375\n",
      "iteration : 3052, loss : 72.39425659179688\n",
      "iteration : 3053, loss : 72.3920669555664\n",
      "iteration : 3054, loss : 72.389892578125\n",
      "iteration : 3055, loss : 72.38770294189453\n",
      "iteration : 3056, loss : 72.38552856445312\n",
      "iteration : 3057, loss : 72.38333892822266\n",
      "iteration : 3058, loss : 72.38115692138672\n",
      "iteration : 3059, loss : 72.37898254394531\n",
      "iteration : 3060, loss : 72.3768081665039\n",
      "iteration : 3061, loss : 72.37461853027344\n",
      "iteration : 3062, loss : 72.3724365234375\n",
      "iteration : 3063, loss : 72.3702621459961\n",
      "iteration : 3064, loss : 72.36807250976562\n",
      "iteration : 3065, loss : 72.36589813232422\n",
      "iteration : 3066, loss : 72.36372375488281\n",
      "iteration : 3067, loss : 72.3615493774414\n",
      "iteration : 3068, loss : 72.35935974121094\n",
      "iteration : 3069, loss : 72.35719299316406\n",
      "iteration : 3070, loss : 72.3550033569336\n",
      "iteration : 3071, loss : 72.35283660888672\n",
      "iteration : 3072, loss : 72.35065460205078\n",
      "iteration : 3073, loss : 72.34848022460938\n",
      "iteration : 3074, loss : 72.34629821777344\n",
      "iteration : 3075, loss : 72.34413146972656\n",
      "iteration : 3076, loss : 72.34194946289062\n",
      "iteration : 3077, loss : 72.33978271484375\n",
      "iteration : 3078, loss : 72.33760070800781\n",
      "iteration : 3079, loss : 72.3354263305664\n",
      "iteration : 3080, loss : 72.333251953125\n",
      "iteration : 3081, loss : 72.33108520507812\n",
      "iteration : 3082, loss : 72.32889556884766\n",
      "iteration : 3083, loss : 72.32672882080078\n",
      "iteration : 3084, loss : 72.3245620727539\n",
      "iteration : 3085, loss : 72.32238006591797\n",
      "iteration : 3086, loss : 72.3202133178711\n",
      "iteration : 3087, loss : 72.31804656982422\n",
      "iteration : 3088, loss : 72.31586456298828\n",
      "iteration : 3089, loss : 72.31369018554688\n",
      "iteration : 3090, loss : 72.31153106689453\n",
      "iteration : 3091, loss : 72.30934143066406\n",
      "iteration : 3092, loss : 72.30718231201172\n",
      "iteration : 3093, loss : 72.30500030517578\n",
      "iteration : 3094, loss : 72.3028335571289\n",
      "iteration : 3095, loss : 72.3006591796875\n",
      "iteration : 3096, loss : 72.29849243164062\n",
      "iteration : 3097, loss : 72.29631805419922\n",
      "iteration : 3098, loss : 72.29415130615234\n",
      "iteration : 3099, loss : 72.29197692871094\n",
      "iteration : 3100, loss : 72.28981018066406\n",
      "iteration : 3101, loss : 72.28764343261719\n",
      "iteration : 3102, loss : 72.28546905517578\n",
      "iteration : 3103, loss : 72.2833023071289\n",
      "iteration : 3104, loss : 72.28113555908203\n",
      "iteration : 3105, loss : 72.27896118164062\n",
      "iteration : 3106, loss : 72.27679443359375\n",
      "iteration : 3107, loss : 72.2746353149414\n",
      "iteration : 3108, loss : 72.27245330810547\n",
      "iteration : 3109, loss : 72.27029418945312\n",
      "iteration : 3110, loss : 72.26813507080078\n",
      "iteration : 3111, loss : 72.26596069335938\n",
      "iteration : 3112, loss : 72.2637939453125\n",
      "iteration : 3113, loss : 72.26163482666016\n",
      "iteration : 3114, loss : 72.25946807861328\n",
      "iteration : 3115, loss : 72.2573013305664\n",
      "iteration : 3116, loss : 72.255126953125\n",
      "iteration : 3117, loss : 72.25296783447266\n",
      "iteration : 3118, loss : 72.25080108642578\n",
      "iteration : 3119, loss : 72.24864196777344\n",
      "iteration : 3120, loss : 72.24647521972656\n",
      "iteration : 3121, loss : 72.24430847167969\n",
      "iteration : 3122, loss : 72.24214172363281\n",
      "iteration : 3123, loss : 72.239990234375\n",
      "iteration : 3124, loss : 72.2378158569336\n",
      "iteration : 3125, loss : 72.23566436767578\n",
      "iteration : 3126, loss : 72.23350524902344\n",
      "iteration : 3127, loss : 72.23133087158203\n",
      "iteration : 3128, loss : 72.22917175292969\n",
      "iteration : 3129, loss : 72.22701263427734\n",
      "iteration : 3130, loss : 72.22483825683594\n",
      "iteration : 3131, loss : 72.22268676757812\n",
      "iteration : 3132, loss : 72.22052001953125\n",
      "iteration : 3133, loss : 72.2183609008789\n",
      "iteration : 3134, loss : 72.2162094116211\n",
      "iteration : 3135, loss : 72.21404266357422\n",
      "iteration : 3136, loss : 72.21187591552734\n",
      "iteration : 3137, loss : 72.20973205566406\n",
      "iteration : 3138, loss : 72.20757293701172\n",
      "iteration : 3139, loss : 72.20539855957031\n",
      "iteration : 3140, loss : 72.2032470703125\n",
      "iteration : 3141, loss : 72.20108795166016\n",
      "iteration : 3142, loss : 72.19892883300781\n",
      "iteration : 3143, loss : 72.19676208496094\n",
      "iteration : 3144, loss : 72.19461059570312\n",
      "iteration : 3145, loss : 72.19245910644531\n",
      "iteration : 3146, loss : 72.19029998779297\n",
      "iteration : 3147, loss : 72.1881332397461\n",
      "iteration : 3148, loss : 72.18598175048828\n",
      "iteration : 3149, loss : 72.18382263183594\n",
      "iteration : 3150, loss : 72.1816635131836\n",
      "iteration : 3151, loss : 72.17951202392578\n",
      "iteration : 3152, loss : 72.17735290527344\n",
      "iteration : 3153, loss : 72.1751937866211\n",
      "iteration : 3154, loss : 72.17304229736328\n",
      "iteration : 3155, loss : 72.17088317871094\n",
      "iteration : 3156, loss : 72.1687240600586\n",
      "iteration : 3157, loss : 72.16658020019531\n",
      "iteration : 3158, loss : 72.1644287109375\n",
      "iteration : 3159, loss : 72.16227722167969\n",
      "iteration : 3160, loss : 72.16011810302734\n",
      "iteration : 3161, loss : 72.15796661376953\n",
      "iteration : 3162, loss : 72.15579986572266\n",
      "iteration : 3163, loss : 72.15365600585938\n",
      "iteration : 3164, loss : 72.15150451660156\n",
      "iteration : 3165, loss : 72.14935302734375\n",
      "iteration : 3166, loss : 72.14720153808594\n",
      "iteration : 3167, loss : 72.1450424194336\n",
      "iteration : 3168, loss : 72.14289093017578\n",
      "iteration : 3169, loss : 72.14073944091797\n",
      "iteration : 3170, loss : 72.13859558105469\n",
      "iteration : 3171, loss : 72.13643646240234\n",
      "iteration : 3172, loss : 72.13429260253906\n",
      "iteration : 3173, loss : 72.13214111328125\n",
      "iteration : 3174, loss : 72.12999725341797\n",
      "iteration : 3175, loss : 72.1278305053711\n",
      "iteration : 3176, loss : 72.12568664550781\n",
      "iteration : 3177, loss : 72.12354278564453\n",
      "iteration : 3178, loss : 72.12138366699219\n",
      "iteration : 3179, loss : 72.1192398071289\n",
      "iteration : 3180, loss : 72.11709594726562\n",
      "iteration : 3181, loss : 72.11494445800781\n",
      "iteration : 3182, loss : 72.11280059814453\n",
      "iteration : 3183, loss : 72.11064147949219\n",
      "iteration : 3184, loss : 72.1084976196289\n",
      "iteration : 3185, loss : 72.10635375976562\n",
      "iteration : 3186, loss : 72.10420989990234\n",
      "iteration : 3187, loss : 72.10205841064453\n",
      "iteration : 3188, loss : 72.09990692138672\n",
      "iteration : 3189, loss : 72.09776306152344\n",
      "iteration : 3190, loss : 72.09562683105469\n",
      "iteration : 3191, loss : 72.09347534179688\n",
      "iteration : 3192, loss : 72.09132385253906\n",
      "iteration : 3193, loss : 72.08917999267578\n",
      "iteration : 3194, loss : 72.08704376220703\n",
      "iteration : 3195, loss : 72.08489227294922\n",
      "iteration : 3196, loss : 72.0827407836914\n",
      "iteration : 3197, loss : 72.08059692382812\n",
      "iteration : 3198, loss : 72.07846069335938\n",
      "iteration : 3199, loss : 72.0763168334961\n",
      "iteration : 3200, loss : 72.07417297363281\n",
      "iteration : 3201, loss : 72.07202911376953\n",
      "iteration : 3202, loss : 72.06987762451172\n",
      "iteration : 3203, loss : 72.06774139404297\n",
      "iteration : 3204, loss : 72.06560516357422\n",
      "iteration : 3205, loss : 72.06346130371094\n",
      "iteration : 3206, loss : 72.06130981445312\n",
      "iteration : 3207, loss : 72.05917358398438\n",
      "iteration : 3208, loss : 72.0570297241211\n",
      "iteration : 3209, loss : 72.05489349365234\n",
      "iteration : 3210, loss : 72.05274963378906\n",
      "iteration : 3211, loss : 72.05060577392578\n",
      "iteration : 3212, loss : 72.0484619140625\n",
      "iteration : 3213, loss : 72.04631805419922\n",
      "iteration : 3214, loss : 72.04418182373047\n",
      "iteration : 3215, loss : 72.04205322265625\n",
      "iteration : 3216, loss : 72.03990173339844\n",
      "iteration : 3217, loss : 72.03776550292969\n",
      "iteration : 3218, loss : 72.03563690185547\n",
      "iteration : 3219, loss : 72.03347778320312\n",
      "iteration : 3220, loss : 72.0313491821289\n",
      "iteration : 3221, loss : 72.02921295166016\n",
      "iteration : 3222, loss : 72.0270767211914\n",
      "iteration : 3223, loss : 72.02494049072266\n",
      "iteration : 3224, loss : 72.02279663085938\n",
      "iteration : 3225, loss : 72.02067565917969\n",
      "iteration : 3226, loss : 72.01850891113281\n",
      "iteration : 3227, loss : 72.0163803100586\n",
      "iteration : 3228, loss : 72.01425170898438\n",
      "iteration : 3229, loss : 72.01212310791016\n",
      "iteration : 3230, loss : 72.00997924804688\n",
      "iteration : 3231, loss : 72.00784301757812\n",
      "iteration : 3232, loss : 72.0057144165039\n",
      "iteration : 3233, loss : 72.00357055664062\n",
      "iteration : 3234, loss : 72.00143432617188\n",
      "iteration : 3235, loss : 71.99929809570312\n",
      "iteration : 3236, loss : 71.99716186523438\n",
      "iteration : 3237, loss : 71.99503326416016\n",
      "iteration : 3238, loss : 71.9928970336914\n",
      "iteration : 3239, loss : 71.99076843261719\n",
      "iteration : 3240, loss : 71.98863983154297\n",
      "iteration : 3241, loss : 71.98650360107422\n",
      "iteration : 3242, loss : 71.98435974121094\n",
      "iteration : 3243, loss : 71.98223114013672\n",
      "iteration : 3244, loss : 71.98009490966797\n",
      "iteration : 3245, loss : 71.97796630859375\n",
      "iteration : 3246, loss : 71.97583770751953\n",
      "iteration : 3247, loss : 71.97370910644531\n",
      "iteration : 3248, loss : 71.97157287597656\n",
      "iteration : 3249, loss : 71.96944427490234\n",
      "iteration : 3250, loss : 71.96731567382812\n",
      "iteration : 3251, loss : 71.9651870727539\n",
      "iteration : 3252, loss : 71.96305084228516\n",
      "iteration : 3253, loss : 71.96090698242188\n",
      "iteration : 3254, loss : 71.95879364013672\n",
      "iteration : 3255, loss : 71.95665740966797\n",
      "iteration : 3256, loss : 71.95452880859375\n",
      "iteration : 3257, loss : 71.95240783691406\n",
      "iteration : 3258, loss : 71.95027160644531\n",
      "iteration : 3259, loss : 71.94813537597656\n",
      "iteration : 3260, loss : 71.94601440429688\n",
      "iteration : 3261, loss : 71.94388580322266\n",
      "iteration : 3262, loss : 71.9417724609375\n",
      "iteration : 3263, loss : 71.93962860107422\n",
      "iteration : 3264, loss : 71.9375\n",
      "iteration : 3265, loss : 71.93537902832031\n",
      "iteration : 3266, loss : 71.93324279785156\n",
      "iteration : 3267, loss : 71.93112182617188\n",
      "iteration : 3268, loss : 71.92900085449219\n",
      "iteration : 3269, loss : 71.92687225341797\n",
      "iteration : 3270, loss : 71.92473602294922\n",
      "iteration : 3271, loss : 71.92262268066406\n",
      "iteration : 3272, loss : 71.92049407958984\n",
      "iteration : 3273, loss : 71.91837310791016\n",
      "iteration : 3274, loss : 71.91624450683594\n",
      "iteration : 3275, loss : 71.91410827636719\n",
      "iteration : 3276, loss : 71.91199493408203\n",
      "iteration : 3277, loss : 71.90986633300781\n",
      "iteration : 3278, loss : 71.90774536132812\n",
      "iteration : 3279, loss : 71.9056167602539\n",
      "iteration : 3280, loss : 71.90350341796875\n",
      "iteration : 3281, loss : 71.90137481689453\n",
      "iteration : 3282, loss : 71.89925384521484\n",
      "iteration : 3283, loss : 71.89713287353516\n",
      "iteration : 3284, loss : 71.89501190185547\n",
      "iteration : 3285, loss : 71.89287567138672\n",
      "iteration : 3286, loss : 71.89076232910156\n",
      "iteration : 3287, loss : 71.88863372802734\n",
      "iteration : 3288, loss : 71.88652038574219\n",
      "iteration : 3289, loss : 71.8843994140625\n",
      "iteration : 3290, loss : 71.88227844238281\n",
      "iteration : 3291, loss : 71.88015747070312\n",
      "iteration : 3292, loss : 71.87803649902344\n",
      "iteration : 3293, loss : 71.87591552734375\n",
      "iteration : 3294, loss : 71.87379455566406\n",
      "iteration : 3295, loss : 71.8716812133789\n",
      "iteration : 3296, loss : 71.86956024169922\n",
      "iteration : 3297, loss : 71.867431640625\n",
      "iteration : 3298, loss : 71.86531829833984\n",
      "iteration : 3299, loss : 71.86319732666016\n",
      "iteration : 3300, loss : 71.861083984375\n",
      "iteration : 3301, loss : 71.85895538330078\n",
      "iteration : 3302, loss : 71.85684204101562\n",
      "iteration : 3303, loss : 71.85472106933594\n",
      "iteration : 3304, loss : 71.85260772705078\n",
      "iteration : 3305, loss : 71.8504867553711\n",
      "iteration : 3306, loss : 71.84837341308594\n",
      "iteration : 3307, loss : 71.84625244140625\n",
      "iteration : 3308, loss : 71.8441390991211\n",
      "iteration : 3309, loss : 71.8420181274414\n",
      "iteration : 3310, loss : 71.83990478515625\n",
      "iteration : 3311, loss : 71.8377914428711\n",
      "iteration : 3312, loss : 71.83566284179688\n",
      "iteration : 3313, loss : 71.83354949951172\n",
      "iteration : 3314, loss : 71.8314437866211\n",
      "iteration : 3315, loss : 71.82933044433594\n",
      "iteration : 3316, loss : 71.82721710205078\n",
      "iteration : 3317, loss : 71.82510375976562\n",
      "iteration : 3318, loss : 71.82298278808594\n",
      "iteration : 3319, loss : 71.82087707519531\n",
      "iteration : 3320, loss : 71.81875610351562\n",
      "iteration : 3321, loss : 71.81664276123047\n",
      "iteration : 3322, loss : 71.81452941894531\n",
      "iteration : 3323, loss : 71.81241607666016\n",
      "iteration : 3324, loss : 71.810302734375\n",
      "iteration : 3325, loss : 71.80818939208984\n",
      "iteration : 3326, loss : 71.80607604980469\n",
      "iteration : 3327, loss : 71.80396270751953\n",
      "iteration : 3328, loss : 71.8018569946289\n",
      "iteration : 3329, loss : 71.79973602294922\n",
      "iteration : 3330, loss : 71.79763793945312\n",
      "iteration : 3331, loss : 71.79551696777344\n",
      "iteration : 3332, loss : 71.79341125488281\n",
      "iteration : 3333, loss : 71.79129028320312\n",
      "iteration : 3334, loss : 71.78917694091797\n",
      "iteration : 3335, loss : 71.78707885742188\n",
      "iteration : 3336, loss : 71.78497314453125\n",
      "iteration : 3337, loss : 71.78285217285156\n",
      "iteration : 3338, loss : 71.78074645996094\n",
      "iteration : 3339, loss : 71.77864074707031\n",
      "iteration : 3340, loss : 71.77654266357422\n",
      "iteration : 3341, loss : 71.77442169189453\n",
      "iteration : 3342, loss : 71.77230834960938\n",
      "iteration : 3343, loss : 71.77020263671875\n",
      "iteration : 3344, loss : 71.76809692382812\n",
      "iteration : 3345, loss : 71.7659912109375\n",
      "iteration : 3346, loss : 71.76388549804688\n",
      "iteration : 3347, loss : 71.76178741455078\n",
      "iteration : 3348, loss : 71.75967407226562\n",
      "iteration : 3349, loss : 71.75756072998047\n",
      "iteration : 3350, loss : 71.75545501708984\n",
      "iteration : 3351, loss : 71.75334930419922\n",
      "iteration : 3352, loss : 71.75125122070312\n",
      "iteration : 3353, loss : 71.74913787841797\n",
      "iteration : 3354, loss : 71.74703216552734\n",
      "iteration : 3355, loss : 71.74492645263672\n",
      "iteration : 3356, loss : 71.7428207397461\n",
      "iteration : 3357, loss : 71.74072265625\n",
      "iteration : 3358, loss : 71.73861694335938\n",
      "iteration : 3359, loss : 71.73651123046875\n",
      "iteration : 3360, loss : 71.73440551757812\n",
      "iteration : 3361, loss : 71.73230743408203\n",
      "iteration : 3362, loss : 71.7302017211914\n",
      "iteration : 3363, loss : 71.72809600830078\n",
      "iteration : 3364, loss : 71.72599792480469\n",
      "iteration : 3365, loss : 71.7238998413086\n",
      "iteration : 3366, loss : 71.72178649902344\n",
      "iteration : 3367, loss : 71.71968078613281\n",
      "iteration : 3368, loss : 71.71757507324219\n",
      "iteration : 3369, loss : 71.71548461914062\n",
      "iteration : 3370, loss : 71.71338653564453\n",
      "iteration : 3371, loss : 71.7112808227539\n",
      "iteration : 3372, loss : 71.70917510986328\n",
      "iteration : 3373, loss : 71.70706939697266\n",
      "iteration : 3374, loss : 71.7049789428711\n",
      "iteration : 3375, loss : 71.702880859375\n",
      "iteration : 3376, loss : 71.70077514648438\n",
      "iteration : 3377, loss : 71.69866943359375\n",
      "iteration : 3378, loss : 71.69657135009766\n",
      "iteration : 3379, loss : 71.6944808959961\n",
      "iteration : 3380, loss : 71.69236755371094\n",
      "iteration : 3381, loss : 71.69027709960938\n",
      "iteration : 3382, loss : 71.68817901611328\n",
      "iteration : 3383, loss : 71.68608093261719\n",
      "iteration : 3384, loss : 71.68399047851562\n",
      "iteration : 3385, loss : 71.681884765625\n",
      "iteration : 3386, loss : 71.6797866821289\n",
      "iteration : 3387, loss : 71.67768859863281\n",
      "iteration : 3388, loss : 71.67559814453125\n",
      "iteration : 3389, loss : 71.6734848022461\n",
      "iteration : 3390, loss : 71.67140197753906\n",
      "iteration : 3391, loss : 71.66930389404297\n",
      "iteration : 3392, loss : 71.66720581054688\n",
      "iteration : 3393, loss : 71.66511535644531\n",
      "iteration : 3394, loss : 71.66300964355469\n",
      "iteration : 3395, loss : 71.6609115600586\n",
      "iteration : 3396, loss : 71.65882873535156\n",
      "iteration : 3397, loss : 71.65673065185547\n",
      "iteration : 3398, loss : 71.6546401977539\n",
      "iteration : 3399, loss : 71.65253448486328\n",
      "iteration : 3400, loss : 71.65043640136719\n",
      "iteration : 3401, loss : 71.64835357666016\n",
      "iteration : 3402, loss : 71.6462631225586\n",
      "iteration : 3403, loss : 71.64415740966797\n",
      "iteration : 3404, loss : 71.64205932617188\n",
      "iteration : 3405, loss : 71.63996887207031\n",
      "iteration : 3406, loss : 71.63788604736328\n",
      "iteration : 3407, loss : 71.63578796386719\n",
      "iteration : 3408, loss : 71.6336898803711\n",
      "iteration : 3409, loss : 71.63159942626953\n",
      "iteration : 3410, loss : 71.6295166015625\n",
      "iteration : 3411, loss : 71.6274185180664\n",
      "iteration : 3412, loss : 71.62532043457031\n",
      "iteration : 3413, loss : 71.62323760986328\n",
      "iteration : 3414, loss : 71.62114715576172\n",
      "iteration : 3415, loss : 71.61904907226562\n",
      "iteration : 3416, loss : 71.6169662475586\n",
      "iteration : 3417, loss : 71.6148681640625\n",
      "iteration : 3418, loss : 71.61277770996094\n",
      "iteration : 3419, loss : 71.6106948852539\n",
      "iteration : 3420, loss : 71.60860443115234\n",
      "iteration : 3421, loss : 71.60651397705078\n",
      "iteration : 3422, loss : 71.60441589355469\n",
      "iteration : 3423, loss : 71.60233306884766\n",
      "iteration : 3424, loss : 71.6002426147461\n",
      "iteration : 3425, loss : 71.59815216064453\n",
      "iteration : 3426, loss : 71.59607696533203\n",
      "iteration : 3427, loss : 71.5939712524414\n",
      "iteration : 3428, loss : 71.5918960571289\n",
      "iteration : 3429, loss : 71.58981323242188\n",
      "iteration : 3430, loss : 71.58772277832031\n",
      "iteration : 3431, loss : 71.58563232421875\n",
      "iteration : 3432, loss : 71.58354187011719\n",
      "iteration : 3433, loss : 71.58145904541016\n",
      "iteration : 3434, loss : 71.5793685913086\n",
      "iteration : 3435, loss : 71.5772933959961\n",
      "iteration : 3436, loss : 71.57520294189453\n",
      "iteration : 3437, loss : 71.57311248779297\n",
      "iteration : 3438, loss : 71.57102966308594\n",
      "iteration : 3439, loss : 71.56893920898438\n",
      "iteration : 3440, loss : 71.56685638427734\n",
      "iteration : 3441, loss : 71.56478881835938\n",
      "iteration : 3442, loss : 71.56269073486328\n",
      "iteration : 3443, loss : 71.56060791015625\n",
      "iteration : 3444, loss : 71.55853271484375\n",
      "iteration : 3445, loss : 71.55644226074219\n",
      "iteration : 3446, loss : 71.55435180664062\n",
      "iteration : 3447, loss : 71.55227661132812\n",
      "iteration : 3448, loss : 71.5501937866211\n",
      "iteration : 3449, loss : 71.54810333251953\n",
      "iteration : 3450, loss : 71.54602813720703\n",
      "iteration : 3451, loss : 71.54395294189453\n",
      "iteration : 3452, loss : 71.54186248779297\n",
      "iteration : 3453, loss : 71.53977966308594\n",
      "iteration : 3454, loss : 71.5376968383789\n",
      "iteration : 3455, loss : 71.5356216430664\n",
      "iteration : 3456, loss : 71.53353881835938\n",
      "iteration : 3457, loss : 71.53144836425781\n",
      "iteration : 3458, loss : 71.52937316894531\n",
      "iteration : 3459, loss : 71.52729034423828\n",
      "iteration : 3460, loss : 71.52521514892578\n",
      "iteration : 3461, loss : 71.52313995361328\n",
      "iteration : 3462, loss : 71.52104949951172\n",
      "iteration : 3463, loss : 71.51897430419922\n",
      "iteration : 3464, loss : 71.51689147949219\n",
      "iteration : 3465, loss : 71.51482391357422\n",
      "iteration : 3466, loss : 71.51274108886719\n",
      "iteration : 3467, loss : 71.51065063476562\n",
      "iteration : 3468, loss : 71.50858306884766\n",
      "iteration : 3469, loss : 71.50650024414062\n",
      "iteration : 3470, loss : 71.5044174194336\n",
      "iteration : 3471, loss : 71.50234985351562\n",
      "iteration : 3472, loss : 71.50025939941406\n",
      "iteration : 3473, loss : 71.4981918334961\n",
      "iteration : 3474, loss : 71.4961166381836\n",
      "iteration : 3475, loss : 71.49403381347656\n",
      "iteration : 3476, loss : 71.4919662475586\n",
      "iteration : 3477, loss : 71.48987579345703\n",
      "iteration : 3478, loss : 71.48780822753906\n",
      "iteration : 3479, loss : 71.48573303222656\n",
      "iteration : 3480, loss : 71.48365783691406\n",
      "iteration : 3481, loss : 71.48158264160156\n",
      "iteration : 3482, loss : 71.4795150756836\n",
      "iteration : 3483, loss : 71.47743225097656\n",
      "iteration : 3484, loss : 71.47535705566406\n",
      "iteration : 3485, loss : 71.47328186035156\n",
      "iteration : 3486, loss : 71.47120666503906\n",
      "iteration : 3487, loss : 71.4691390991211\n",
      "iteration : 3488, loss : 71.46705627441406\n",
      "iteration : 3489, loss : 71.46498107910156\n",
      "iteration : 3490, loss : 71.4629135131836\n",
      "iteration : 3491, loss : 71.4608383178711\n",
      "iteration : 3492, loss : 71.45877075195312\n",
      "iteration : 3493, loss : 71.4566879272461\n",
      "iteration : 3494, loss : 71.45462036132812\n",
      "iteration : 3495, loss : 71.45254516601562\n",
      "iteration : 3496, loss : 71.45047760009766\n",
      "iteration : 3497, loss : 71.44841003417969\n",
      "iteration : 3498, loss : 71.44633483886719\n",
      "iteration : 3499, loss : 71.44426727294922\n",
      "iteration : 3500, loss : 71.44219207763672\n",
      "iteration : 3501, loss : 71.44012451171875\n",
      "iteration : 3502, loss : 71.43805694580078\n",
      "iteration : 3503, loss : 71.43598175048828\n",
      "iteration : 3504, loss : 71.43390655517578\n",
      "iteration : 3505, loss : 71.43183898925781\n",
      "iteration : 3506, loss : 71.42977142333984\n",
      "iteration : 3507, loss : 71.42770385742188\n",
      "iteration : 3508, loss : 71.4256362915039\n",
      "iteration : 3509, loss : 71.42356872558594\n",
      "iteration : 3510, loss : 71.42149353027344\n",
      "iteration : 3511, loss : 71.41942596435547\n",
      "iteration : 3512, loss : 71.4173583984375\n",
      "iteration : 3513, loss : 71.4153060913086\n",
      "iteration : 3514, loss : 71.4132308959961\n",
      "iteration : 3515, loss : 71.41116333007812\n",
      "iteration : 3516, loss : 71.40908813476562\n",
      "iteration : 3517, loss : 71.40702056884766\n",
      "iteration : 3518, loss : 71.40496063232422\n",
      "iteration : 3519, loss : 71.40290069580078\n",
      "iteration : 3520, loss : 71.40082550048828\n",
      "iteration : 3521, loss : 71.39875793457031\n",
      "iteration : 3522, loss : 71.39668273925781\n",
      "iteration : 3523, loss : 71.3946304321289\n",
      "iteration : 3524, loss : 71.39256286621094\n",
      "iteration : 3525, loss : 71.39049530029297\n",
      "iteration : 3526, loss : 71.38843536376953\n",
      "iteration : 3527, loss : 71.38636016845703\n",
      "iteration : 3528, loss : 71.3843002319336\n",
      "iteration : 3529, loss : 71.38224029541016\n",
      "iteration : 3530, loss : 71.38016510009766\n",
      "iteration : 3531, loss : 71.37811279296875\n",
      "iteration : 3532, loss : 71.37605285644531\n",
      "iteration : 3533, loss : 71.37397766113281\n",
      "iteration : 3534, loss : 71.3719253540039\n",
      "iteration : 3535, loss : 71.3698501586914\n",
      "iteration : 3536, loss : 71.3677978515625\n",
      "iteration : 3537, loss : 71.36573028564453\n",
      "iteration : 3538, loss : 71.36367797851562\n",
      "iteration : 3539, loss : 71.36160278320312\n",
      "iteration : 3540, loss : 71.35955047607422\n",
      "iteration : 3541, loss : 71.35749053955078\n",
      "iteration : 3542, loss : 71.35543060302734\n",
      "iteration : 3543, loss : 71.35335540771484\n",
      "iteration : 3544, loss : 71.35131072998047\n",
      "iteration : 3545, loss : 71.34923553466797\n",
      "iteration : 3546, loss : 71.34717559814453\n",
      "iteration : 3547, loss : 71.34512329101562\n",
      "iteration : 3548, loss : 71.34306335449219\n",
      "iteration : 3549, loss : 71.34100341796875\n",
      "iteration : 3550, loss : 71.33894348144531\n",
      "iteration : 3551, loss : 71.33688354492188\n",
      "iteration : 3552, loss : 71.33482360839844\n",
      "iteration : 3553, loss : 71.332763671875\n",
      "iteration : 3554, loss : 71.3307113647461\n",
      "iteration : 3555, loss : 71.32865142822266\n",
      "iteration : 3556, loss : 71.32659149169922\n",
      "iteration : 3557, loss : 71.32453155517578\n",
      "iteration : 3558, loss : 71.32247161865234\n",
      "iteration : 3559, loss : 71.32042694091797\n",
      "iteration : 3560, loss : 71.318359375\n",
      "iteration : 3561, loss : 71.31629943847656\n",
      "iteration : 3562, loss : 71.31424713134766\n",
      "iteration : 3563, loss : 71.31219482421875\n",
      "iteration : 3564, loss : 71.31013488769531\n",
      "iteration : 3565, loss : 71.30807495117188\n",
      "iteration : 3566, loss : 71.30602264404297\n",
      "iteration : 3567, loss : 71.30396270751953\n",
      "iteration : 3568, loss : 71.30191040039062\n",
      "iteration : 3569, loss : 71.29986572265625\n",
      "iteration : 3570, loss : 71.29779815673828\n",
      "iteration : 3571, loss : 71.29574584960938\n",
      "iteration : 3572, loss : 71.29369354248047\n",
      "iteration : 3573, loss : 71.29164123535156\n",
      "iteration : 3574, loss : 71.28959655761719\n",
      "iteration : 3575, loss : 71.28753662109375\n",
      "iteration : 3576, loss : 71.28547668457031\n",
      "iteration : 3577, loss : 71.2834243774414\n",
      "iteration : 3578, loss : 71.2813720703125\n",
      "iteration : 3579, loss : 71.2793197631836\n",
      "iteration : 3580, loss : 71.27726745605469\n",
      "iteration : 3581, loss : 71.27522277832031\n",
      "iteration : 3582, loss : 71.2731704711914\n",
      "iteration : 3583, loss : 71.27111053466797\n",
      "iteration : 3584, loss : 71.26905822753906\n",
      "iteration : 3585, loss : 71.26701354980469\n",
      "iteration : 3586, loss : 71.26495361328125\n",
      "iteration : 3587, loss : 71.26290893554688\n",
      "iteration : 3588, loss : 71.2608642578125\n",
      "iteration : 3589, loss : 71.25880432128906\n",
      "iteration : 3590, loss : 71.25675201416016\n",
      "iteration : 3591, loss : 71.25470733642578\n",
      "iteration : 3592, loss : 71.25264739990234\n",
      "iteration : 3593, loss : 71.25060272216797\n",
      "iteration : 3594, loss : 71.24855041503906\n",
      "iteration : 3595, loss : 71.24650573730469\n",
      "iteration : 3596, loss : 71.24445343017578\n",
      "iteration : 3597, loss : 71.24240112304688\n",
      "iteration : 3598, loss : 71.2403564453125\n",
      "iteration : 3599, loss : 71.23831176757812\n",
      "iteration : 3600, loss : 71.23625946044922\n",
      "iteration : 3601, loss : 71.23421478271484\n",
      "iteration : 3602, loss : 71.232177734375\n",
      "iteration : 3603, loss : 71.2301254272461\n",
      "iteration : 3604, loss : 71.22807312011719\n",
      "iteration : 3605, loss : 71.22602844238281\n",
      "iteration : 3606, loss : 71.22398376464844\n",
      "iteration : 3607, loss : 71.22193145751953\n",
      "iteration : 3608, loss : 71.21988677978516\n",
      "iteration : 3609, loss : 71.21784973144531\n",
      "iteration : 3610, loss : 71.2157974243164\n",
      "iteration : 3611, loss : 71.21375274658203\n",
      "iteration : 3612, loss : 71.21171569824219\n",
      "iteration : 3613, loss : 71.20966339111328\n",
      "iteration : 3614, loss : 71.2076187133789\n",
      "iteration : 3615, loss : 71.20558166503906\n",
      "iteration : 3616, loss : 71.20352935791016\n",
      "iteration : 3617, loss : 71.20147705078125\n",
      "iteration : 3618, loss : 71.1994400024414\n",
      "iteration : 3619, loss : 71.19739532470703\n",
      "iteration : 3620, loss : 71.19534301757812\n",
      "iteration : 3621, loss : 71.19331359863281\n",
      "iteration : 3622, loss : 71.19126892089844\n",
      "iteration : 3623, loss : 71.1892318725586\n",
      "iteration : 3624, loss : 71.18718719482422\n",
      "iteration : 3625, loss : 71.18513488769531\n",
      "iteration : 3626, loss : 71.18309783935547\n",
      "iteration : 3627, loss : 71.1810531616211\n",
      "iteration : 3628, loss : 71.17900848388672\n",
      "iteration : 3629, loss : 71.1769790649414\n",
      "iteration : 3630, loss : 71.17493438720703\n",
      "iteration : 3631, loss : 71.17288970947266\n",
      "iteration : 3632, loss : 71.17084503173828\n",
      "iteration : 3633, loss : 71.1688003540039\n",
      "iteration : 3634, loss : 71.16676330566406\n",
      "iteration : 3635, loss : 71.16472625732422\n",
      "iteration : 3636, loss : 71.16268920898438\n",
      "iteration : 3637, loss : 71.16065216064453\n",
      "iteration : 3638, loss : 71.15860748291016\n",
      "iteration : 3639, loss : 71.15657043457031\n",
      "iteration : 3640, loss : 71.15452575683594\n",
      "iteration : 3641, loss : 71.15249633789062\n",
      "iteration : 3642, loss : 71.15045928955078\n",
      "iteration : 3643, loss : 71.1484146118164\n",
      "iteration : 3644, loss : 71.14637756347656\n",
      "iteration : 3645, loss : 71.14434051513672\n",
      "iteration : 3646, loss : 71.14230346679688\n",
      "iteration : 3647, loss : 71.14025115966797\n",
      "iteration : 3648, loss : 71.13822174072266\n",
      "iteration : 3649, loss : 71.13619232177734\n",
      "iteration : 3650, loss : 71.1341552734375\n",
      "iteration : 3651, loss : 71.13211059570312\n",
      "iteration : 3652, loss : 71.13008117675781\n",
      "iteration : 3653, loss : 71.1280517578125\n",
      "iteration : 3654, loss : 71.12600708007812\n",
      "iteration : 3655, loss : 71.12397003173828\n",
      "iteration : 3656, loss : 71.12193298339844\n",
      "iteration : 3657, loss : 71.1198959350586\n",
      "iteration : 3658, loss : 71.11786651611328\n",
      "iteration : 3659, loss : 71.11582946777344\n",
      "iteration : 3660, loss : 71.1137924194336\n",
      "iteration : 3661, loss : 71.11177062988281\n",
      "iteration : 3662, loss : 71.10973358154297\n",
      "iteration : 3663, loss : 71.1076889038086\n",
      "iteration : 3664, loss : 71.10565948486328\n",
      "iteration : 3665, loss : 71.10362243652344\n",
      "iteration : 3666, loss : 71.10159301757812\n",
      "iteration : 3667, loss : 71.09956359863281\n",
      "iteration : 3668, loss : 71.0975341796875\n",
      "iteration : 3669, loss : 71.09549713134766\n",
      "iteration : 3670, loss : 71.09346008300781\n",
      "iteration : 3671, loss : 71.09142303466797\n",
      "iteration : 3672, loss : 71.08939361572266\n",
      "iteration : 3673, loss : 71.08737182617188\n",
      "iteration : 3674, loss : 71.08533477783203\n",
      "iteration : 3675, loss : 71.08330535888672\n",
      "iteration : 3676, loss : 71.08128356933594\n",
      "iteration : 3677, loss : 71.0792465209961\n",
      "iteration : 3678, loss : 71.07720947265625\n",
      "iteration : 3679, loss : 71.07518768310547\n",
      "iteration : 3680, loss : 71.07315063476562\n",
      "iteration : 3681, loss : 71.07112121582031\n",
      "iteration : 3682, loss : 71.069091796875\n",
      "iteration : 3683, loss : 71.06706237792969\n",
      "iteration : 3684, loss : 71.06503295898438\n",
      "iteration : 3685, loss : 71.0630111694336\n",
      "iteration : 3686, loss : 71.06098175048828\n",
      "iteration : 3687, loss : 71.0589370727539\n",
      "iteration : 3688, loss : 71.05691528320312\n",
      "iteration : 3689, loss : 71.05489349365234\n",
      "iteration : 3690, loss : 71.0528564453125\n",
      "iteration : 3691, loss : 71.05084228515625\n",
      "iteration : 3692, loss : 71.04881286621094\n",
      "iteration : 3693, loss : 71.04678344726562\n",
      "iteration : 3694, loss : 71.04475402832031\n",
      "iteration : 3695, loss : 71.04273223876953\n",
      "iteration : 3696, loss : 71.04070281982422\n",
      "iteration : 3697, loss : 71.0386734008789\n",
      "iteration : 3698, loss : 71.03665161132812\n",
      "iteration : 3699, loss : 71.03462982177734\n",
      "iteration : 3700, loss : 71.03260803222656\n",
      "iteration : 3701, loss : 71.03057098388672\n",
      "iteration : 3702, loss : 71.02854919433594\n",
      "iteration : 3703, loss : 71.02652740478516\n",
      "iteration : 3704, loss : 71.02450561523438\n",
      "iteration : 3705, loss : 71.02247619628906\n",
      "iteration : 3706, loss : 71.02044677734375\n",
      "iteration : 3707, loss : 71.0184326171875\n",
      "iteration : 3708, loss : 71.01639556884766\n",
      "iteration : 3709, loss : 71.0143814086914\n",
      "iteration : 3710, loss : 71.0123519897461\n",
      "iteration : 3711, loss : 71.01033782958984\n",
      "iteration : 3712, loss : 71.00831604003906\n",
      "iteration : 3713, loss : 71.00628662109375\n",
      "iteration : 3714, loss : 71.00426483154297\n",
      "iteration : 3715, loss : 71.00223541259766\n",
      "iteration : 3716, loss : 71.0002212524414\n",
      "iteration : 3717, loss : 70.9981918334961\n",
      "iteration : 3718, loss : 70.99617004394531\n",
      "iteration : 3719, loss : 70.99414825439453\n",
      "iteration : 3720, loss : 70.99212646484375\n",
      "iteration : 3721, loss : 70.9901123046875\n",
      "iteration : 3722, loss : 70.98809051513672\n",
      "iteration : 3723, loss : 70.98607635498047\n",
      "iteration : 3724, loss : 70.98403930664062\n",
      "iteration : 3725, loss : 70.9820327758789\n",
      "iteration : 3726, loss : 70.98001098632812\n",
      "iteration : 3727, loss : 70.97799682617188\n",
      "iteration : 3728, loss : 70.97596740722656\n",
      "iteration : 3729, loss : 70.97395324707031\n",
      "iteration : 3730, loss : 70.97193908691406\n",
      "iteration : 3731, loss : 70.96990966796875\n",
      "iteration : 3732, loss : 70.9678955078125\n",
      "iteration : 3733, loss : 70.96588134765625\n",
      "iteration : 3734, loss : 70.96385192871094\n",
      "iteration : 3735, loss : 70.96183013916016\n",
      "iteration : 3736, loss : 70.95982360839844\n",
      "iteration : 3737, loss : 70.95780181884766\n",
      "iteration : 3738, loss : 70.9557876586914\n",
      "iteration : 3739, loss : 70.95376586914062\n",
      "iteration : 3740, loss : 70.95175170898438\n",
      "iteration : 3741, loss : 70.94973754882812\n",
      "iteration : 3742, loss : 70.9477310180664\n",
      "iteration : 3743, loss : 70.94570922851562\n",
      "iteration : 3744, loss : 70.94369506835938\n",
      "iteration : 3745, loss : 70.9416732788086\n",
      "iteration : 3746, loss : 70.93965911865234\n",
      "iteration : 3747, loss : 70.93763732910156\n",
      "iteration : 3748, loss : 70.93562316894531\n",
      "iteration : 3749, loss : 70.93360900878906\n",
      "iteration : 3750, loss : 70.93158721923828\n",
      "iteration : 3751, loss : 70.92958068847656\n",
      "iteration : 3752, loss : 70.92756652832031\n",
      "iteration : 3753, loss : 70.92555236816406\n",
      "iteration : 3754, loss : 70.92354583740234\n",
      "iteration : 3755, loss : 70.92151641845703\n",
      "iteration : 3756, loss : 70.91950988769531\n",
      "iteration : 3757, loss : 70.91748809814453\n",
      "iteration : 3758, loss : 70.91547393798828\n",
      "iteration : 3759, loss : 70.9134750366211\n",
      "iteration : 3760, loss : 70.91146087646484\n",
      "iteration : 3761, loss : 70.90945434570312\n",
      "iteration : 3762, loss : 70.90743255615234\n",
      "iteration : 3763, loss : 70.90542602539062\n",
      "iteration : 3764, loss : 70.90341186523438\n",
      "iteration : 3765, loss : 70.90140533447266\n",
      "iteration : 3766, loss : 70.89938354492188\n",
      "iteration : 3767, loss : 70.89737701416016\n",
      "iteration : 3768, loss : 70.8953628540039\n",
      "iteration : 3769, loss : 70.89335632324219\n",
      "iteration : 3770, loss : 70.89134216308594\n",
      "iteration : 3771, loss : 70.88933563232422\n",
      "iteration : 3772, loss : 70.8873291015625\n",
      "iteration : 3773, loss : 70.88531494140625\n",
      "iteration : 3774, loss : 70.88330841064453\n",
      "iteration : 3775, loss : 70.88130187988281\n",
      "iteration : 3776, loss : 70.8792953491211\n",
      "iteration : 3777, loss : 70.87728881835938\n",
      "iteration : 3778, loss : 70.8752670288086\n",
      "iteration : 3779, loss : 70.87326049804688\n",
      "iteration : 3780, loss : 70.87125396728516\n",
      "iteration : 3781, loss : 70.86924743652344\n",
      "iteration : 3782, loss : 70.86723327636719\n",
      "iteration : 3783, loss : 70.86522674560547\n",
      "iteration : 3784, loss : 70.86322021484375\n",
      "iteration : 3785, loss : 70.86121368408203\n",
      "iteration : 3786, loss : 70.85921478271484\n",
      "iteration : 3787, loss : 70.85721588134766\n",
      "iteration : 3788, loss : 70.85520935058594\n",
      "iteration : 3789, loss : 70.85320281982422\n",
      "iteration : 3790, loss : 70.85118865966797\n",
      "iteration : 3791, loss : 70.84918975830078\n",
      "iteration : 3792, loss : 70.84718322753906\n",
      "iteration : 3793, loss : 70.84517669677734\n",
      "iteration : 3794, loss : 70.84317016601562\n",
      "iteration : 3795, loss : 70.8411636352539\n",
      "iteration : 3796, loss : 70.83916473388672\n",
      "iteration : 3797, loss : 70.83716583251953\n",
      "iteration : 3798, loss : 70.83515167236328\n",
      "iteration : 3799, loss : 70.8331527709961\n",
      "iteration : 3800, loss : 70.83114624023438\n",
      "iteration : 3801, loss : 70.82913970947266\n",
      "iteration : 3802, loss : 70.8271255493164\n",
      "iteration : 3803, loss : 70.82512664794922\n",
      "iteration : 3804, loss : 70.8231201171875\n",
      "iteration : 3805, loss : 70.82112884521484\n",
      "iteration : 3806, loss : 70.81912231445312\n",
      "iteration : 3807, loss : 70.81712341308594\n",
      "iteration : 3808, loss : 70.81512451171875\n",
      "iteration : 3809, loss : 70.81311798095703\n",
      "iteration : 3810, loss : 70.81111145019531\n",
      "iteration : 3811, loss : 70.80912017822266\n",
      "iteration : 3812, loss : 70.80712127685547\n",
      "iteration : 3813, loss : 70.80511474609375\n",
      "iteration : 3814, loss : 70.80311584472656\n",
      "iteration : 3815, loss : 70.80110168457031\n",
      "iteration : 3816, loss : 70.79911041259766\n",
      "iteration : 3817, loss : 70.797119140625\n",
      "iteration : 3818, loss : 70.79511260986328\n",
      "iteration : 3819, loss : 70.7931137084961\n",
      "iteration : 3820, loss : 70.7911148071289\n",
      "iteration : 3821, loss : 70.78910827636719\n",
      "iteration : 3822, loss : 70.78711700439453\n",
      "iteration : 3823, loss : 70.78511810302734\n",
      "iteration : 3824, loss : 70.78311157226562\n",
      "iteration : 3825, loss : 70.78111267089844\n",
      "iteration : 3826, loss : 70.77912139892578\n",
      "iteration : 3827, loss : 70.77711486816406\n",
      "iteration : 3828, loss : 70.77513122558594\n",
      "iteration : 3829, loss : 70.77312469482422\n",
      "iteration : 3830, loss : 70.77112579345703\n",
      "iteration : 3831, loss : 70.76912689208984\n",
      "iteration : 3832, loss : 70.76712799072266\n",
      "iteration : 3833, loss : 70.76512908935547\n",
      "iteration : 3834, loss : 70.76313018798828\n",
      "iteration : 3835, loss : 70.76114654541016\n",
      "iteration : 3836, loss : 70.7591552734375\n",
      "iteration : 3837, loss : 70.75716400146484\n",
      "iteration : 3838, loss : 70.75515747070312\n",
      "iteration : 3839, loss : 70.75316619873047\n",
      "iteration : 3840, loss : 70.75117492675781\n",
      "iteration : 3841, loss : 70.74917602539062\n",
      "iteration : 3842, loss : 70.74717712402344\n",
      "iteration : 3843, loss : 70.74518585205078\n",
      "iteration : 3844, loss : 70.74319458007812\n",
      "iteration : 3845, loss : 70.74119567871094\n",
      "iteration : 3846, loss : 70.73919677734375\n",
      "iteration : 3847, loss : 70.7372055053711\n",
      "iteration : 3848, loss : 70.7352066040039\n",
      "iteration : 3849, loss : 70.73321533203125\n",
      "iteration : 3850, loss : 70.73123168945312\n",
      "iteration : 3851, loss : 70.72924041748047\n",
      "iteration : 3852, loss : 70.72724151611328\n",
      "iteration : 3853, loss : 70.72525787353516\n",
      "iteration : 3854, loss : 70.72325897216797\n",
      "iteration : 3855, loss : 70.72126770019531\n",
      "iteration : 3856, loss : 70.71927642822266\n",
      "iteration : 3857, loss : 70.71729278564453\n",
      "iteration : 3858, loss : 70.71530151367188\n",
      "iteration : 3859, loss : 70.71330261230469\n",
      "iteration : 3860, loss : 70.71131134033203\n",
      "iteration : 3861, loss : 70.70932006835938\n",
      "iteration : 3862, loss : 70.70732879638672\n",
      "iteration : 3863, loss : 70.70533752441406\n",
      "iteration : 3864, loss : 70.7033462524414\n",
      "iteration : 3865, loss : 70.70135498046875\n",
      "iteration : 3866, loss : 70.6993637084961\n",
      "iteration : 3867, loss : 70.69738006591797\n",
      "iteration : 3868, loss : 70.69539642333984\n",
      "iteration : 3869, loss : 70.69340515136719\n",
      "iteration : 3870, loss : 70.69141387939453\n",
      "iteration : 3871, loss : 70.68942260742188\n",
      "iteration : 3872, loss : 70.68744659423828\n",
      "iteration : 3873, loss : 70.6854476928711\n",
      "iteration : 3874, loss : 70.68345642089844\n",
      "iteration : 3875, loss : 70.68147277832031\n",
      "iteration : 3876, loss : 70.67948150634766\n",
      "iteration : 3877, loss : 70.67750549316406\n",
      "iteration : 3878, loss : 70.67552185058594\n",
      "iteration : 3879, loss : 70.67353057861328\n",
      "iteration : 3880, loss : 70.6715316772461\n",
      "iteration : 3881, loss : 70.66954040527344\n",
      "iteration : 3882, loss : 70.66757202148438\n",
      "iteration : 3883, loss : 70.66557312011719\n",
      "iteration : 3884, loss : 70.6635971069336\n",
      "iteration : 3885, loss : 70.66160583496094\n",
      "iteration : 3886, loss : 70.65962219238281\n",
      "iteration : 3887, loss : 70.65763854980469\n",
      "iteration : 3888, loss : 70.65565490722656\n",
      "iteration : 3889, loss : 70.6536636352539\n",
      "iteration : 3890, loss : 70.65167236328125\n",
      "iteration : 3891, loss : 70.64970397949219\n",
      "iteration : 3892, loss : 70.64772033691406\n",
      "iteration : 3893, loss : 70.64573669433594\n",
      "iteration : 3894, loss : 70.64374542236328\n",
      "iteration : 3895, loss : 70.64176940917969\n",
      "iteration : 3896, loss : 70.63977813720703\n",
      "iteration : 3897, loss : 70.63780212402344\n",
      "iteration : 3898, loss : 70.63581085205078\n",
      "iteration : 3899, loss : 70.63383483886719\n",
      "iteration : 3900, loss : 70.6318588256836\n",
      "iteration : 3901, loss : 70.6298599243164\n",
      "iteration : 3902, loss : 70.62788391113281\n",
      "iteration : 3903, loss : 70.62590789794922\n",
      "iteration : 3904, loss : 70.6239242553711\n",
      "iteration : 3905, loss : 70.62193298339844\n",
      "iteration : 3906, loss : 70.61995697021484\n",
      "iteration : 3907, loss : 70.61798095703125\n",
      "iteration : 3908, loss : 70.61599731445312\n",
      "iteration : 3909, loss : 70.614013671875\n",
      "iteration : 3910, loss : 70.61203002929688\n",
      "iteration : 3911, loss : 70.61005401611328\n",
      "iteration : 3912, loss : 70.60807800292969\n",
      "iteration : 3913, loss : 70.6061019897461\n",
      "iteration : 3914, loss : 70.60411834716797\n",
      "iteration : 3915, loss : 70.60214233398438\n",
      "iteration : 3916, loss : 70.60015869140625\n",
      "iteration : 3917, loss : 70.59818267822266\n",
      "iteration : 3918, loss : 70.59620666503906\n",
      "iteration : 3919, loss : 70.59423065185547\n",
      "iteration : 3920, loss : 70.59223937988281\n",
      "iteration : 3921, loss : 70.59027099609375\n",
      "iteration : 3922, loss : 70.58828735351562\n",
      "iteration : 3923, loss : 70.58631134033203\n",
      "iteration : 3924, loss : 70.58433532714844\n",
      "iteration : 3925, loss : 70.58236694335938\n",
      "iteration : 3926, loss : 70.58038330078125\n",
      "iteration : 3927, loss : 70.57839965820312\n",
      "iteration : 3928, loss : 70.5764389038086\n",
      "iteration : 3929, loss : 70.57444763183594\n",
      "iteration : 3930, loss : 70.57247161865234\n",
      "iteration : 3931, loss : 70.57049560546875\n",
      "iteration : 3932, loss : 70.56852722167969\n",
      "iteration : 3933, loss : 70.56654357910156\n",
      "iteration : 3934, loss : 70.56456756591797\n",
      "iteration : 3935, loss : 70.5625991821289\n",
      "iteration : 3936, loss : 70.56061553955078\n",
      "iteration : 3937, loss : 70.55864715576172\n",
      "iteration : 3938, loss : 70.55667877197266\n",
      "iteration : 3939, loss : 70.55470275878906\n",
      "iteration : 3940, loss : 70.55272674560547\n",
      "iteration : 3941, loss : 70.5507583618164\n",
      "iteration : 3942, loss : 70.54877471923828\n",
      "iteration : 3943, loss : 70.54680633544922\n",
      "iteration : 3944, loss : 70.5448226928711\n",
      "iteration : 3945, loss : 70.54286193847656\n",
      "iteration : 3946, loss : 70.54088592529297\n",
      "iteration : 3947, loss : 70.5389175415039\n",
      "iteration : 3948, loss : 70.53694915771484\n",
      "iteration : 3949, loss : 70.53497314453125\n",
      "iteration : 3950, loss : 70.53299713134766\n",
      "iteration : 3951, loss : 70.53103637695312\n",
      "iteration : 3952, loss : 70.52906036376953\n",
      "iteration : 3953, loss : 70.52709197998047\n",
      "iteration : 3954, loss : 70.5251235961914\n",
      "iteration : 3955, loss : 70.52314758300781\n",
      "iteration : 3956, loss : 70.52117919921875\n",
      "iteration : 3957, loss : 70.51919555664062\n",
      "iteration : 3958, loss : 70.51722717285156\n",
      "iteration : 3959, loss : 70.5152587890625\n",
      "iteration : 3960, loss : 70.51329040527344\n",
      "iteration : 3961, loss : 70.51131439208984\n",
      "iteration : 3962, loss : 70.50935363769531\n",
      "iteration : 3963, loss : 70.50738525390625\n",
      "iteration : 3964, loss : 70.50541687011719\n",
      "iteration : 3965, loss : 70.5034408569336\n",
      "iteration : 3966, loss : 70.50148010253906\n",
      "iteration : 3967, loss : 70.49950408935547\n",
      "iteration : 3968, loss : 70.4975357055664\n",
      "iteration : 3969, loss : 70.49556732177734\n",
      "iteration : 3970, loss : 70.49360656738281\n",
      "iteration : 3971, loss : 70.49163055419922\n",
      "iteration : 3972, loss : 70.48966979980469\n",
      "iteration : 3973, loss : 70.48770141601562\n",
      "iteration : 3974, loss : 70.48573303222656\n",
      "iteration : 3975, loss : 70.48377227783203\n",
      "iteration : 3976, loss : 70.48179626464844\n",
      "iteration : 3977, loss : 70.4798355102539\n",
      "iteration : 3978, loss : 70.47787475585938\n",
      "iteration : 3979, loss : 70.47591400146484\n",
      "iteration : 3980, loss : 70.47394561767578\n",
      "iteration : 3981, loss : 70.47197723388672\n",
      "iteration : 3982, loss : 70.47000885009766\n",
      "iteration : 3983, loss : 70.46804809570312\n",
      "iteration : 3984, loss : 70.46607971191406\n",
      "iteration : 3985, loss : 70.46411895751953\n",
      "iteration : 3986, loss : 70.46215057373047\n",
      "iteration : 3987, loss : 70.4601821899414\n",
      "iteration : 3988, loss : 70.45822143554688\n",
      "iteration : 3989, loss : 70.45625305175781\n",
      "iteration : 3990, loss : 70.45429992675781\n",
      "iteration : 3991, loss : 70.45233154296875\n",
      "iteration : 3992, loss : 70.45036315917969\n",
      "iteration : 3993, loss : 70.44841003417969\n",
      "iteration : 3994, loss : 70.44644165039062\n",
      "iteration : 3995, loss : 70.44448852539062\n",
      "iteration : 3996, loss : 70.44252014160156\n",
      "iteration : 3997, loss : 70.44055938720703\n",
      "iteration : 3998, loss : 70.4385986328125\n",
      "iteration : 3999, loss : 70.43663787841797\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=4000\n",
    "## Création d'une couche linéaire de dimension Xdim->1\n",
    "net = torch.nn.Linear(Xdim, 1) \n",
    "## Passe forward du module :  équivalent à net.forward(x)[:10]\n",
    "print(\"Sortie du réseau\", net(boston_x)[:10])\n",
    "## affiche la liste des paramètres du modèle\n",
    "print(\"Paramètres et noms des paramètres\", list(zip(list(net.parameters()), list(net.named_parameters()))))\n",
    "\n",
    "## Création d'une fonction de loss aux moindres carrés\n",
    "mseloss = torch.nn.MSELoss()\n",
    "## on créé un optimiseur pour le réseau (paramètres w et b), avec un pas de gradient lr\n",
    "optim = torch.optim.SGD(params=net.parameters(),lr=EPS) \n",
    "# Juste pour info, ce n'est pas utile, les paramètres sont déjà initialisés.\n",
    "net.reset_parameters()\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    loss = mseloss(net(boston_x).view(-1,1),boston_y.view(-1,1))\n",
    "    print(f\"iteration : {i}, loss : {loss}\")\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxdRGqZtSlVt"
   },
   "source": [
    "## C.1. Création d'un réseau de neurones\n",
    "\n",
    "Avec ces briques élémentaires, il est très facile de définir un réseau de neurones standard :\n",
    "* soit en utilisant le conteneur **torch.nn.Sequential** qui permet d'enchaîner séquentiellement plusieurs modules\n",
    "* soit en définissant à la main un nouveau module.\n",
    "\n",
    "Ci-dessous un exemple  pour créer un réseau à deux couches linéaires avec une fonction d'activation tanh des deux manières différentes. Vous remarquez qu'il n'y a pas besoin de définir la méthode **backward**, celle-ci est héritée du conteneur abstrait et ne fait qu'appeler séquentiellement en ordre inverse les méthodes **backward** des différents modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "7c6I-PZRD-aN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0, loss : 607.9660034179688\n",
      "iteration : 1, loss : 491.9415588378906\n",
      "iteration : 2, loss : 400.0263671875\n",
      "iteration : 3, loss : 328.8254089355469\n",
      "iteration : 4, loss : 273.68743896484375\n",
      "iteration : 5, loss : 230.9886016845703\n",
      "iteration : 6, loss : 197.92263793945312\n",
      "iteration : 7, loss : 172.31634521484375\n",
      "iteration : 8, loss : 152.48684692382812\n",
      "iteration : 9, loss : 137.13084411621094\n",
      "iteration : 10, loss : 125.23919677734375\n",
      "iteration : 11, loss : 116.03026580810547\n",
      "iteration : 12, loss : 108.8989028930664\n",
      "iteration : 13, loss : 103.3763656616211\n",
      "iteration : 14, loss : 99.09970092773438\n",
      "iteration : 15, loss : 95.78787231445312\n",
      "iteration : 16, loss : 93.22318267822266\n",
      "iteration : 17, loss : 91.2370834350586\n",
      "iteration : 18, loss : 89.69905090332031\n",
      "iteration : 19, loss : 88.50798797607422\n",
      "iteration : 20, loss : 87.58563995361328\n",
      "iteration : 21, loss : 86.8713607788086\n",
      "iteration : 22, loss : 86.31824493408203\n",
      "iteration : 23, loss : 85.88990783691406\n",
      "iteration : 24, loss : 85.55819702148438\n",
      "iteration : 25, loss : 85.30132293701172\n",
      "iteration : 26, loss : 85.10238647460938\n",
      "iteration : 27, loss : 84.94833374023438\n",
      "iteration : 28, loss : 84.82904815673828\n",
      "iteration : 29, loss : 84.73666381835938\n",
      "iteration : 30, loss : 84.66512298583984\n",
      "iteration : 31, loss : 84.60972595214844\n",
      "iteration : 32, loss : 84.56681823730469\n",
      "iteration : 33, loss : 84.53360748291016\n",
      "iteration : 34, loss : 84.50786590576172\n",
      "iteration : 35, loss : 84.48796081542969\n",
      "iteration : 36, loss : 84.4725112915039\n",
      "iteration : 37, loss : 84.46056365966797\n",
      "iteration : 38, loss : 84.4513168334961\n",
      "iteration : 39, loss : 84.44415283203125\n",
      "iteration : 40, loss : 84.43860626220703\n",
      "iteration : 41, loss : 84.4343032836914\n",
      "iteration : 42, loss : 84.43097686767578\n",
      "iteration : 43, loss : 84.42840576171875\n",
      "iteration : 44, loss : 84.42640686035156\n",
      "iteration : 45, loss : 84.42485809326172\n",
      "iteration : 46, loss : 84.42366790771484\n",
      "iteration : 47, loss : 84.4227294921875\n",
      "iteration : 48, loss : 84.42202758789062\n",
      "iteration : 49, loss : 84.42145538330078\n"
     ]
    }
   ],
   "source": [
    "EPS = 1e-2\n",
    "EPOCHS=50\n",
    "\n",
    "#Réseau à la main (on le refera à la main derriere)\n",
    "class DeuxCouches(torch.nn.Module):\n",
    "  \n",
    "  def __init__(self):\n",
    "    super(DeuxCouches,self).__init__()\n",
    "    self.un = torch.nn.Linear(Xdim,5)\n",
    "    self.act = torch.nn.Tanh()\n",
    "    self.deux = torch.nn.Linear(5,1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.deux(self.act(self.un(x)))\n",
    "\n",
    "netDeuxCouches = DeuxCouches()\n",
    "\n",
    "mseloss = torch.nn.MSELoss()\n",
    "    \n",
    "optim = torch.optim.SGD(params=netDeuxCouches.parameters(),lr=EPS)\n",
    "for i in range(EPOCHS):\n",
    "    loss = mseloss(netDeuxCouches(boston_x),boston_y.view(-1,1))\n",
    "    print(f\"iteration : {i}, loss : {loss}\")\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0, loss : 570.4990234375\n",
      "iteration : 1, loss : 465.8817443847656\n",
      "iteration : 2, loss : 379.8238525390625\n",
      "iteration : 3, loss : 313.1806335449219\n",
      "iteration : 4, loss : 261.5721435546875\n",
      "iteration : 5, loss : 221.6064910888672\n",
      "iteration : 6, loss : 190.6571044921875\n",
      "iteration : 7, loss : 166.68995666503906\n",
      "iteration : 8, loss : 148.1297607421875\n",
      "iteration : 9, loss : 133.7567138671875\n",
      "iteration : 10, loss : 122.62628173828125\n",
      "iteration : 11, loss : 114.00684356689453\n",
      "iteration : 12, loss : 107.33195495605469\n",
      "iteration : 13, loss : 102.16291809082031\n",
      "iteration : 14, loss : 98.1600112915039\n",
      "iteration : 15, loss : 95.06016540527344\n",
      "iteration : 16, loss : 92.65962982177734\n",
      "iteration : 17, loss : 90.80069732666016\n",
      "iteration : 18, loss : 89.36109161376953\n",
      "iteration : 19, loss : 88.24628448486328\n",
      "iteration : 20, loss : 87.38298034667969\n",
      "iteration : 21, loss : 86.71442413330078\n",
      "iteration : 22, loss : 86.19670867919922\n",
      "iteration : 23, loss : 85.79578399658203\n",
      "iteration : 24, loss : 85.48530578613281\n",
      "iteration : 25, loss : 85.244873046875\n",
      "iteration : 26, loss : 85.05867767333984\n",
      "iteration : 27, loss : 84.91449737548828\n",
      "iteration : 28, loss : 84.8028335571289\n",
      "iteration : 29, loss : 84.71636962890625\n",
      "iteration : 30, loss : 84.64939880371094\n",
      "iteration : 31, loss : 84.5975570678711\n",
      "iteration : 32, loss : 84.5573959350586\n",
      "iteration : 33, loss : 84.52629089355469\n",
      "iteration : 34, loss : 84.50221252441406\n",
      "iteration : 35, loss : 84.48357391357422\n",
      "iteration : 36, loss : 84.4691390991211\n",
      "iteration : 37, loss : 84.45794677734375\n",
      "iteration : 38, loss : 84.44927215576172\n",
      "iteration : 39, loss : 84.44258117675781\n",
      "iteration : 40, loss : 84.43738555908203\n",
      "iteration : 41, loss : 84.43336486816406\n",
      "iteration : 42, loss : 84.43024444580078\n",
      "iteration : 43, loss : 84.42784118652344\n",
      "iteration : 44, loss : 84.42596435546875\n",
      "iteration : 45, loss : 84.42452239990234\n",
      "iteration : 46, loss : 84.42340087890625\n",
      "iteration : 47, loss : 84.42253875732422\n",
      "iteration : 48, loss : 84.42185974121094\n",
      "iteration : 49, loss : 84.42134094238281\n"
     ]
    }
   ],
   "source": [
    "#Création d'un réseau à 1 couche cachée avec le module séquentiel (remplace l'objet précédent)\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "\n",
    "mseloss = torch.nn.MSELoss()\n",
    "    \n",
    "optim = torch.optim.SGD(params=netSeq.parameters(),lr=EPS) # extraction auto des paramètres :)\n",
    "for i in range(EPOCHS):\n",
    "    loss = mseloss(netSeq(boston_x),boston_y.view(-1,1))\n",
    "    print(f\"iteration : {i}, loss : {loss}\")\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGIDLqItECDu"
   },
   "source": [
    "##  C.2. Méthodologie expérimentale et boîte à outils\n",
    "Pytorch dispose d'un ensemble d'outils qui permettent de simplifier les démarches expérimentales. Nous allons voir en particulier : \n",
    "* le DataLoader qui permet de gérer le chargement de données, le partitionement et la constitution d'ensembles de test et d'apprentissage; \n",
    "* le checkpointing qui permet de sauvegarder/charger les modèles en cours d'entraînement.\n",
    "* le TensorBoard (qui vient de tensorflow) qui permet de suivre l'évolution en apprentissage de vos modèles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJ4MoJP4k4i0"
   },
   "source": [
    "### C.2.1 DataLoader\n",
    "Le <a href=https://pytorch.org/docs/stable/data.html>**DataLoader**</a> et la classe associée <a href=https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset> **Dataset**</a>  permettent en particulier de :\n",
    "* charger des données\n",
    "* pré-processer les données\n",
    "* de gérer les mini-batchs (sous-ensembles sur lequel on effectue une descente de gradient).\n",
    "\n",
    "La classe **Dataset** est une classe abstraite qui nécessite l'implémentation que d'une seule méthode, ```__getitem__(self,index)``` : elle renvoie le i-ème objet du jeu de données (généralement un couple *(exemple,label)*. \n",
    "\n",
    "La classe **TensorDataset** est l'instanciation la plus courante d'un **Dataset**, elle permet de créer un objet **Dataset** à partir d'une liste de tenseurs qui renvoie pour un index $i$ donné le tuple contenant les $i$-èmes ligne de chaque tenseur.\n",
    "\n",
    "La classe **DataLoader** permet essentiellement de randomiser et de constituer des mini-batchs de façon simple à partir d'une instance de **Dataset**. Chaque mini-batch est constitué d'exemples tirés aléatoirement dans le **Dataset** passé en paramètre et mis bout à bout dans des tenseurs. La méthode ```collate_fn(*args)``` est utilisée pour cela (nous verrons une customization de cette fonction dans une séance ultérieure). C'est ce générateur qui est généralement parcouru lors de l'apprentissage à chaque itération d'optimisation.\n",
    "\n",
    "Voici un exemple de code pour utiliser le DataLoader : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "AZaWAFO8k8ze"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET:\n",
      " 506 (tensor([2.9850e-02, 0.0000e+00, 2.1800e+00, 0.0000e+00, 4.5800e-01, 6.4300e+00,\n",
      "        5.8700e+01, 6.0622e+00, 3.0000e+00, 2.2200e+02, 1.8700e+01, 3.9412e+02,\n",
      "        5.2100e+00]), tensor(28.7000))\n",
      "DATA LOADER:\n",
      " 32 \n",
      " torch.Size([16, 13])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "## Création d'un dataset à partir des deux tenseurs d'exemples et de labels\n",
    "train_data = TensorDataset(boston_x,boston_y)\n",
    "## On peut indexer et connaitre la longueur d'un dataset\n",
    "print(\"DATASET:\\n\",len(train_data),train_data[5])\n",
    "\n",
    "## Création d'un DataLoader\n",
    "## tailles de mini-batch de 16, shuffle=True permet de mélanger les exemples\n",
    "# loader est un itérateur sur les mini-batchs des données\n",
    "loader = DataLoader(train_data, batch_size=16,shuffle=True ) # n'hésitez pas à jouer avec les paramètres\n",
    "\n",
    "#Premier batch (aléatoire) du dataloader : (nb batch = len/batch_size)\n",
    "print(\"DATA LOADER:\\n\",len(iter(loader)),\"\\n\",next(iter(loader))[0].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0, loss : 575.6213703155518\n",
      "iteration : 1, loss : 542.3252592086792\n",
      "iteration : 2, loss : 502.7911653518677\n",
      "iteration : 3, loss : 472.87234687805176\n",
      "iteration : 4, loss : 443.42073154449463\n",
      "iteration : 5, loss : 419.5021781921387\n",
      "iteration : 6, loss : 391.9601163864136\n",
      "iteration : 7, loss : 373.6591601371765\n",
      "iteration : 8, loss : 348.5039439201355\n",
      "iteration : 9, loss : 329.04142475128174\n",
      "iteration : 10, loss : 310.0396227836609\n",
      "iteration : 11, loss : 294.86099767684937\n",
      "iteration : 12, loss : 277.57253646850586\n",
      "iteration : 13, loss : 263.235888004303\n",
      "iteration : 14, loss : 250.66201829910278\n",
      "iteration : 15, loss : 237.50153517723083\n",
      "iteration : 16, loss : 227.52620911598206\n",
      "iteration : 17, loss : 217.27533531188965\n",
      "iteration : 18, loss : 206.48750948905945\n",
      "iteration : 19, loss : 196.72088623046875\n",
      "iteration : 20, loss : 192.0271019935608\n",
      "iteration : 21, loss : 183.38809084892273\n",
      "iteration : 22, loss : 176.48983669281006\n",
      "iteration : 23, loss : 168.79206824302673\n",
      "iteration : 24, loss : 161.02604484558105\n",
      "iteration : 25, loss : 154.93890523910522\n",
      "iteration : 26, loss : 152.16469550132751\n",
      "iteration : 27, loss : 145.6324669122696\n",
      "iteration : 28, loss : 140.08455222845078\n",
      "iteration : 29, loss : 137.41506242752075\n",
      "iteration : 30, loss : 131.97022837400436\n",
      "iteration : 31, loss : 131.80964708328247\n",
      "iteration : 32, loss : 125.40607380867004\n",
      "iteration : 33, loss : 123.65801763534546\n",
      "iteration : 34, loss : 120.44830197095871\n",
      "iteration : 35, loss : 117.52010703086853\n",
      "iteration : 36, loss : 115.6929070353508\n",
      "iteration : 37, loss : 113.1015802025795\n",
      "iteration : 38, loss : 110.23285692930222\n",
      "iteration : 39, loss : 107.67377158999443\n",
      "iteration : 40, loss : 106.38380324840546\n",
      "iteration : 41, loss : 104.89519536495209\n",
      "iteration : 42, loss : 104.21263146400452\n",
      "iteration : 43, loss : 103.691315472126\n",
      "iteration : 44, loss : 100.60816532373428\n",
      "iteration : 45, loss : 100.02473473548889\n",
      "iteration : 46, loss : 99.17873191833496\n",
      "iteration : 47, loss : 96.89746582508087\n",
      "iteration : 48, loss : 96.37579470872879\n",
      "iteration : 49, loss : 94.99451780319214\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPS=1e-4\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "optim = torch.optim.SGD(params=netSeq.parameters(),lr=EPS)\n",
    "\n",
    "# La boucle d'apprentissage :\n",
    "for i in range(EPOCHS):\n",
    "    cumloss = 0\n",
    "    # On parcourt tous les exemples par batch de 16 (paramètre batch_size de DataLoader)\n",
    "    for bx,by in loader:\n",
    "        loss = mseloss(netSeq(bx).view(-1),by)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        cumloss += loss.item() # item pour un scalaire (sinon .data ou detach)\n",
    "    print(f\"iteration : {i}, loss : {cumloss/len(loader)}\") # loss sur un batch => diviser pour avoir une grandeur interprétable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9x2LC_6lCQm"
   },
   "source": [
    "### C.2.2 Checkpointing\n",
    "Les modèles Deep sont généralement long à apprendre. Afin de ne pas perdre des résultats en cours de calcul, il est fortement recommander de faire du **checkpointing**, c'est-à-dire d'enregistrer des points d'étapes du modèle en cours d'apprentissage pour pouvoir reprendre à n'importe quel moment l'apprentissage du modèle en cas de problème.  Il s'agit en pratique de sauvegarder l'état du modèle et de l'optimisateur (et de tout autre objet qui peut servir lors de l'apprentissage) toutes les n itérations. Toutes les variables d'intérêt sont en général disponibles par la méthode **state_dict()** des modèles et de l'optimiseur. \n",
    "\n",
    "En pratique, vous pouvez utilisé un code dérivé de celui ci-dessous.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "URQTq8hrPJO0"
   },
   "outputs": [],
   "source": [
    "# Il existe différentes solutions: en voici une\n",
    "# mais ça marche\n",
    "# \n",
    "import os\n",
    "\n",
    "def save_state(epoch,model,optim,fichier):\n",
    "      \"\"\" sauvegarde du modèle et de l'état de l'optimiseur dans fichier \"\"\"\n",
    "      state = {'epoch' : epoch, 'model_state': model.state_dict(), 'optim_state': optim.state_dict()}\n",
    "      torch.save(state,fichier) # pas besoin de passer par pickle\n",
    " \n",
    "def load_state(fichier,model,optim):\n",
    "      \"\"\" Si le fichier existe, on charge le modèle et l'optimiseur \"\"\"\n",
    "      epoch = 0\n",
    "      if os.path.isfile(fichier):\n",
    "          state = torch.load(fichier)\n",
    "          model.load_state_dict(state['model_state'])\n",
    "          optim.load_state_dict(state['optim_state'])\n",
    "          epoch = state['epoch']\n",
    "      return epoch\n",
    " \n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "optim = torch.optim.SGD(params=netSeq.parameters(),lr=EPS) # extraction auto des paramètres\n",
    "fichier = \"/tmp/netSeq.pth\"\n",
    "start_epoch = load_state(fichier,netSeq,optim)\n",
    "for epoch in range(start_epoch,EPOCHS):\n",
    "    cumloss = 0\n",
    "    for bx,by in loader:\n",
    "        loss = mseloss(netSeq(bx).view(-1),by)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        cumloss += loss.item()\n",
    "    if epoch % 10 ==0: save_state(epoch,netSeq,optim,fichier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IstQCvKblSvT"
   },
   "source": [
    "\n",
    "### C.2.3 GPU \n",
    "Afin d'utiliser un GPU lors des calculs, il est nécessaire de transférer les données et le modèle sur le GPU par l'intermédiaire de la fonction **to(device)** des tenseurs et des modules.  Il est impossible de faire une opération lorsqu'une partie des tenseurs sont sur GPU et l'autre sur CPU. Il faut que tous les tenseurs et paramètres soient sur le même device ! On doit donc s'assurer que le modèle, les exemples et les labels sont sur GPU pour faire les opérations.\n",
    "\n",
    "Par ailleurs, on peut connaître le device sur lequel est chargé un tenseur par l'intermédiaire de ```.device``` (mais pas pour un modèle, il faut aller voir les paramètres dans ce cas).\n",
    "\n",
    "Une manière simple d'utiliser un GPU quand il existe et donc d'avoir un code agnostique est la suivante : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Fs8s7EwwlWTn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch  0\n",
      "batch  10\n",
      "batch  20\n",
      "batch  30\n",
      "Device du mini-batch :  cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "## On charge le modèle sur GPU\n",
    "## A faire avant la déclaration de l'optimiseur, sinon les paramètres optimisés ne seront pas les mêmes! \n",
    "## model =  model.to(device) \n",
    "loader = DataLoader(TensorDataset(boston_x,boston_y), batch_size=16,shuffle=True ) \n",
    "\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "netSeq = netSeq.to(device)\n",
    "optim = torch.optim.SGD(params=netSeq.parameters(),lr=EPS)\n",
    "\n",
    "for i,(bx,by) in enumerate(loader):\n",
    "    ## On charge le batch sur GPU\n",
    "    bx, by = bx.to(device), by.to(device)\n",
    "    loss = mseloss(netSeq(bx).view(-1),by)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if i % 10 ==0: print(\"batch \",i)\n",
    "\n",
    "\n",
    "print(\"Device du mini-batch : \", bx.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5J1b55_lFR-"
   },
   "source": [
    "\n",
    "### C.2.4 TensorBoard\n",
    "\n",
    "Durant l'apprentissage de vos modèles, il est agréable de visualiser de quelle manière évolue le coût, la précision sur l'ensemble de validation ainsi que d'autres éléments. TensorFlow dispose d'un outil très apprécié, le TensorBoard, qui permet de gérer très facilement de tels affichages. On retrouve tensorboard dans **Pytorch** dans ```torch.utils.ensorboard``` qui permet de faire le pont de pytorch vers cet outil. \n",
    "\n",
    "Le principe est le suivant :\n",
    "* tensorboard fait tourner en fait un serveur web local qui va lire les fichiers de log dans un répertoire local. L'affichage se fait dans votre navigateur à partir d'un lien fourni lors du lancement de tensorboard.\n",
    "* Les éléments que vous souhaitez visualiser (scalaire, graphes, distributions, histogrammes) sont écrits dans le fichier de log à partir d'un objet **SummaryWriter** .\n",
    "* la méthode ```add_scalar(tag, valeur, global_step)``` permet de logger une valeur à un step donné, ```add_scalar(tag, tag_scalar_dic, global_step)``` un ensemble de valeurs par l'intermédiaire du dictionnaire ```tag_scalar_dic``` (un regroupement des scalaires est fait en fonction du tag passé, chaque sous-tag séparé par un **/**).\n",
    "\n",
    "Il existe d'autres méthodes ```add_XXX``` pour visualiser par exemple des images, des histogrammes (cf <a href=https://pytorch.org/docs/stable/tensorboard.html>la doc </a>).\n",
    "\n",
    "Le code suivant illustre une manière de l'utiliser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "1kIhHDnElQd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 414745), started 0:02:36 ago. (Use '!kill 414745' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f04ea14874d3d7f4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f04ea14874d3d7f4\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting TensorBoard with logdir /tmp/logs (started 0:02:36 ago; port 6006, pid 414745).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1e0053a317cd632c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1e0053a317cd632c\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spécial notebook, les commandes suivantes permettent de lancer tensorboard\n",
    "# En dehors du notebook, il faut le lancer à la main dans le shell : \n",
    "# tensorboard --logdir logs\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /tmp/logs\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Spécial notebook : pour avoir les courbes qui s'affichent dans le notebook, \n",
    "# sinon aller à l'adresse web local indiquée lors du lancement de tensorboard\n",
    "from tensorboard import notebook\n",
    "notebook.display() # A voir si vous avez une autre fenêtre de gestion de tensorboard ou si vous le voulez à la suite\n",
    "\n",
    "EPS = 1e-5\n",
    "EPOCHS=1000\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "netDeuxCouches = DeuxCouches()\n",
    "netSeq.name = \"Sequentiel\" # nommer les modèles\n",
    "netDeuxCouches.name = \"DeuxCouches\"\n",
    "\n",
    "\n",
    "mseloss = torch.nn.MSELoss()\n",
    "for model in [netSeq, netDeuxCouches]:\n",
    "    ## Obtention d'un SummaryWriter\n",
    "    ## meme répertoire que la commande %tensorboard --logdir logs \n",
    "    summary = SummaryWriter(f\"/tmp/logs/test/{model.name}/\") # on peut ajouter un timestamp ou des paramètres\n",
    "\n",
    "    optim = torch.optim.SGD(params=model.parameters(),lr=EPS) \n",
    "    for i in range(EPOCHS):\n",
    "        cumloss = 0\n",
    "        for bx, by in loader:\n",
    "            loss = mseloss(model(boston_x),boston_y.view(-1,1))\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()  \n",
    "            cumloss+= loss.item()\n",
    "        summary.add_scalar(f\"loss\",cumloss,i) # c'est ici qu'on fait le lien\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaW5Av4elaBN"
   },
   "source": [
    "# D. Exemple typique de code complet & applications\n",
    "* Le graphe de calcul est instancié de manière dynamique sous pytorch, et cela consomme des ressources. Lorsqu'il n'y a pas de rétropropagation qui intervient - lors de l'évaluation d'un modèle par exemple -, il faut à tout prix éviter de le calculer. L'environnement **torch.no_grad()** permet de désactiver temporairement l'instanciation du graphe. **Toutes les procédures d'évaluation doivent se faire dans cet environnement afin d'économiser du temps !**\n",
    "* Pour certains modules, le comportement est différent entre l'évaluation et l'apprentissage (pour le dropout ou la batchnormalisation par exemple, ou pour les RNNs). Afin d'indiquer à pytorch dans quelle phase on se situe, deux méthodes sont disponibles dans la classe module,  **.train()** et **.eval()** qui permettent de basculer entre les deux environnements.\n",
    "\n",
    "Les deux fonctionalités sont très différentes : **no_grad** agit au niveau du graphe de calcul et désactive sa construction (comme si les variables avaient leur propriété **requires_grad** à False), alors que **eval/train** agissent au niveau du module et influence le comportement du module.\n",
    "\n",
    "Vous trouverez ci-dessous un exemple typique de code pytorch qui reprend l'ensemble des éléments de ce tutoriel. Vous êtes prêt maintenant à expérimenter la puissance de ce framework.\n",
    "\n",
    "## D.1. Exemple complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "b3TRg2p5ldCJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 414745), started 0:18:55 ago. (Use '!kill 414745' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1057d0332954744c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1057d0332954744c\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting TensorBoard with logdir /tmp/logs (started 0:18:55 ago; port 6006, pid 414745).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-fde1dade85d77a5f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-fde1dade85d77a5f\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import os\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /tmp/logs\n",
    "\n",
    "notebook.display()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "def save_state(epoch,model,optim,fichier):\n",
    "    state = {'epoch' : epoch, 'model_state': model.state_dict(), 'optim_state': optim.state_dict()}\n",
    "    torch.save(state,fichier)\n",
    "\n",
    "def load_state(fichier,model,optim):\n",
    "    epoch = 0\n",
    "    if os.path.isfile(fichier):\n",
    "        state = torch.load(fichier)\n",
    "        model.load_state_dict(state['model_state'])\n",
    "        optim.load_state_dict(state['optim_state'])\n",
    "        epoch = state['epoch']\n",
    "    return epoch\n",
    "\n",
    "\n",
    "    # Datasets\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston() ## chargement des données\n",
    "all_data = torch.tensor(boston['data'],dtype=torch.float)\n",
    "all_labels = torch.tensor(boston['target'],dtype=torch.float)\n",
    "\n",
    "# Il est toujours bon de normaliser\n",
    "all_data = (all_data-all_data.mean(0))/all_data.std(0)\n",
    "all_labels = (all_labels-all_labels.mean())/all_labels.std()\n",
    "\n",
    "train_tensor_data = TensorDataset(all_data, all_labels)\n",
    "\n",
    "# Split en 80% apprentissage et 20% test\n",
    "train_size = int(0.8 * len(train_tensor_data))\n",
    "validate_size = len(train_tensor_data) - train_size\n",
    "train_data, valid_data = torch.utils.data.random_split(train_tensor_data, [train_size, validate_size])\n",
    "\n",
    "\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "net = torch.nn.Sequential(torch.nn.Linear(all_data.size(1),5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "net.name = \"mon_premier_reseau\"\n",
    "CHECK_FILE = \"/tmp/mon_premier_reseau.chk\"\n",
    "net = net.to(device)\n",
    "MyLoss = torch.nn.MSELoss()\n",
    "optim = torch.optim.SGD(params=net.parameters(),lr=1e-5)\n",
    "\n",
    "start_epoch = load_state(CHECK_FILE,net,optim)\n",
    "\n",
    "# On créé un writer avec la date du modèle pour s'y retrouver\n",
    "summary = SummaryWriter(f\"/tmp/logs/model-{time.asctime()}\")\n",
    "for epoch in range(EPOCHS):\n",
    "    # Apprentissage\n",
    "    # .train() inutile tant qu'on utilise pas de normalisation ou de récurrent\n",
    "    net.train()\n",
    "    cumloss = 0\n",
    "    for xbatch, ybatch in train_loader:\n",
    "        xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
    "        outputs = net(xbatch)\n",
    "        loss = MyLoss(outputs.view(-1),ybatch)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        cumloss += loss.item()\n",
    "    summary.add_scalar(\"loss/train loss\",  cumloss/len(train_loader),epoch)\n",
    "     \n",
    "    if epoch % 10 == 0: \n",
    "        save_state(epoch,net,optim,CHECK_FILE)\n",
    "        # Validation\n",
    "        # .eval() inutile tant qu'on utilise pas de normalisation ou de récurrent\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            cumloss = 0\n",
    "            for xbatch, ybatch in valid_loader:\n",
    "                xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
    "                outputs = net(xbatch)\n",
    "            cumloss += MyLoss(outputs.view(-1),ybatch).item()\n",
    "        summary.add_scalar(\"loss/validation loss\", cumloss/len(valid_loader) ,epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HujGOuB9lte2"
   },
   "source": [
    "## D.2. Jeu de données MNIST\n",
    "Ce jeu de données est l'équivalent du *Hello world* en programmation. Chaque donnée est un chiffre manuscrit (de 0 à 9). Les lignes suivantes vous permettent de charger le jeu de données.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "KH_GScQbltD0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959f388769aa49db9a796747b62678a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b057a2bbfeaf426e91bcbd873928eb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b521cf64f1431a90ea133b497c6ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9491a84d5444e41beab89bb08ba6b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import os\n",
    "from tensorboard import notebook\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "\n",
    "# Téléchargement des données\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.,), (1.0,))])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dimension of images (flattened)\n",
    "HEIGHT,WIDTH = train_set[0][0].shape[1],train_set[0][0].shape[2] # taille de l'image\n",
    "INPUT_DIM = HEIGHT * WIDTH\n",
    "\n",
    "#On utilise un DataLoader pour faciliter les manipulations, on fixe arbitrairement la taille du mini batch à 32\n",
    "all_train_loader = DataLoader(train_set,batch_size=32,shuffle=True)\n",
    "all_test_loader = DataLoader(test_set,batch_size=32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "alloOoFulri7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "## Affichage de quelques chiffres\n",
    "ex,lab = next(iter(all_train_loader))\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(ex[i].view(WIDTH,HEIGHT), cmap='gray', interpolation='none')\n",
    "  plt.title(\"Label : {}\".format(lab[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  ax = plt.gca()\n",
    "  ax.set_facecolor('white')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjPoDcEj28uk"
   },
   "source": [
    "##  D.3. <span class=\"alert-success\"> Exercice : Classification multi-labels, nombre de couche de couches, fonction de coût </span>\n",
    "\n",
    "L'objectif est de classer chaque image parmi les 10 chiffres qu'ils représentent. Le réseau aura donc 10 sorties, une par classe, chacune représentant la probabilité d'appartenance à chaque classe. Pour garantir une distribution de probabilité en sortie, il faut utiliser le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html> **Softmax** </a> : $$Sotfmax(\\mathbf{x}) = \\frac{\\exp{x_i}}{\\sum_{i=1^d} x_i}$$ qui permet de normaliser le vecteur de sortie.\n",
    "\n",
    "* Faites quelques exemples de réseau à 1, 2, 3 couches et en faisant varier les nombre de neurones par couche. Utilisez un coût moindre carré dans un premier temps. Pour superviser ce coût, on doit construire le vecteur one-hot correspondant à la classe : un vecteur qui ne contient que des 0 sauf à l'index de la classe qui contient un 1 (utilisez ```torch.nn.functional.one_hot```).  Comparez les courbes de coût et d'erreurs en apprentissage et en test selon l'architecture.\n",
    "* Le coût privilégié en multi-classe est la *cross-entropy**. Ce coût représente la négative log-vraisemblance : $$NNL(y,\\mathbf{x}) = -x_{y}$$ en notant $y$ l'indice de la classe et $\\mathbf{x}$ le vecteur de log-probabilité inféré. On peut utiliser soit son implémentation par le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss>**NLLLoss**</a>, soit - plus pratique - le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>**CrossEntropyLoss** <a>  qui combine un *logSoftmax* et la cross entropie, ce qui évite d'avoir à ajouter un module de *Softmax* en sortie du réseau. Utilisez ce dernier coût et observez les changements.\n",
    "* Changez la fonction d'activation en une ReLU et observez l'effet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "38Q58_1b-tQs",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting TensorBoard with logdir /tmp/logs (started 0:30:56 ago; port 6006, pid 414745).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c67e70e980070881\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c67e70e980070881\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn.functional import one_hot\n",
    "from tqdm import tqdm\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "notebook.display()\n",
    "\n",
    "## On utilise qu'une partie du training test pour mettre en évidence le sur-apprentissage\n",
    "TRAIN_RATIO = 0.01\n",
    "train_length = int(len(train_set)*TRAIN_RATIO)\n",
    "ds_train, ds_test = random_split(train_set, (train_length, len(train_set)- train_length))\n",
    "\n",
    "#On utilise un DataLoader pour faciliter les manipulations, on fixe  la taille du mini batch à 300\n",
    "train_loader = DataLoader(ds_train,batch_size=300,shuffle=True)\n",
    "test_loader = DataLoader(ds_test,batch_size=300,shuffle=False)\n",
    "\n",
    "\n",
    "def accuracy(yhat,y):\n",
    "    # si  y encode les indexes\n",
    "    if len(y.shape)==1 or y.size(1)==1:\n",
    "        return (torch.argmax(yhat,1).view(y.size(0),-1)== y.view(-1,1)).double().mean()\n",
    "    # si y est encodé en onehot\n",
    "    return (torch.argmax(yhat,1).view(-1) == torch.argmax(y,1).view(-1)).double().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 414745), started 1:02:26 ago. (Use '!kill 414745' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ad793f818d0c1201\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ad793f818d0c1201\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting TensorBoard with logdir /tmp/logs (started 1:02:26 ago; port 6006, pid 414745).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-df76fa442ec0bcd3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-df76fa442ec0bcd3\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/gardette/Document/IODAA/IA/tuto_deep/notebooks/1-pytorch.ipynb Cellule 47\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gardette/Document/IODAA/IA/tuto_deep/notebooks/1-pytorch.ipynb#X64sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m EPOCHS \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gardette/Document/IODAA/IA/tuto_deep/notebooks/1-pytorch.ipynb#X64sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m BATCH_SIZE \u001b[39m=\u001b[39m \u001b[39m300\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gardette/Document/IODAA/IA/tuto_deep/notebooks/1-pytorch.ipynb#X64sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m net1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mSequential(torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mLinear(train_loader\u001b[39m.\u001b[39;49msize(\u001b[39m1\u001b[39m),\u001b[39m5\u001b[39m),torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mTanh(),torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mLinear(\u001b[39m5\u001b[39m,\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gardette/Document/IODAA/IA/tuto_deep/notebooks/1-pytorch.ipynb#X64sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m net1\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnet1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gardette/Document/IODAA/IA/tuto_deep/notebooks/1-pytorch.ipynb#X64sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m CHECK_FILE \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/tmp/net1.chk\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "\n",
    "# On construit 4 réseaux à tester\n",
    "# 10 sorties pour chaque réseau, une par classe. \n",
    "# Comme on va utiliser une cross entropy loss, on ne rescale pas les sorties (la cross entropy combine un softmax + NLLloss)\n",
    "# On pourrait utiliser une BCE loss (vu qu'on est dans un cas binaire pour chaque sortie), dans ce cas il faudrait ajouter une sigmoide en derniere couche.\n",
    "\n",
    "##  TODO \n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import os\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /tmp/logs\n",
    "\n",
    "notebook.display()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "def save_state(epoch,model,optim,fichier):\n",
    "    state = {'epoch' : epoch, 'model_state': model.state_dict(), 'optim_state': optim.state_dict()}\n",
    "    torch.save(state,fichier)\n",
    "\n",
    "def load_state(fichier,model,optim):\n",
    "    epoch = 0\n",
    "    if os.path.isfile(fichier):\n",
    "        state = torch.load(fichier)\n",
    "        model.load_state_dict(state['model_state'])\n",
    "        optim.load_state_dict(state['optim_state'])\n",
    "        epoch = state['epoch']\n",
    "    return epoch\n",
    "\n",
    "# Datasets\n",
    "\n",
    "## On utilise qu'une partie du training test pour mettre en évidence le sur-apprentissage\n",
    "TRAIN_RATIO = 0.01\n",
    "train_length = int(len(train_set)*TRAIN_RATIO)\n",
    "ds_train, ds_test = random_split(train_set, (train_length, len(train_set)- train_length))\n",
    "\n",
    "#On utilise un DataLoader pour faciliter les manipulations, on fixe  la taille du mini batch à 300\n",
    "train_loader = DataLoader(ds_train,batch_size=300,shuffle=True)\n",
    "test_loader = DataLoader(ds_test,batch_size=300,shuffle=False)\n",
    "\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 300\n",
    "\n",
    "net1 = torch.nn.Sequential(torch.nn.Linear(train_loader.size(1),5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "net1.name = \"net1\"\n",
    "CHECK_FILE = \"/tmp/net1.chk\"\n",
    "net1 = net1.to(device)\n",
    "\n",
    "net2 = torch.nn.Sequential(torch.nn.Linear(train_loader.size(1),5),torch.nn.ReLU(),torch.nn.Linear(5,1))\n",
    "net2.name = \"net2\"\n",
    "CHECK_FILE = \"/tmp/net2.chk\"\n",
    "net2 = net2.to(device)\n",
    "\n",
    "net3 = torch.nn.Sequential(torch.nn.Linear(train_loader.size(1),5),torch.nn.Tanh(),train_loader.size(1)/2,5),torch.nn.Tanh(),torch.nn.Linear(5,1)\n",
    "net3.name = \"net3\"\n",
    "CHECK_FILE = \"/tmp/net3.chk\"\n",
    "net3 = net3.to(device)\n",
    "\n",
    "net4 = torch.nn.Sequential(torch.nn.Linear(train_loader.size(1),5),torch.nn.ReLU(),train_loader.size(1)/2,5),torch.nn.ReLU(),torch.nn.Linear(5,1)\n",
    "net4.name = \"net4\"\n",
    "CHECK_FILE = \"/tmp/net4.chk\"\n",
    "net4 = net4.to(device)\n",
    "\n",
    "MyLoss = torch.nn.NLLLoss()\n",
    "optim = torch.optim.SGD(params=net.parameters(),lr=1e-5)\n",
    "\n",
    "# start_epoch = load_state(CHECK_FILE,net,optim)\n",
    "\n",
    "\n",
    "for model in [net1, net2, net3, net4]:\n",
    "    # On créé un writer avec la date du modèle pour s'y retrouver\n",
    "    summary = SummaryWriter(f\"/tmp/logs/{model.name}-{time.asctime()}\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Apprentissage\n",
    "        # .train() inutile tant qu'on utilise pas de normalisation ou de récurrent\n",
    "        model.train()\n",
    "        cumloss = 0\n",
    "        for xbatch, ybatch in train_loader:\n",
    "            xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
    "            outputs = model(xbatch)\n",
    "            loss = MyLoss(outputs.view(-1),ybatch)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            cumloss += loss.item()\n",
    "        summary.add_scalar(\"loss/train loss\",  cumloss/len(train_loader),epoch)\n",
    "        \n",
    "        if epoch % 10 == 0: \n",
    "            save_state(epoch,model,optim,CHECK_FILE)\n",
    "            # Validation\n",
    "            # .eval() inutile tant qu'on utilise pas de normalisation ou de récurrent\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss = 0\n",
    "                for xbatch, ybatch in test_loader:\n",
    "                    xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
    "                    outputs = model(xbatch)\n",
    "                cumloss += MyLoss(outputs.view(-1),ybatch).item()\n",
    "            summary.add_scalar(\"loss/validation loss\", cumloss/len(test_loader) ,epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D.4.  <span class=\"alert-success\"> Exercice : Régularisation des réseaux </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pénalisation des couches\n",
    "Une première technique pour éviter le sur-apprentissage est de régulariser chaque couche par une pénalisation sur les poids, i.e. de favoriser des poids faibles. On parle de pénalisation L1 lorsque la pénalité est de la forme $\\|W\\|_1$ et L2 lorsque la norme L2 est utilisée : $\\|W\\|_2^2$. En pratique, cela consiste à rajouter à la fonction de coût globale du réseau un terme en $\\lambda Pen(W)$ pour les paramètres de chaque couche que l'on veut régulariser (cf code ci-dessous).\n",
    "\n",
    "Expérimentez avec une norme L2 dans $\\{0,10^{-5},10^{-4},10^{-3},10^{-2},\\}$, observez les histogrammes de la distribution des poids et l'évolution de la pénalisation et du coût en fonction du nombre d'époques. Utilisez pour cela  un réseau à 3 couches chacune de taille 100 et un coût de CrossEntropy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_l2(model,epochs,l2):\n",
    "    writer = SummaryWriter(f\"/tmp/logs/l2-{l2}-{model.name}\")\n",
    "    optim = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "    model = model.to(device)\n",
    "    print(f\"running {model.name}-{l2}\")\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        for x,y in train_loader:\n",
    "            optim.zero_grad()\n",
    "            x,y = x.view(x.size(0),-1).to(device), y.to(device)\n",
    "            yhat = model(x)\n",
    "            l = loss(yhat,y)\n",
    "            # Ajout d'une pénalisation L2 sur toutes les couches\n",
    "            l2_loss = 0.\n",
    "            for name, value in model.named_parameters():\n",
    "                if name.endswith(\".weight\"):\n",
    "                    l2_loss += (value ** 2).sum()\n",
    "            l += l2*l2_loss\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)\n",
    "            cumacc += accuracy(yhat,y)*len(x)\n",
    "            count += len(x)\n",
    "        writer.add_scalar('loss/train',cumloss/count,epoch)\n",
    "        writer.add_scalar('accuracy/train',cumacc/count,epoch)\n",
    "        writer.add_scalar('loss/l2',l2_loss,epoch)\n",
    "        if epoch % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                cumloss, cumacc, count = 0, 0, 0\n",
    "                for x,y in test_loader:\n",
    "                    x,y = x.view(x.size(0),-1).to(device), y.to(device)\n",
    "                    yhat = model(x)\n",
    "                    cumloss += loss(yhat,y)*len(x)\n",
    "                    cumacc += accuracy(yhat,y)*len(x)\n",
    "                    count += len(x)\n",
    "                writer.add_scalar(f'loss/test',cumloss/count,epoch)\n",
    "                writer.add_scalar('accuracy/test',cumacc/count,epoch)\n",
    "                ix = 0\n",
    "                for module in model.layers:\n",
    "                    if isinstance(module, nn.Linear):\n",
    "                        writer.add_histogram(f'linear/{ix}/weight',module.weight, epoch)\n",
    "                        ix += 1\n",
    "\n",
    "##  TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Une autre technique très utilisée est le <a href=https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html> **Dropout** </a>. L’idée du Dropout est proche du moyennage de modèle : en entraînant k modèles de manière indépendante, on réduit la variance du modèle. Entraîner k modèles présente un surcoût non négligeable, et l’intérêt du Dropout est de réduire la complexité mémoire/temps de calcul. Le Dropout consiste à chaque itération à *geler* certains neurones aléatoirement dans le réseau en fixant leur sortie à zéro. Cela a pour conséquence de rendre plus robuste le réseau.\n",
    "\n",
    "Le comportement du réseau est donc différent en apprentissage et en inférence. Il est obligatoire d'utiliser ```model.train()``` et ```model.eval()``` pour différencier les comportements.\n",
    "Testez sur quelques réseaux pour voir l'effet du dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQsK7jHuxKqM"
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_dropout(model,epochs):\n",
    "    writer = SummaryWriter(f\"/tmp/logs/{model.name}\")\n",
    "    optim = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "    model = model.to(device)\n",
    "    print(f\"running {model.name}\")\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        model.train()\n",
    "        for x,y in train_loader:\n",
    "            optim.zero_grad()\n",
    "            x,y = x.view(x.size(0),-1).to(device), y.to(device)\n",
    "            yhat = model(x)\n",
    "            l = loss(yhat,y)\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)\n",
    "            cumacc += accuracy(yhat,y)*len(x)\n",
    "            count += len(x)\n",
    "        writer.add_scalar('loss/train',cumloss/count,epoch)\n",
    "        writer.add_scalar('accuracy/train',cumacc/count,epoch)\n",
    "        if epoch % 50 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss, cumacc, count = 0, 0, 0\n",
    "                for x,y in test_loader:\n",
    "                    x,y = x.view(x.size(0),-1).to(device), y.to(device)\n",
    "                    yhat = model(x)\n",
    "                    cumloss += loss(yhat,y)*len(x)\n",
    "                    cumacc += accuracy(yhat,y)*len(x)\n",
    "                    count += len(x)\n",
    "                writer.add_scalar(f'loss/test',cumloss/count,epoch)\n",
    "                writer.add_scalar('accuracy/test',cumacc/count,epoch)\n",
    "\n",
    "\n",
    "def get_dropout_net(in_features,out_features,dims,dropout):\n",
    "    layers = []\n",
    "    dim = in_features\n",
    "    \n",
    "    for newdim in dims:\n",
    "        layers.append(nn.Linear(dim, newdim))\n",
    "        dim = newdim\n",
    "        if dropout>0: layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.ReLU())\n",
    "        dim = newdim\n",
    "    layers.append(nn.Linear(dim,out_features))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "##  TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchNorm\n",
    "\n",
    "On sait que les données centrées réduites permettent un apprentissage plus rapide et stable d’un modèle ; bien qu’on puisse faire en sorte que les données en entrées soient centrées réduites, cela est plus délicat pour les couches internes d’un réseau de neurones. La technique de <a href=https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html> **BatchNorm**</a> consiste à ajouter une couche qui a pour but de centrer/réduire les données en utilisant une moyenne/variance glissante (en inférence) et les statistiques du batch (en\n",
    "apprentissage).\n",
    "\n",
    "Tout comme pour le dropout, il est nécessaire d'utiliser ```model.train()``` et ```model.eval()```. \n",
    "Expérimentez la batchnorm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batchnorm_net(in_features,out_features,dims):\n",
    "    layers = []\n",
    "    dim = in_features\n",
    "    for newdim in dims:\n",
    "        layers.append(nn.Linear(dim, newdim))\n",
    "        dim = newdim\n",
    "        layers.append(nn.BatchNorm1d(dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        dim = newdim\n",
    "    layers.append(nn.Linear(dim,out_features))\n",
    "    return nn.Sequential(*layers)\n",
    "##  TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du sujet à partir de la correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO )\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepLearning fc TP1 2020-2021-correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc5a6fcd42aefd11aead72b01bc4e6842c205ebb69c94514e1be95f4dbf05863"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
